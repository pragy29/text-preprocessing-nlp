# -*- coding: utf-8 -*-
"""task2_31940757.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1caDcDHwml1dYHNL2fdugA2PL_O3gu2C5

<div class="alert alert-block alert-danger">

# FIT5196 Task 2 in Assessment 1
    
#### Student Name: Pragy Parashar
#### Student ID: 31940757

Date: 10/04/2023

Environment: Google Colab

Libraries used:
* os (for interacting with the operating system, included in Python xxxx) 
* pandas 1.1.0 (for dataframe, installed and imported) 
* multiprocessing (for performing processes on multi cores, included in Python 3.6.9 package) 
* itertools (for performing operations on iterables)
* nltk 3.5 (Natural Language Toolkit, installed and imported)
* nltk.collocations (for finding bigrams, installed and imported)
* nltk.tokenize (for tokenization, installed and imported)
* nltk.stem (for stemming the tokens, installed and imported)

    </div>

<div class="alert alert-block alert-info">
    
## Table of Contents

</div>

[1. Introduction](#Intro) <br>
[2. Importing Libraries](#libs) <br>
[3. Examining Input File](#examine) <br>
[4. Loading and Parsing Files](#load) <br>
$\;\;\;\;$[4.1. Tokenization](#tokenize) <br>
$\;\;\;\;$[4.2. Sparse Feature Generation ](#whetev) <br>
$\;\;\;\;$[4.3. Finding First 200 Bigrams](#bigrams) <br>
[5. Writing Output Files](#write) <br>
$\;\;\;\;$[5.1. Vocabulary List](#write-vocab) <br>
$\;\;\;\;$[5.2. Sparse Matrix](#write-sparseMat) <br>
$\;\;\;\;$[5.3. Statistics Matrix](#write-sparseMat) <br>
[6. References](#Ref) <br>

<div class="alert alert-block alert-success">
    
## 1.  Introduction  <a class="anchor" name="Intro"></a>

This assessment concerns textual data and the aim is to extract data, process them, and transform them into a proper format. The dataset provided is in the format of a PDF file containing ....

<div class="alert alert-block alert-success">
    
## 2.  Importing Libraries  <a class="anchor" name="libs"></a>

In this assessment, any python packages is permitted to be used. The following packages were used to accomplish the related tasks:

* **os:** to interact with the operating system, e.g. navigate through folders to read files
* **re:** to define and use regular expressions
* **pandas:** to work with dataframes
* **multiprocessing:** to perform processes on multi cores for fast performance
*   **urllib** : to browse the url and download the required files
*   **collections** to import container datatypes

<div class="alert alert-block alert-warning">
    
### 2.1. Installing Python Modules <a class="anchor" name="mods"></a>

This section involves installing following librariea to Colab Environment:
*   **langid** : Language Identification tool for procrssing text data.
*   **pdfminer** : Text Extraction tool for processing the pdf files.
*   **tika** : File Processing tool by Apache
"""

pip install langid

pip install pdfminer

pip install tika

import os
import re
import langid
import pandas as pd
import multiprocessing
from itertools import chain
import nltk
from nltk.probability import *
from nltk.collocations import *
from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import MWETokenizer
from nltk.stem import PorterStemmer
from nltk.util import ngrams
import pdfminer
import urllib.request
import time
from tika import parser
from collections import Counter

"""-------------------------------------

<div class="alert alert-block alert-success">
    
## 3.  Examining Input File <a class="anchor" name="examine"></a>

Connecting to the google drive to access the files for performing the required tasks
"""

from google.colab import drive
drive.mount('/content/drive')

"""Let's examine what is the content of the file."""

#Extracting the content of the input pdf file using pdfminer's pdf2txt.py
!pdf2txt.py -o 31940575_task2.txt '/content/drive/Shareddrives/FIT5196_S1_2023/Assessment1/student_data/task2/31940757.pdf'

"""The extracted file is stored in path specified in the command line above as text file "**31940575_task2.txt**"
"""

#accessing the input file using file handling operations
assessment_txt = './31940575_task2.txt'
# opens the file in read mode
with open(assessment_txt, 'r') as pdf_txt:
#loop through the lines in the file
  for line in pdf_txt:
      # prints text contained in the file.
      print (repr(line))

"""It is noticed that file contains columns filename name and url to download the file."""

# creates a list to store the url to the directory
url_list = []
# regex expression to retrieve file name
re_paperid = r'PP[0-9]*'
# regex expression to to retrieve url
re_url = r'https:.*'
# opens file in read mode
with open(assessment_txt, 'r') as pdf_txt:
  # loops over the content of the file
  for line in pdf_txt:
      # looks for the url and file name in the lines
      if re.findall(re_paperid, line) and re.findall(re_url, line):
        # removes the passed pattern from the url
        url = re.sub(r'\n', '', re.findall(re_url, line)[0])
        # adds the file name and url to the list
        url_list.append([re.findall(re_paperid, line)[0], url])

"""This code block downloads the data from the url extracted. Since google drive limits 20 files to be downloaded within a 20 min span, we have halted the code by using time module of python We process the url in batch of 40 and halt execution for 29 mins."""

'''# creates a new directory for storing the required files
os.makedirs("/content/drive/MyDrive/Asessment1/Task2")
# changes the working directory to the specified location
os.chdir("/content/drive/MyDrive/Asessment1/Task2")
# sets total number of files required to be downloaded
number_of_paper = len(url_list)
# instantiate a variable to loop over the url list
i = 0
# loops for the number of files 
while i <=  number_of_paper:
  if i < number_of_paper:
    # loop 20 files at a time as google drive only supports downloading 20 files in 20 mins
    for url in url_list[i:i+40]:
      # file name of the downloaded file
      file_name = url[0] + '.pdf'
      url = url[1]
      # downloads the file using the url to the specified directory
      urllib.request.urlretrieve(url, f'{os.getcwd()}/{file_name}')
  else:
    url = url_list[number_of_paper]
    file_name = url[0] + '.pdf'
    url = url[1]
    # downloads the file using the url to the specified directory
    urllib.request.urlretrieve(url, f'{os.getcwd()}/{file_name}')

  # halts the execution for 20 minutes
  time.sleep(1200)
  i+=20'''

"""<div class="alert alert-block alert-success">
    
## 4.  Loading and Parsing File <a class="anchor" name="load"></a>

In this section, all the previously downloaded files are parsed and the data is stored in a dictionary with the file name as the key.
"""

# changes the working directory to the required path
os.chdir("/content/drive/MyDrive")
# path for the files used in the assignment
file_path = os.getcwd()+ "/Asessment1/Task2"
# dictonary to store file name as key and the text contained as values
file_dict = {}
# loops through all the files stored in the specified directory
for files in os.listdir(file_path):
  # parser object to read and store data in pdf files
  file_data = parser.from_file(file_path + '/'+ files)
  # indexing the parser_object content to retrieve the text in the files
  paper_content = file_data["content"]
  # storing the parsed content in dictionary
  file_dict[re.sub(r'\.pdf', '', files)] = paper_content
file_dict["PP6030"]

"""Here the extreacted data is processed to retrieve the paper body using regex.

"""

# dictionary to store the paper body as the values and file name as the key
paper_content_dict = {}
# loops through the file dictionary
for keys, values in file_dict.items():
  # regex function to match the pattern to the string
  match = re.search(r"((Paper Body?)(.*)(References?))",values,re.DOTALL)
  # retrives the body content of the paper
  paper_body = match.group(3)
  # removes the page numbers and the empty spaces before the page num
  paper_body = re.sub(r'\n\n\d*\n\n\n\n',' ',paper_body)
  # concatenates the hyphenated split words at the end of each line
  paper_body = re.sub(r'-\n','',paper_body)
  # removes - from the hyphented words and replaces it with empty space
  paper_body = re.sub(r'-',' ',paper_body)
  # removes the extra empty spaces between different lines of the paper body
  paper_body = re.sub(r'\n+',' ',paper_body)
  paper_body=re.sub(r'Ô¨Å','fi',paper_body)
  # removes the html tags from the paper bdy
  paper_body=re.sub(r'&#\d','',paper_body)
  paper_body=re.sub(r'.\s2\s$','',paper_body)
  # stores the paper body content as value in the dictionary
  paper_content_dict[keys] = paper_body
paper_content_dict["PP3675"]

"""The regex pattern looks matches the data in three group:


1.   Paper Body: Group 2
2.   Anything in between i.e. the paper body content: Group 3
3.   Reference: Group 4

The whole pattern is stored as Group 4. The paper body content is fetched by using group function.




![image.png](data:image/png;base64,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)

Then the paper body is processed for removing all the unwanted tags and spaces.

Let's examine the dictionary generated. For counting the total number of papers extracted. Using length function it is clear we have to process 200 paper that have been downloaded. The content dictionary stores tghe paper id as the key and the content as the value.
"""

len(paper_content_dict)

paper_content_dict["PP3675"]

"""<div class="alert alert-block alert-warning">
    
### 4.1. Tokenization <a class="anchor" name="tokenize"></a>

Tokenization is a principal step in text processing and producing unigrams and bigrams. In this section, we start by tokenizing the sentences in ordere to achieve normalized sentences. As per required we are required to have the first letter of a word in a sentence to be in lowercase. punkt mdoule from nltk is used to achieve sentecne tokenization.
"""

# unsupervised algorithm that helops to achieve sentence segmentation
nltk.download('punkt')
# dictionary to the segemented list for all the paper id as keys
sentence_seg = {}
# loops through the content dictionary
for key, value in paper_content_dict.items():
  # segements the entire paper body into different sentences 
  paper_segment = ""
  segment_list = []
  sentence = nltk.sent_tokenize(value)
  # sentence normaliation
  for i in sentence:
    segment = i.strip()
    paper_segment += segment[0].lower() + segment[1:]
  sentence_seg[key] = ''.join(paper_segment)
sentence_seg["PP3675"]

"""Here we tokenize all the paper body contents to have a list of all the token in each paper. After the tokens are created we add them in a list and set it as a value in the dictionary having the paper id as the key."""

# dictionary to store the tokens as the values and paper id as the key
token_dict = {}
# instansiates tokenizer accroding to the regex pattern passed
tokenizer = RegexpTokenizer(r"[A-Za-z]\w+(?:[-'?]\w+)?")
# loops through the normalized content 
for key,value in sentence_seg.items():
  # tokenizes the paper body content for each individual paper
  tokens = tokenizer.tokenize(value)
  # stores the generated tokens
  token_dict[key] = tokens
token_dict["PP3675"]

"""The above operation results in a dictionary with paper id as the key and all the paper content tokenized and stored as values.


"""

# generates a list of all the tokens from the dictionary for all the papers
token_list = list(chain.from_iterable(token_dict.values()))
token_list[10]

"""<div class="alert alert-block alert-warning">
    
### 4.2. Sparse Feature Generation <a class="anchor" name="whetev"></a>

Stopword/ stop tokens are those tokens which we don't need in our vocablary. These are extracted from the file provided in the sharedrive. We store these stopwords in a list.
"""

# list to store all the stop words
stopwords_list = []
# opens the file containing the stop words
with open('/content/drive/Shareddrives/FIT5196_S1_2023/Assessment1/stopwords_en.txt') as f:
  # adds the stop words to the list
  stopwords_list = f.read().splitlines() 
stopwords_list[5]

"""<div class="alert alert-block alert-warning">
    
### 4.3. Finding First 200 Bigrams <a class="anchor" name="bigrams"></a>

One of the tasks is to find the first 200 bigrams based on frequency. These bigrams should also be included in our vocabulary list. We need to make sure that the bigrams are not in the stopwords. After we have our bigrams we check if they are present in the stopwaords list or not. If our bigrams are not part of stop word only then we add them to our vocabulary. Also we need to make sure we have only the 200 most frequent bigrams in our vocab.
"""

# generates a list containing set of bigrams from the token list
bigram_list = list(nltk.bigrams(token_list))
# list to store the bigrams that are not included in the stop words list
fltrd_bgrms = []
for bigrams in bigram_list:
  # checks if the words in bigrams are contained in teh stop words
  if bigrams[0] not in stopwords_list and bigrams[1] not in stopwords_list:
    fltrd_bgrms.append(bigrams)
fltrd_bgrms[10]

# creates a iter object of Counter type containing bigrams ad their frequencies of occurances
bgrms_frq = Counter(fltrd_bgrms)
# returns a list of most frequently occured 200 bigrams along with their frequencies
most_frq_bgrms = bgrms_frq.most_common(200)
top_200_bgrms = []
for values in most_frq_bgrms:
  top_200_bgrms.append(values[0])
top_200_bgrms[7]

"""We add the bigrams to our existing tokens list by concatenating the bigram with __ using MWETokenizer. We make our vocab list by converting the list of the updated tokens into a set which eleminated the duplicates and gives us unique tokens."""

# creates a new tokenizer object with the bigrams concatenated with __
mwetokenizer = MWETokenizer(top_200_bgrms, separator = "__")
# creates a new dict object from the previos tokens and updates them with the new bigrams as tokens
colloc_dict =  dict((paper_id, mwetokenizer.tokenize(values)) for paper_id,values in token_dict.items())
# creates a list of all the updated tokens unigrams and top 200 bigrams
all_words_colloc = list(chain.from_iterable(colloc_dict.values()))
# creates a list of all unique tokens
colloc_voc = list(set(all_words_colloc))
colloc_voc[5]

"""At this stage, we have a dictionary of tokenized words, whose keys are indicative of file they are contained in.

"""

# dictionary to store the tokens that are not in the stop words
stopped_token_dict = {}
# iterates through the updated dict containing bigrams
for keys, values in colloc_dict.items():
    stopped_tokens = []
    for tokens in values:
      if values not in stopwords_list:
        stopped_tokens.append(tokens)
    # adds the tokens which are not stop words to the dict
    stopped_token_dict[keys] = stopped_tokens
stopped_token_dict["PP3675"]

"""We eleminate the rare tokens. For this we need to make sure the frequency of occurance of each token should be less than 95% and more than 3% and token is of more than 3 characters."""

# stores the tokens which are not rare
rare_token_dict = {}
rare_tokens = []
# list of all the tokens without stop words
tokens = list(chain.from_iterable([set(value) for value in stopped_token_dict.values()]))
# iterable object which stores the token as key and its frequency of occurance as value
token_frq = FreqDist(tokens).most_common(len(colloc_voc))
for tokens in token_frq:
  # if not rare tokens
  if len(tokens[0]) >= 3 and ((tokens[1]/len(stopped_token_dict)) <= 0.95) and ((tokens[1]/len(stopped_token_dict)) >= 0.03):
    rare_tokens.append(tokens[0])
for keys, values in stopped_token_dict.items():
  rare_val = []
  for tokens in values:
    if tokens in rare_tokens:  
      rare_val.append(tokens)
  # updated dict storing the tokens that are not rare
  rare_token_dict[keys] = rare_val
rare_token_dict["PP3675"]

"""Stemming is the process reducing a word to it stek value. Here we try to achieve that using the PorterStemmer class."""

stemmed_dict = {}
# instantiates a class object
stemmer = PorterStemmer()
for keys in rare_token_dict.keys():
  st_token_list = []
  for value in rare_token_dict[keys]:
    if "__" in value: 
      # bigrans are not stemmed but used as is
      st_token_list.append(value)
    else:
      # stems any other tokens than bigrams
      st_token_list.append(stemmer.stem(value))
  stemmed_dict[keys] = st_token_list

len(stemmed_dict["PP3675"])

"""<div class="alert alert-block alert-success">
    
## 5. Writing Output Files <a class="anchor" name="write"></a>
"""

stemmed_list = list(chain.from_iterable(stemmed_dict.values()))

"""files need to be generated:
* Vocabulary list
* Sparse matrix (count_vectors)
* Statistics matrix

This is performed in the following sections.

<div class="alert alert-block alert-warning">
    
### 5.1. Vocabulary List <a class="anchor" name="write-vocab"></a>

List of vocabulary should also be written to a file, sorted alphabetically, with their reference codes in front of them. This file also refers to the sparse matrix in the next file. For this purpose.
"""

# list of tokens sorted alphabetically
vocab_list = sorted(set(stemmed_list))
vocab_dict = {}

for i in range(len(vocab_list)):
  # getting the index position of the token
  vocab_dict[vocab_list[i]] = i

os.chdir("/content/drive/MyDrive/Asessment1/OutputFiles/Task2")
# creates a file containing the token and its index position
with open('task2_31940757_vocab.txt','w') as vocab:
  for key, value in vocab_dict.items():
    vocab.write(key+":"+str(value)+"\n")

vocab_list

"""<div class="alert alert-block alert-warning">
    
### 5.2. Sparse Matrix <a class="anchor" name="write-sparseMat"></a>

For writing sparse matrix for a paper, we firstly calculate the frequency of words for that paper. We achieve this by converting the text to numerical data using CountVectorizer. This is used to convert the text data to a matrix containing count of the tokens.
"""

from sklearn.feature_extraction.text import CountVectorizer
# creates a CountVectorizer object
vectorizer = CountVectorizer(analyzer = "word") 
# creates the sparse matrix        
sparse_matrix = vectorizer.fit_transform([' '.join(value) for value in stemmed_dict.values()])
print (sparse_matrix.toarray())

"""<div class="alert alert-block alert-warning">
    
### 5.3. Statistics Matrix <a class="anchor" name="write-sparseMat"></a>
"""

# create a file for storing the vector count
vector_file = open("./task2_31940757_count_vectors.txt", 'w')
# creates a dict for storing numerical tokens
sparse_dict = {}
# instantiates index 
idx = 0

for token in vocab_list:
  # replace the token value with its index value
  sparse_dict[token] = idx
  idx = idx + 1
    
# iterates through updated dict    
for key, value in stemmed_dict.items():
  # token id list
  value_id = [sparse_dict[token] for token in value]
  vec_str = ""
  vector_file.write("{paper_id},".format(paper_id = key))
  for m, n in FreqDist(value_id).items():
    vec_str += "{token_index}:{word_count},".format(token_index = m, word_count = n)
  vector_file.write(vec_str[:-2])
  vector_file.write('\n')
vector_file.close()

"""For writing overall statistics of papers, we aggregate statistics"""

auth_dict = {}
title_dict = {}
abstract_dict = {}
for keys in file_dict.keys():
  # matches for author data
  auth_matches = re.search(r'(?:Authored by:)(.*)(?:Abstract\n)',file_dict[keys], re.DOTALL)
  if auth_matches:
    # authors names in captured groups
    authors = auth_matches.group(1)
    authors = re.sub('-\n','',authors)
    authors = re.sub('\n',',',authors)
    auth_dict[keys] = authors
  # matches for title data
  title_matches = re.search(r'(.*)(?:Authored by:)',file_dict[keys], re.DOTALL)
  if title_matches:
    title = title_matches.group(1)
    title = re.sub('-\n','',title)
    title = re.sub('\n',' ',title)
    title_dict[keys] = title
  # matches for abstract data
  abstract_matches = re.search(r'(?:Abstract\n)(.*)(?:\d*Paper Body)',file_dict[keys], re.DOTALL)
  if abstract_matches:
    abstract = abstract_matches.group(1)
    abstract = re.sub('-\n','',abstract)
    abstract = re.sub('\n',' ',abstract)
    abstract_dict[keys] = abstract

title_token = {} 
# create titke tokens
tokenizer = RegexpTokenizer(r"[A-Za-z]\w+(?:[-'?]\w+)?")
for keys, values in title_dict.items():
    tokens = tokenizer.tokenize(values.lower())
    title_token[keys] = tokens

title_token

# removing stop words
title_tok_list = list(chain.from_iterable(title_token.values()))
stop_title = {}
for keys, values in title_token.items():
  stopped_tokens = []
  for tokens in values:
    if tokens not in stopwords_list:
      stopped_tokens.append(tokens)
  stop_title[keys] = stopped_tokens

# updated title token list
title_tok_list = list(chain.from_iterable(stop_title.values()))
# creates iter object with frequency distribution of tokens
title_freq = FreqDist(title_tok_list).most_common(10)[:10]
# sorts data in descending order of frequency and alphabetically
sorted_title = sorted(sorted(title_freq, key = lambda x: x[0]), key = lambda x: x[1], reverse = True)
# creates list of the top 10 tokens
top_title = list([value[0] for value in sorted_title])

title_freq

abstract_seg = {}
# loops through the content dictionary
for key, value in abstract_dict.items():
  abstract_sentence = []
  # segements the entire paper body into different sentences 
  sentence = nltk.sent_tokenize(value)
  # sentence normaliation
  for i in range(0, len(sentence)):
    segment = sentence[i].strip()
    norm_list = segment.split(' ')
    norm_list[0] = norm_list[0].lower()
    norm_sentence = ' '.join(norm_list)
    # first word of each sentence is changed to lower case
    #sentence[i] = sentence[i][0].lower() + sentence[i][1:]
    # stores the normalized sentence
    abstract_sentence.append(norm_sentence)
  abstract_seg[key] = ''.join(abstract_sentence)

abstract_token = {} 
tokenizer = RegexpTokenizer(r"[A-Za-z]\w+(?:[-'?]\w+)?")
for keys, values in abstract_dict.items():
    tokens = tokenizer.tokenize(values.lower())
    abstract_token[keys] = tokens

abstract_tok_list = list(chain.from_iterable(abstract_token.values()))
stop_abstract = {}
for keys, values in abstract_token.items():
  stopped_tokens = []
  for tokens in values:
    if tokens not in stopwords_list:
      stopped_tokens.append(tokens)
  stop_abstract[keys] = stopped_tokens

abstract_tok_list = list(chain.from_iterable(stop_abstract.values()))
abstract_freq = FreqDist(abstract_tok_list).most_common(10)[:10]
sorted_abstract = sorted(sorted(abstract_freq, key = lambda x: x[0]), key = lambda x: x[1], reverse = True)
top_abstract = list([value[0] for value in sorted_abstract])

top_abstract

authors = []
# separates the full name of all the authors in the data
for key in auth_dict.keys():
  auth_list = auth_dict[key].split(",")
  for i in auth_list:
    if i != '':
       authors.append(i)
authors[5]

auth_freq = FreqDist(authors).most_common(10)[:10]
sorted_authors = sorted(sorted(auth_freq, key = lambda x: x[0]), key = lambda x: x[1], reverse = True)
top_authors = list([value[0] for value in sorted_authors])

#create df for the stats
stats_df = pd.DataFrame({"top10_terms_in_abstracts" : top_abstract,
                        "top10_terms_in_titles" : top_title,
                        "top10_authors" : top_authors})
stats_df

stats_df.to_csv('/content/drive/MyDrive/Asessment1/OutputFiles/Task2/task2_31940757_stats.csv')

"""-------------------------------------

<div class="alert alert-block alert-success">
    
## 6. References <a class="anchor" name="Ref"></a>

[1] https://cheatography.com/davechild/cheat-sheets/regular-expressions/

[2] https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html

[3] https://towardsdatascience.com/basics-of-countvectorizer-e26677900f9c#:~:text=Countvectorizer%20is%20a%20method%20to,sparse%20matrix%20as%20shown%20below.

[4] https://stackoverflow.com/questions/4233476/sort-a-list-by-multiple-attributes

## --------------------------------------------------------------------------------------------------------------------------
"""