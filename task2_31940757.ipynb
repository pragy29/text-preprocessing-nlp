{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8tinZOUlDER"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\">\n",
        "\n",
        "# FIT5196 Task 2 in Assessment 1\n",
        "    \n",
        "#### Student Name: Pragy Parashar\n",
        "#### Student ID: 31940757\n",
        "\n",
        "Date: 10/04/2023\n",
        "\n",
        "Environment: Google Colab\n",
        "\n",
        "Libraries used:\n",
        "* os (for interacting with the operating system, included in Python xxxx) \n",
        "* pandas 1.1.0 (for dataframe, installed and imported) \n",
        "* multiprocessing (for performing processes on multi cores, included in Python 3.6.9 package) \n",
        "* itertools (for performing operations on iterables)\n",
        "* nltk 3.5 (Natural Language Toolkit, installed and imported)\n",
        "* nltk.collocations (for finding bigrams, installed and imported)\n",
        "* nltk.tokenize (for tokenization, installed and imported)\n",
        "* nltk.stem (for stemming the tokens, installed and imported)\n",
        "\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnnLnFnLlDEU"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "## Table of Contents\n",
        "\n",
        "</div>\n",
        "\n",
        "[1. Introduction](#Intro) <br>\n",
        "[2. Importing Libraries](#libs) <br>\n",
        "[3. Examining Input File](#examine) <br>\n",
        "[4. Loading and Parsing Files](#load) <br>\n",
        "$\\;\\;\\;\\;$[4.1. Tokenization](#tokenize) <br>\n",
        "$\\;\\;\\;\\;$[4.2. Sparse Feature Generation ](#whetev) <br>\n",
        "$\\;\\;\\;\\;$[4.3. Finding First 200 Bigrams](#bigrams) <br>\n",
        "[5. Writing Output Files](#write) <br>\n",
        "$\\;\\;\\;\\;$[5.1. Vocabulary List](#write-vocab) <br>\n",
        "$\\;\\;\\;\\;$[5.2. Sparse Matrix](#write-sparseMat) <br>\n",
        "$\\;\\;\\;\\;$[5.3. Statistics Matrix](#write-sparseMat) <br>\n",
        "[6. References](#Ref) <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8mo6PPRlDEU"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 1.  Introduction  <a class=\"anchor\" name=\"Intro\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewZrff73lDEV"
      },
      "source": [
        "This assessment concerns textual data and the aim is to extract data, process them, and transform them into a proper format. The dataset provided is in the format of a PDF file containing ...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSr_kwKclDEV"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 2.  Importing Libraries  <a class=\"anchor\" name=\"libs\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acwZw2NklDEW"
      },
      "source": [
        "In this assessment, any python packages is permitted to be used. The following packages were used to accomplish the related tasks:\n",
        "\n",
        "* **os:** to interact with the operating system, e.g. navigate through folders to read files\n",
        "* **re:** to define and use regular expressions\n",
        "* **pandas:** to work with dataframes\n",
        "* **multiprocessing:** to perform processes on multi cores for fast performance\n",
        "*   **urllib** : to browse the url and download the required files\n",
        "*   **collections** to import container datatypes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 2.1. Installing Python Modules <a class=\"anchor\" name=\"mods\"></a>"
      ],
      "metadata": {
        "id": "myi52KehsQRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section involves installing following librariea to Colab Environment:\n",
        "*   **langid** : Language Identification tool for procrssing text data.\n",
        "*   **pdfminer** : Text Extraction tool for processing the pdf files.\n",
        "*   **tika** : File Processing tool by Apache"
      ],
      "metadata": {
        "id": "mfUbDTHjsagw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAAY5-QNKmnf",
        "outputId": "bdf02202-b2c5-42e7-d919-94f13897f309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langid in /usr/local/lib/python3.9/dist-packages (1.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from langid) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "pip install langid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi1cHzgeBtFn",
        "outputId": "f74440d3-a4b9-40ef-a121-439b8df91d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pdfminer in /usr/local/lib/python3.9/dist-packages (20191125)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.9/dist-packages (from pdfminer) (3.17)\n"
          ]
        }
      ],
      "source": [
        "pip install pdfminer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Dg-bOkm7ekT",
        "outputId": "729e5576-b96c-4455-c958-62c311695b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tika in /usr/local/lib/python3.9/dist-packages (2.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tika) (67.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from tika) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->tika) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->tika) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->tika) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->tika) (3.4)\n"
          ]
        }
      ],
      "source": [
        "pip install tika"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qgmGWs8HlDEW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import langid\n",
        "import pandas as pd\n",
        "import multiprocessing\n",
        "from itertools import chain\n",
        "import nltk\n",
        "from nltk.probability import *\n",
        "from nltk.collocations import *\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.util import ngrams\n",
        "import pdfminer\n",
        "import urllib.request\n",
        "import time\n",
        "from tika import parser\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwNp0KnWlDEX"
      },
      "source": [
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "SA7fSJiRlDEY"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 3.  Examining Input File <a class=\"anchor\" name=\"examine\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connecting to the google drive to access the files for performing the required tasks"
      ],
      "metadata": {
        "id": "fis8uA-Su8Pd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPCuEl8smTHW",
        "outputId": "6eeaaee4-2a9e-4282-892c-39e43037083e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CJDLDI6lDEY"
      },
      "source": [
        "Let's examine what is the content of the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RyLbqkRxmCEZ"
      },
      "outputs": [],
      "source": [
        "#Extracting the content of the input pdf file using pdfminer's pdf2txt.py\n",
        "!pdf2txt.py -o 31940575_task2.txt '/content/drive/Shareddrives/FIT5196_S1_2023/Assessment1/student_data/task2/31940757.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The extracted file is stored in path specified in the command line above as text file \"**31940575_task2.txt**\""
      ],
      "metadata": {
        "id": "Ed8_Sdmnvoi3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZX7XBbOPIeG",
        "outputId": "7045cce7-e4c3-45d7-b914-11fb75570056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'filename\\n'\n",
            "'\\n'\n",
            "'url\\n'\n",
            "'\\n'\n",
            "'PP3164.pdf https://drive.google.com/uc?export=download&id=1KGospAeT509Iy4az0wnZc1aNNEF44Lfb\\n'\n",
            "'PP3219.pdf https://drive.google.com/uc?export=download&id=1I5AzXMkIED3-k6KBAIQiKw-3UbGrme04\\n'\n",
            "'PP3220.pdf https://drive.google.com/uc?export=download&id=1jS0EQ3U7HzSoMLZZOkI1ZSSTQLJFZxs_\\n'\n",
            "'PP3249.pdf https://drive.google.com/uc?export=download&id=1cw7WWdpYOQNpTwsWMCiv6BTDNhuc9KUy\\n'\n",
            "'PP3250.pdf https://drive.google.com/uc?export=download&id=1uq-f5R78le939qB3qpzAFH7BfkjHIgX3\\n'\n",
            "'PP3279.pdf https://drive.google.com/uc?export=download&id=1z8CIA5riRiw7Gc5MWwvo_BgaVn0msqN-\\n'\n",
            "'PP3296.pdf https://drive.google.com/uc?export=download&id=1cPk1R1kulowLj3LZQmhCDla8-Uo7WD-W\\n'\n",
            "'PP3305.pdf https://drive.google.com/uc?export=download&id=1rgIUKVDgqHCjeTb6QvylOKeHj5chjrvI\\n'\n",
            "'PP3309.pdf https://drive.google.com/uc?export=download&id=166801_pe1itXfHzk0rxG7P3YDtUze0lY\\n'\n",
            "'PP3357.pdf https://drive.google.com/uc?export=download&id=1rdknW_Zh7pWWxk920Lq0sgxXPYVa9HdV\\n'\n",
            "'PP3402.pdf https://drive.google.com/uc?export=download&id=1XRXoFzEq7iKZrpvkwyLIrUk3XM1kHhB4\\n'\n",
            "'PP3420.pdf https://drive.google.com/uc?export=download&id=1i1V09tSzvS3RoyQW16MVhe5VeyNZKG5a\\n'\n",
            "'PP3464.pdf https://drive.google.com/uc?export=download&id=1HH_uw4xEsFFx3vmW2NKihPw5NGIULJEx\\n'\n",
            "'PP3466.pdf https://drive.google.com/uc?export=download&id=1hYyivtsETqrkYvI-Il6T9MvEHBtVdmXA\\n'\n",
            "'PP3469.pdf https://drive.google.com/uc?export=download&id=1MEidzkVJiB2ncr62b8kS4-_YTrHGq812\\n'\n",
            "'PP3485.pdf https://drive.google.com/uc?export=download&id=1IWavzsPgIfS4yOajk7PPymM5nutrcfhP\\n'\n",
            "'PP3501.pdf https://drive.google.com/uc?export=download&id=1-Ngw-oxzVlebXbI-lpQx4SbXFIpes8oT\\n'\n",
            "'PP3533.pdf https://drive.google.com/uc?export=download&id=14GHTHZ0fAj8SLTIRH2wJkl8lvx0E5xcn\\n'\n",
            "'PP3575.pdf https://drive.google.com/uc?export=download&id=1SKl8ED7qyvVTufcXv_DmGz8h64omQrK5\\n'\n",
            "'PP3583.pdf https://drive.google.com/uc?export=download&id=1eDYV9Du-3ykEkBaBf43g9oDARuftl0S5\\n'\n",
            "'PP3587.pdf https://drive.google.com/uc?export=download&id=1Mdj29YDNEWBxAtu1Nxls7eQqKwRpRhC2\\n'\n",
            "'PP3618.pdf https://drive.google.com/uc?export=download&id=1iIw7i5HRlk8uY7ZLOluoWYg5YIblOQEZ\\n'\n",
            "'PP3631.pdf https://drive.google.com/uc?export=download&id=1xaSEH2IjrkSvTx5SNqIeW5nDDqikyFpI\\n'\n",
            "'PP3632.pdf https://drive.google.com/uc?export=download&id=1Yg_8NYubNws5JPqmmrQ4JBSdML3pb6um\\n'\n",
            "'PP3637.pdf https://drive.google.com/uc?export=download&id=1AnXdJuNwZqoqDTPtCbWRRDq1GvcYnpr6\\n'\n",
            "'PP3675.pdf https://drive.google.com/uc?export=download&id=1q8syWIqC-hCnLwVivQMPPab0WR2-gUP8\\n'\n",
            "'PP3707.pdf https://drive.google.com/uc?export=download&id=1AuakAM6xtSHUFC2chaLfDSg_5qVzYN67\\n'\n",
            "'PP3766.pdf https://drive.google.com/uc?export=download&id=1SFKCVq7oEVIGufroMN8D6qaTDg4enDFR\\n'\n",
            "'PP3785.pdf https://drive.google.com/uc?export=download&id=1NS5b8TM3hhr0MAqQRHGSFMR3euuJ5LzG\\n'\n",
            "'PP3809.pdf https://drive.google.com/uc?export=download&id=1QjHuVwXHZVpr4hyKCUH63MTH2Rw__37s\\n'\n",
            "'PP3847.pdf https://drive.google.com/uc?export=download&id=1j_iKEbopmlmiFCmARB8sqQPN-JtafoNI\\n'\n",
            "'PP3855.pdf https://drive.google.com/uc?export=download&id=1h3Of-jmLA2YqpehWh04Zz1C7Jgpdrn9o\\n'\n",
            "'PP3869.pdf https://drive.google.com/uc?export=download&id=12MOF0HHjJOUA5LLjnW7SpSnYhaaI9nQc\\n'\n",
            "'PP3870.pdf https://drive.google.com/uc?export=download&id=1ha9fQRNqvxOqLEeimy1BmCGm1Tfk5ymk\\n'\n",
            "'PP3872.pdf https://drive.google.com/uc?export=download&id=1Usce84HNceHxQ7VnYy5r_t-8sSTNCDeV\\n'\n",
            "'PP3906.pdf https://drive.google.com/uc?export=download&id=1OcCoaCMUdVnHWH4paxgZ-Oj7hfnPFX1c\\n'\n",
            "'PP3910.pdf https://drive.google.com/uc?export=download&id=165b-qfsJjUESxWES0Af-iBXuQS4e3Ae7\\n'\n",
            "'PP3950.pdf https://drive.google.com/uc?export=download&id=1mmJ_hCkOI_ZPZoSz7QonriqbuD8Ha6Xk\\n'\n",
            "'PP4004.pdf https://drive.google.com/uc?export=download&id=1cMZPW8YgXLrWfnTTAweKcxZK4SsPR3B8\\n'\n",
            "'PP4050.pdf https://drive.google.com/uc?export=download&id=1BF-CHYg2F7RV7ihoZ0i2h7e5qNb59YRq\\n'\n",
            "'PP4070.pdf https://drive.google.com/uc?export=download&id=1jWcejcR8nSb_y7SngYWlVgvwtHCHNKMm\\n'\n",
            "'PP4094.pdf https://drive.google.com/uc?export=download&id=1DTh8bAm8g_cblm3vsi4mFFlrDey37pL9\\n'\n",
            "'PP4111.pdf https://drive.google.com/uc?export=download&id=1Vv1PqrjC7PYgmNcwr6xTTNOBXz1sI4Bs\\n'\n",
            "'PP4127.pdf https://drive.google.com/uc?export=download&id=195sUsvYNCyF2YsHKhIvujuHq56A7T6s7\\n'\n",
            "'PP4134.pdf https://drive.google.com/uc?export=download&id=1WCWfBc84I2_OrrtxG7OZpYa4rEDtQOlk\\n'\n",
            "'PP4157.pdf https://drive.google.com/uc?export=download&id=11lk0yPY3a8BVJzumLHSP8eI2e5OYF3og\\n'\n",
            "'PP4210.pdf https://drive.google.com/uc?export=download&id=1kbQ6yx839igeX1u7ytosCc2LF7wgTK1T\\n'\n",
            "'PP4219.pdf https://drive.google.com/uc?export=download&id=1D-U2nH1OKfmjD5XFWHm0ZAPp17bFOWNV\\n'\n",
            "'PP4222.pdf https://drive.google.com/uc?export=download&id=1RVO7gX_E7Nw6jQ6t-vgzaI4S3sxC59OL\\n'\n",
            "'\\n'\n",
            "'\\x0cfilename\\n'\n",
            "'\\n'\n",
            "'url\\n'\n",
            "'\\n'\n",
            "'PP4225.pdf https://drive.google.com/uc?export=download&id=1E32J0pWhqqn8_paN0PA86KXHUm0uyZhk\\n'\n",
            "'PP4246.pdf https://drive.google.com/uc?export=download&id=1rRcxwgHggHYln4F9BviOBG-5Ut9QWQnJ\\n'\n",
            "'PP4259.pdf https://drive.google.com/uc?export=download&id=1RYgigXLG3OgyAf5lRjNoNnifQWuPPqcM\\n'\n",
            "'PP4364.pdf https://drive.google.com/uc?export=download&id=1d_v3N2VY_2UoZnMpmk77Ud6xOzgNMHX4\\n'\n",
            "'PP4421.pdf https://drive.google.com/uc?export=download&id=1Jiw-wSBxw7W2xvfBEKN725bKZONqJfz_\\n'\n",
            "'PP4429.pdf https://drive.google.com/uc?export=download&id=1GH719FnKy9O353p2OMY59gWv1tm8aDU2\\n'\n",
            "'PP4457.pdf https://drive.google.com/uc?export=download&id=1jqRzFo8XOKZwSwDWCF8tCdw3i00u09dl\\n'\n",
            "'PP4477.pdf https://drive.google.com/uc?export=download&id=1XDZnB7JXAZQ8WN_5hDts8BkA8lRWUIPW\\n'\n",
            "'PP4502.pdf https://drive.google.com/uc?export=download&id=1lURT12G3DzOQ9y1VMXFAfohtaBHoM8C5\\n'\n",
            "'PP4509.pdf https://drive.google.com/uc?export=download&id=18kuwDpnTC-H-bWMvI5GgkDXfdgX_vgfL\\n'\n",
            "'PP4512.pdf https://drive.google.com/uc?export=download&id=1GPM7v_3keBqPbWliTAyILIKcQOo82rNj\\n'\n",
            "'PP4558.pdf https://drive.google.com/uc?export=download&id=1EACgpDUpU3tVR3V5GnjFEUA0quRbE0l6\\n'\n",
            "'PP4569.pdf https://drive.google.com/uc?export=download&id=1ftoOPeVr3syN1YGrvDZ8QY8UIAZyOp2E\\n'\n",
            "'PP4608.pdf https://drive.google.com/uc?export=download&id=1vaoIFFpQAj4V42oudmWsBzLcQDd5Cwe3\\n'\n",
            "'PP4622.pdf https://drive.google.com/uc?export=download&id=1BaDmz1hny_-QwyL1qLXF_8-wqQ26BS-E\\n'\n",
            "'PP4640.pdf https://drive.google.com/uc?export=download&id=1GqWhDQUYhmScQsbZ4wNfI3uCxAQVwHu5\\n'\n",
            "'PP4654.pdf https://drive.google.com/uc?export=download&id=1TeoqQYjABOwqvw6nB8Bw6MmwDcYwg4I3\\n'\n",
            "'PP4670.pdf https://drive.google.com/uc?export=download&id=19_2lovekLeRaMlxNPGS_iNflEJ9Lwn_2\\n'\n",
            "'PP4675.pdf https://drive.google.com/uc?export=download&id=1v0mWT_nSWQXLl1m2nj-tGBxuTpRHkWK1\\n'\n",
            "'PP4689.pdf https://drive.google.com/uc?export=download&id=1J1Va50lbvCkSCvftMdGKHlmNTAdeMzJb\\n'\n",
            "'PP4701.pdf https://drive.google.com/uc?export=download&id=19rbMEb5NIQ0ylWE19rio31DYpeIFjvLQ\\n'\n",
            "'PP4732.pdf https://drive.google.com/uc?export=download&id=11sgJGBYSHAUVEDWyKPn455eBGHKMIoa5\\n'\n",
            "'PP4768.pdf https://drive.google.com/uc?export=download&id=1CwAA4HlOZ2BspBgRouns0xGvjeIiLSWn\\n'\n",
            "'PP4784.pdf https://drive.google.com/uc?export=download&id=1Ab2pW7uVqn-dzv_qs31UHOhQ9k9Z9B7c\\n'\n",
            "'PP4792.pdf https://drive.google.com/uc?export=download&id=1RLfKPCwfa6tcaUbYvqN-tE_rV1xSxDIq\\n'\n",
            "'PP4800.pdf https://drive.google.com/uc?export=download&id=1BZ84o75UXygpNRKa3wd-7M0Ya-brNWMr\\n'\n",
            "'PP4814.pdf https://drive.google.com/uc?export=download&id=1_neWLYsiKJJlkAJSAUvW5u1OygWOKp0-\\n'\n",
            "'PP4836.pdf https://drive.google.com/uc?export=download&id=1LJ9mHHvG3k7uzvgimkrKAJF8Hp4kwDnH\\n'\n",
            "'PP4863.pdf https://drive.google.com/uc?export=download&id=1kKhhEHLwyB_RFjPAl2d1Uto65tr3be_O\\n'\n",
            "'PP4873.pdf https://drive.google.com/uc?export=download&id=1AHTXAjN-G2--5kodlNtgcExAJOkCxj39\\n'\n",
            "'PP4949.pdf https://drive.google.com/uc?export=download&id=1BTTrPwTX8tcCojbwU806hADpdbcP4y00\\n'\n",
            "'PP4950.pdf https://drive.google.com/uc?export=download&id=19fmc9vRS2L0CReZBsC8LPQMkdv-E2PMk\\n'\n",
            "'PP4958.pdf https://drive.google.com/uc?export=download&id=1Zc-VIXVIPPM-edYzlATB4HS9oNn_Q9FT\\n'\n",
            "'PP4979.pdf https://drive.google.com/uc?export=download&id=1tsCrQ7ndSk1YgxyLLAhl-jRD_xfNG22U\\n'\n",
            "'PP4980.pdf https://drive.google.com/uc?export=download&id=1sz2lSVYO42q4qtaHRVJ3iR18jgDS3zHk\\n'\n",
            "'PP4988.pdf https://drive.google.com/uc?export=download&id=1gyarXp3jwdAskC2uU4R3StNxcbBVcy-S\\n'\n",
            "'PP5042.pdf https://drive.google.com/uc?export=download&id=1XcUhUmFbPExYsuCgQll2lcTrM3APgPeC\\n'\n",
            "'PP5072.pdf https://drive.google.com/uc?export=download&id=1eOnICWVdMFSORYsmYBaCHKzN3sfo1EDl\\n'\n",
            "'PP5090.pdf https://drive.google.com/uc?export=download&id=1upLJbwhvD8TMIFzK36JqJFIgbHJxSvtQ\\n'\n",
            "'PP5108.pdf https://drive.google.com/uc?export=download&id=1Syg1R8BruO5GSlvzmq4QwLmnzQxZIsaf\\n'\n",
            "'PP5131.pdf https://drive.google.com/uc?export=download&id=1_5WU5YIXodIg8KqOCGUCUwsC0kJpE2Z9\\n'\n",
            "'PP5148.pdf https://drive.google.com/uc?export=download&id=1ZY5hmMPwUzROWV0hDqkDMTAerD8BdmQw\\n'\n",
            "'PP5179.pdf https://drive.google.com/uc?export=download&id=1DNAiGeXm9AGBmkh0GJQBoJkXBJCS4hcv\\n'\n",
            "'PP5184.pdf https://drive.google.com/uc?export=download&id=1LI8RYOmhhdFVhZicWK7mR7Yb5joS-E6b\\n'\n",
            "'PP5193.pdf https://drive.google.com/uc?export=download&id=1tj90n4Ek1U3qqAdLVujoBXfgYCKqdzTG\\n'\n",
            "'PP5198.pdf https://drive.google.com/uc?export=download&id=16lvMgTsfD0alsfJ2ZJPdujM_tolHVpO-\\n'\n",
            "'PP5258.pdf https://drive.google.com/uc?export=download&id=1JWx5dnVhXNcAOBClK3PTFknMGlokSeAX\\n'\n",
            "'PP5261.pdf https://drive.google.com/uc?export=download&id=1_M5H8f97vEwJS2S_SuqTjJRRbfuBqoq5\\n'\n",
            "'\\n'\n",
            "'\\x0cfilename\\n'\n",
            "'\\n'\n",
            "'url\\n'\n",
            "'\\n'\n",
            "'PP5307.pdf https://drive.google.com/uc?export=download&id=1RgbjqxBRJ4T6EX_5EF_YwjkPo2X1UJwr\\n'\n",
            "'PP5315.pdf https://drive.google.com/uc?export=download&id=1tiSDA75v9F7RphdBggQUV6B5hW50fEb4\\n'\n",
            "'PP5325.pdf https://drive.google.com/uc?export=download&id=1UniUjMpsMoVCGVJ2Ht80RfveHmYL7Chx\\n'\n",
            "'PP5341.pdf https://drive.google.com/uc?export=download&id=1MhJfpWS_tgGKEo6gqIE3ezMY3VLgJ2Wy\\n'\n",
            "'PP5380.pdf https://drive.google.com/uc?export=download&id=1VYzzKMhRvwP0tt-4899tXiz-yW6nTgXc\\n'\n",
            "'PP5385.pdf https://drive.google.com/uc?export=download&id=1jay6prZymRLg7dumA2lsPpgYc_mt6Msp\\n'\n",
            "'PP5403.pdf https://drive.google.com/uc?export=download&id=1eA9aSpWHBclFkgZTUvgbbF7wpu9Joc_H\\n'\n",
            "'PP5425.pdf https://drive.google.com/uc?export=download&id=1n475vByU-9cHiy7EyGsIe3iucdqNXJ7-\\n'\n",
            "'PP5437.pdf https://drive.google.com/uc?export=download&id=1kY9Aqi3v-0UFytLqYHpw2snm0pWG_2RZ\\n'\n",
            "'PP5443.pdf https://drive.google.com/uc?export=download&id=1lUkHC2aT35vwXjAVkjcAu3tdu9uRBrNz\\n'\n",
            "'PP5477.pdf https://drive.google.com/uc?export=download&id=1RCnOTbctuKzsVRVypr6_kmXdIc3G1nxi\\n'\n",
            "'PP5482.pdf https://drive.google.com/uc?export=download&id=1xHmByPnJ4340Uk_i8P4km7F-FcMzc8q2\\n'\n",
            "'PP5503.pdf https://drive.google.com/uc?export=download&id=1MKzeUfLjVGuI4MIpfd8UYVAo2r7lGStv\\n'\n",
            "'PP5507.pdf https://drive.google.com/uc?export=download&id=1fNiLV72V6E21HhnAL8ttzNkaS9fInEMy\\n'\n",
            "'PP5512.pdf https://drive.google.com/uc?export=download&id=1ix1sRBS7xBym-zUjaXIgI6nYVEGRaMi_\\n'\n",
            "'PP5549.pdf https://drive.google.com/uc?export=download&id=1mgSQVxierGqOo_1ISpJozkzSMAIf9c6J\\n'\n",
            "'PP5569.pdf https://drive.google.com/uc?export=download&id=1-3m7SxSNlDG2vrrLu3Yd4zzwMgdOlXVF\\n'\n",
            "'PP5575.pdf https://drive.google.com/uc?export=download&id=1MX4ONGbYFiq2JwsfBwiqgMbCBOXibIUl\\n'\n",
            "'PP5594.pdf https://drive.google.com/uc?export=download&id=1khMpqoa763fMpjNJnQiCI-M4CZHElf8N\\n'\n",
            "'PP5618.pdf https://drive.google.com/uc?export=download&id=1TxJo8O99AeoQ9-wCO9IYzFXZsDzCfXvq\\n'\n",
            "'PP5640.pdf https://drive.google.com/uc?export=download&id=1un9_XayrJ3sLryR88wMmMLy2Yzp3dUPt\\n'\n",
            "'PP5650.pdf https://drive.google.com/uc?export=download&id=1fLejSg_vMEUICTEUZZhKaNNjml7h94tk\\n'\n",
            "'PP5662.pdf https://drive.google.com/uc?export=download&id=1MulSpFWUNoRZCkPhlGW3u9-zrNWxIOeA\\n'\n",
            "'PP5671.pdf https://drive.google.com/uc?export=download&id=1U58lj95M0FX4aZFMkoLu6XHccNOq_jrr\\n'\n",
            "'PP5696.pdf https://drive.google.com/uc?export=download&id=1HSTG2SRqzCHMBLhe48Wx3S637YbTnj-7\\n'\n",
            "'PP5751.pdf https://drive.google.com/uc?export=download&id=1dequ0Q4KpIUFFp_76LU4k9SqgsTkRgHP\\n'\n",
            "'PP5752.pdf https://drive.google.com/uc?export=download&id=1CdCRM6BIeZHtfjIMBf6nL67xlny0r1rc\\n'\n",
            "'PP5808.pdf https://drive.google.com/uc?export=download&id=1zoxYnGwNC6R0jd01coDN3dqvalDj42WJ\\n'\n",
            "'PP5813.pdf https://drive.google.com/uc?export=download&id=15r65q66RHb8dQg5ZFrfkvJIVuryvYjg-\\n'\n",
            "'PP5860.pdf https://drive.google.com/uc?export=download&id=1Lw6pAp6EOVfBZFUczC62H7sJIYvDlNEe\\n'\n",
            "'PP5876.pdf https://drive.google.com/uc?export=download&id=102lPX_Q-vV8jFIJHsx0ot9Y3zrh7vpSK\\n'\n",
            "'PP5905.pdf https://drive.google.com/uc?export=download&id=1Gp6TuKEPj__nBeJDUIDwxsCNJ7FQ0-Vl\\n'\n",
            "'PP5916.pdf https://drive.google.com/uc?export=download&id=1Dyz83RM4XgIoTDgRCd7f--uVGqyfJR_p\\n'\n",
            "'PP5975.pdf https://drive.google.com/uc?export=download&id=1pXV9HiwCMTbfVXv-DszCQq6FO5sTWaTf\\n'\n",
            "'PP5993.pdf https://drive.google.com/uc?export=download&id=1p560f-VuH6WoF5UbaLXJnBIUrxGzeAql\\n'\n",
            "'PP6017.pdf https://drive.google.com/uc?export=download&id=1xFqkmFhDMsPNoZ201tPX0iySi7YdTe_X\\n'\n",
            "'PP6018.pdf https://drive.google.com/uc?export=download&id=1VvklIIlsXk7sKe6gJXjl3txC8NoeBLuQ\\n'\n",
            "'PP6019.pdf https://drive.google.com/uc?export=download&id=1tMGNDHlgg-qg6AxyaiCyRTK1Sk1PiUrA\\n'\n",
            "'PP6020.pdf https://drive.google.com/uc?export=download&id=1fYRJz9onyFcGxFINfMN_LZsbjSObpZfo\\n'\n",
            "'PP6022.pdf https://drive.google.com/uc?export=download&id=1XlVCXG4xbB5c-Hs1LWv8H4ttOibxdNto\\n'\n",
            "'PP6030.pdf https://drive.google.com/uc?export=download&id=1SHO87ndB5DFplTITV5arjaNb4rExUSXg\\n'\n",
            "'PP6035.pdf https://drive.google.com/uc?export=download&id=18xF4uj2iHw-9jMIcU45ai1hpQDu0X1y_\\n'\n",
            "'PP6037.pdf https://drive.google.com/uc?export=download&id=1518xM_WTvF3s27BgLdCtXXwuRtSeWC-Y\\n'\n",
            "'PP6043.pdf https://drive.google.com/uc?export=download&id=1IgxZZthZXRZNAo4zuPVynAInbcSOeXcB\\n'\n",
            "'PP6045.pdf https://drive.google.com/uc?export=download&id=1tNRVm96XK7cB4Nt_4uZP0gpl54CqJbL6\\n'\n",
            "'PP6050.pdf https://drive.google.com/uc?export=download&id=1SV6fjuMGNXeppQy9qhkXTTMkKuj7lq3G\\n'\n",
            "'PP6067.pdf https://drive.google.com/uc?export=download&id=1U3NG9Qk-OYAy08BQHiF1_A6bfVeimebQ\\n'\n",
            "'PP6104.pdf https://drive.google.com/uc?export=download&id=1PI1H64uqxqMs34ZOH4rlC7Owh-pWrKE-\\n'\n",
            "'\\n'\n",
            "'\\x0cfilename\\n'\n",
            "'\\n'\n",
            "'url\\n'\n",
            "'\\n'\n",
            "'PP6126.pdf https://drive.google.com/uc?export=download&id=1gN7-R-zoJxtPLr13Lp4z39SRJXW8O9uS\\n'\n",
            "'PP6130.pdf https://drive.google.com/uc?export=download&id=1InizSWFX5E3RCMF5o5pC4lFHqsssatzy\\n'\n",
            "'PP6153.pdf https://drive.google.com/uc?export=download&id=1RmSuBTw86LE-GSYmDUFEue_YaLcJ8a2A\\n'\n",
            "'PP6203.pdf https://drive.google.com/uc?export=download&id=13XaWPZTImlf75O_K5-zRE7qPhxTP-Ksz\\n'\n",
            "'PP6207.pdf https://drive.google.com/uc?export=download&id=1qHrzyOkqxGxfk2eKnUVVH-hekzxZyT1W\\n'\n",
            "'PP6208.pdf https://drive.google.com/uc?export=download&id=1iVQ2kQ2ER8NBioVdrZE3oP-0W4_5Sj8j\\n'\n",
            "'PP6231.pdf https://drive.google.com/uc?export=download&id=1fhdOtLNueVX37nXRsf5F3w3ZfvaJlWvP\\n'\n",
            "'PP6237.pdf https://drive.google.com/uc?export=download&id=1QK7Rw9TWf-h_pdtuRhkc6_ijumt01pGo\\n'\n",
            "'PP6284.pdf https://drive.google.com/uc?export=download&id=1myAt5YZGDyLjibBG9yhuLv8Z1yhmXfSu\\n'\n",
            "'PP6299.pdf https://drive.google.com/uc?export=download&id=1mVx357cum4BWI5smt9J5T3PUt1klYQpI\\n'\n",
            "'PP6331.pdf https://drive.google.com/uc?export=download&id=1FrYiRb4o09DiEhmPiBq20fu06j7WDWwm\\n'\n",
            "'PP6333.pdf https://drive.google.com/uc?export=download&id=1pd7UQTXCDjgE-B73EY3cEKgNWo8xb55X\\n'\n",
            "'PP6354.pdf https://drive.google.com/uc?export=download&id=1QQl_dcud1-2rA0y-qFYlAmJy-sjJ3FXt\\n'\n",
            "'PP6400.pdf https://drive.google.com/uc?export=download&id=1E45dL3N1hFWbrwqjJtkGB-ziRoU3D6GV\\n'\n",
            "'PP6408.pdf https://drive.google.com/uc?export=download&id=1_7isGLkrmevRLEBn-8QmQSYgRb5E7hqY\\n'\n",
            "'PP6428.pdf https://drive.google.com/uc?export=download&id=1G98bfR25jnqLQrAr2Dnlv1n7LIDSkPHm\\n'\n",
            "'PP6430.pdf https://drive.google.com/uc?export=download&id=1KJyygv66dfaF7smeohbTCdRiZW33sN-w\\n'\n",
            "'PP6509.pdf https://drive.google.com/uc?export=download&id=1MAPqLtWnl24ZTK3M6PEWS5uw-hHGUj4l\\n'\n",
            "'PP6516.pdf https://drive.google.com/uc?export=download&id=1OavKCalHhUBz6uG5H9w3dz2CODsK_iKC\\n'\n",
            "'PP6546.pdf https://drive.google.com/uc?export=download&id=1XWD8CSYilLT4DmFwZf_fD3FFgiXbNHW-\\n'\n",
            "'PP6560.pdf https://drive.google.com/uc?export=download&id=1U7_DjtwnQn8DNXl9cLQ-Js-ltEo3DfI0\\n'\n",
            "'PP6561.pdf https://drive.google.com/uc?export=download&id=1aKmVk9uhEumkef42TZQ8jts3kAzqQOCZ\\n'\n",
            "'PP6575.pdf https://drive.google.com/uc?export=download&id=12zwFG6ncDfZT886fC3l6a4ixc8gSPU6h\\n'\n",
            "'PP6617.pdf https://drive.google.com/uc?export=download&id=1z-xS56ftJ3K2SSjHcqHrrOte5PvTaj9q\\n'\n",
            "'PP6652.pdf https://drive.google.com/uc?export=download&id=1YTOuE8XxSm5XHJm2sSpqARKyE3LaQovG\\n'\n",
            "'PP6672.pdf https://drive.google.com/uc?export=download&id=1z5zqpWs-clxlKsy_D8eHb5Uhf4VPet5y\\n'\n",
            "'PP6694.pdf https://drive.google.com/uc?export=download&id=1MjKnBJCa0Ifsz4yVmIAqRxBTLzNWJrzT\\n'\n",
            "'PP6695.pdf https://drive.google.com/uc?export=download&id=1IxD6j0pMQyIAIwS_DQPMvH1Npf-b0zmj\\n'\n",
            "'PP6703.pdf https://drive.google.com/uc?export=download&id=1kIfyMD92Q9OHlIsOz1XDzs_pc9Cm6-Rg\\n'\n",
            "'PP6716.pdf https://drive.google.com/uc?export=download&id=1PLcrpVakM8KPB-nvm2ut0B54N55k1jpE\\n'\n",
            "'PP6729.pdf https://drive.google.com/uc?export=download&id=13iWX7tUVTR-0pH6b5o0WJRx8F7-9D1vG\\n'\n",
            "'PP6767.pdf https://drive.google.com/uc?export=download&id=1yG_ao-rlWpe8WtOW5sHIfu0uL2LGzU_4\\n'\n",
            "'PP6770.pdf https://drive.google.com/uc?export=download&id=1E80QdcUZtzuiVigFpmJD-lrpYpmW6hJ-\\n'\n",
            "'PP6773.pdf https://drive.google.com/uc?export=download&id=1CLkU5-WDlWaFREe2fhka7Q6OTnTEvhMU\\n'\n",
            "'PP6789.pdf https://drive.google.com/uc?export=download&id=1JGPwnOkHJTqidA1nwQ7W77c-0eBspLgQ\\n'\n",
            "'PP6806.pdf https://drive.google.com/uc?export=download&id=1Ts0McNhBXwgc5V8YCNTU617EhKU8h-_c\\n'\n",
            "'PP6862.pdf https://drive.google.com/uc?export=download&id=1iNj-MRr9RI6DQOakp5tPdmqhld4TiS7a\\n'\n",
            "'PP6885.pdf https://drive.google.com/uc?export=download&id=1XNsO2kVeTwFAtmFFXIUFT5A1-sVjkyaS\\n'\n",
            "'PP6888.pdf https://drive.google.com/uc?export=download&id=15dan2K23nbaY-OHRYrwR2s9yfz8PXIE7\\n'\n",
            "'PP6945.pdf https://drive.google.com/uc?export=download&id=1GSfkL_oVbEp5Quxubxy0BAnE7BcveCN7\\n'\n",
            "'PP6953.pdf https://drive.google.com/uc?export=download&id=1c2l7Qia2fFG8IwCvyn3AjmECjRJj4QmW\\n'\n",
            "'PP6959.pdf https://drive.google.com/uc?export=download&id=17nOc72MVJFXnAtUx08utSZwDZA6lT9ul\\n'\n",
            "'PP6998.pdf https://drive.google.com/uc?export=download&id=1tbi8GlgGgiAz-Gzr1np7pY2B_G3oHK3N\\n'\n",
            "'PP7027.pdf https://drive.google.com/uc?export=download&id=1OJQVzcAtjTWDCMIkbp9hbirDAJ74-eVQ\\n'\n",
            "'PP7038.pdf https://drive.google.com/uc?export=download&id=1PNjIMcZXWrUKRLp5ZNjEGhkX0N0oNFUb\\n'\n",
            "'PP7039.pdf https://drive.google.com/uc?export=download&id=1zCowrBPPi_1m3UJEKJhqjwYTlqUp-jQt\\n'\n",
            "'PP7093.pdf https://drive.google.com/uc?export=download&id=1uFYi0BH7OlXn1TlBXzZmUslNLs3iKY3x\\n'\n",
            "'PP7103.pdf https://drive.google.com/uc?export=download&id=16n56wcUVYemgLyNOLBjfWoorlbZouaxX\\n'\n",
            "'PP7123.pdf https://drive.google.com/uc?export=download&id=17NOYcIVlzxAT5Eez4xysvXFdaIVdlR0M\\n'\n",
            "'\\n'\n",
            "'\\x0cfilename\\n'\n",
            "'\\n'\n",
            "'url\\n'\n",
            "'\\n'\n",
            "'PP7129.pdf https://drive.google.com/uc?export=download&id=17naDu4FD1la7UYbtboVMzFBWIfkfk9y7\\n'\n",
            "'PP7131.pdf https://drive.google.com/uc?export=download&id=13STHr6VL6MZuo5umk6miY-dshUl4T-KY\\n'\n",
            "'PP7146.pdf https://drive.google.com/uc?export=download&id=1i5OVXcxIhJZkMaNNUwrXHL4Zo2KRYY7z\\n'\n",
            "'PP7245.pdf https://drive.google.com/uc?export=download&id=1lD24SUVXafuRWaUXVA--dm6d4TIinOe_\\n'\n",
            "'PP7249.pdf https://drive.google.com/uc?export=download&id=17JiYKzdanPN17eFpx7PiI7Yg4Y2ngB8y\\n'\n",
            "'PP7262.pdf https://drive.google.com/uc?export=download&id=1GKkY4ntyUuqPmv8ifbYXBJrDzZODQ1Mp\\n'\n",
            "'\\n'\n",
            "'\\x0c'\n"
          ]
        }
      ],
      "source": [
        "#accessing the input file using file handling operations\n",
        "assessment_txt = './31940575_task2.txt'\n",
        "# opens the file in read mode\n",
        "with open(assessment_txt, 'r') as pdf_txt:\n",
        "#loop through the lines in the file\n",
        "  for line in pdf_txt:\n",
        "      # prints text contained in the file.\n",
        "      print (repr(line))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpDVyW4YlDEZ"
      },
      "source": [
        "It is noticed that file contains columns filename name and url to download the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "i0yElwywlDEa"
      },
      "outputs": [],
      "source": [
        "# creates a list to store the url to the directory\n",
        "url_list = []\n",
        "# regex expression to retrieve file name\n",
        "re_paperid = r'PP[0-9]*'\n",
        "# regex expression to to retrieve url\n",
        "re_url = r'https:.*'\n",
        "# opens file in read mode\n",
        "with open(assessment_txt, 'r') as pdf_txt:\n",
        "  # loops over the content of the file\n",
        "  for line in pdf_txt:\n",
        "      # looks for the url and file name in the lines\n",
        "      if re.findall(re_paperid, line) and re.findall(re_url, line):\n",
        "        # removes the passed pattern from the url\n",
        "        url = re.sub(r'\\n', '', re.findall(re_url, line)[0])\n",
        "        # adds the file name and url to the list\n",
        "        url_list.append([re.findall(re_paperid, line)[0], url])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block downloads the data from the url extracted. Since google drive limits 20 files to be downloaded within a 20 min span, we have halted the code by using time module of python We process the url in batch of 40 and halt execution for 29 mins."
      ],
      "metadata": {
        "id": "DAdQ2ndDFA8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0SA37sl0lDEa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "17ff0a23-533a-403d-8f08-057f38b22f20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# creates a new directory for storing the required files\\nos.makedirs(\"/content/drive/MyDrive/Asessment1/Task2\")\\n# changes the working directory to the specified location\\nos.chdir(\"/content/drive/MyDrive/Asessment1/Task2\")\\n# sets total number of files required to be downloaded\\nnumber_of_paper = len(url_list)\\n# instantiate a variable to loop over the url list\\ni = 0\\n# loops for the number of files \\nwhile i <=  number_of_paper:\\n  if i < number_of_paper:\\n    # loop 20 files at a time as google drive only supports downloading 20 files in 20 mins\\n    for url in url_list[i:i+40]:\\n      # file name of the downloaded file\\n      file_name = url[0] + \\'.pdf\\'\\n      url = url[1]\\n      # downloads the file using the url to the specified directory\\n      urllib.request.urlretrieve(url, f\\'{os.getcwd()}/{file_name}\\')\\n  else:\\n    url = url_list[number_of_paper]\\n    file_name = url[0] + \\'.pdf\\'\\n    url = url[1]\\n    # downloads the file using the url to the specified directory\\n    urllib.request.urlretrieve(url, f\\'{os.getcwd()}/{file_name}\\')\\n\\n  # halts the execution for 20 minutes\\n  time.sleep(1200)\\n  i+=20'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "'''# creates a new directory for storing the required files\n",
        "os.makedirs(\"/content/drive/MyDrive/Asessment1/Task2\")\n",
        "# changes the working directory to the specified location\n",
        "os.chdir(\"/content/drive/MyDrive/Asessment1/Task2\")\n",
        "# sets total number of files required to be downloaded\n",
        "number_of_paper = len(url_list)\n",
        "# instantiate a variable to loop over the url list\n",
        "i = 0\n",
        "# loops for the number of files \n",
        "while i <=  number_of_paper:\n",
        "  if i < number_of_paper:\n",
        "    # loop 20 files at a time as google drive only supports downloading 20 files in 20 mins\n",
        "    for url in url_list[i:i+40]:\n",
        "      # file name of the downloaded file\n",
        "      file_name = url[0] + '.pdf'\n",
        "      url = url[1]\n",
        "      # downloads the file using the url to the specified directory\n",
        "      urllib.request.urlretrieve(url, f'{os.getcwd()}/{file_name}')\n",
        "  else:\n",
        "    url = url_list[number_of_paper]\n",
        "    file_name = url[0] + '.pdf'\n",
        "    url = url[1]\n",
        "    # downloads the file using the url to the specified directory\n",
        "    urllib.request.urlretrieve(url, f'{os.getcwd()}/{file_name}')\n",
        "\n",
        "  # halts the execution for 20 minutes\n",
        "  time.sleep(1200)\n",
        "  i+=20'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ENnHWjoXlDEc"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 4.  Loading and Parsing File <a class=\"anchor\" name=\"load\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9esGMx8lDEc"
      },
      "source": [
        "In this section, all the previously downloaded files are parsed and the data is stored in a dictionary with the file name as the key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YsnRR2c4lDEc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "b81e3a73-2de7-4d40-88ef-56b9355dbb29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFighting Bandits with a New Kind of Smoothness\\n\\nAuthored by:\\n\\nAmbuj Tewari\\nJacob D. Abernethy\\n\\nChansoo Lee\\n\\nAbstract\\n\\nWe focus on the adversarial multi-armed bandit problem. The EXP3\\nalgorithm of Auer et al. (2003) was shown to have a regret bound of\\n$O(sqrt{T N log N})$, where $T$ is the time horizon and $N$ is the\\nnumber of available actions (arms). More recently, Audibert and Bubeck\\n(2009) improved the bound by a logarithmic factor via an entirely dif-\\nferent method. In the present work, we provide a new set of analysis\\ntools, using the notion of convex smoothing, to provide several novel algo-\\nrithms with optimal guarantees. First we show that regularization via the\\nTsallis entropy matches the minimax rate of Audibert and Bubeck (2009)\\nwith an even tighter constant; it also fully generalizes EXP3. Second\\nwe show that a wide class of perturbation methods lead to near-optimal\\nbandit algorithms as long as a simple condition on the perturbation dis-\\ntribution $mathcal{D}$ is met: one needs that the hazard function of\\n$mathcal{D}$ remain bounded. The Gumbel, Weibull, Frechet, Pareto,\\nand Gamma distributions all satisfy this key property; interestingly, the\\nGaussian and Uniform distributions do not.\\n\\n1 Paper Body\\n\\nWe provide a new analysis framework for the adversarial multi-armed bandit\\nproblem. Using the notion of convex smoothing, we define a novel family of\\nalgorithms with minimax optimal regret guarantees. First, we show that reg-\\nularizationp via the Tsallis entropy, which includes EXP3 as a special case,\\nmatches the O( N T ) minimax regret with a smaller constant factor. Second,\\nwe show that a p wide class of perturbation methods achieve a near-optimal\\nregret as low as O( N T log N ), as long as the perturbation distribution has a\\nbounded hazard function. For example, the Gumbel, Weibull, Frechet, Pareto,\\nand Gamma distributions all satisfy this key property and lead to near-optimal\\nalgorithms.\\n\\n1\\nIntroduction\\n\\n1\\n\\n\\n\\nThe classic multi-armed bandit (MAB) problem, generally attributed to the\\nearly work of Robbins (1952), poses a generic online decision scenario in which\\nan agent must make a sequence of choices from a fixed set of options. After\\neach decision is made, the agent receives some feedback in the form of a loss\\n(or gain) associated with her choice, but no information is provided on the out-\\ncomes of alternative options. The agent?s goal is to minimize the total loss over\\ntime, and the agent is thus faced with the balancing act of both experimenting\\nwith the menu of choices while also utilizing the data gathered in the process\\nto improve her decisions. The MAB framework is not only mathematically ele-\\ngant, but useful for a wide range of applications including medical experiments\\ndesign (Gittins, 1996), automated poker playing strategies (Van den Broeck et\\nal., 2009), and hyperparameter tuning (Pacula et al., 2012). Early MAB results\\nrelied on stochastic assumptions (e.g., IID) on the loss sequence (Auer et al.,\\n2002; Gittins et al., 2011; Lai and Robbins, 1985). As researchers began to\\nestablish non-stochastic, worst-case guarantees for sequential decision problems\\nsuch as prediction with expert advice (Littlestone and Warmuth, 1994), a nat-\\nural question arose as to whether similar guarantees were possible for the ban-\\ndit setting. The pioneering work of Auer, Cesa-Bianchi, Freund, and Schapire\\n(2003) answered this in the affirmative by showing that their algorithm EXP3\\npossesses nearly-optimal regret bounds with matching lower bounds. Attention\\nlater turned to the bandit version of online linear optimization, and several\\nassociated guarantees were published the following decade (Abernethy et al.,\\n2012; Dani and Hayes, 2006; Dani et al., 2008; Flaxman et al., 2005; McMa-\\nhan and Blum, 2004). Nearly all proposed methods have relied on a particular\\nalgorithmic blueprint; they reduce the bandit problem to the full-information\\nsetting, while using randomization to make decisions and to estimate the losses.\\nA well-studied family of algorithms for the full-information setting is Follow\\nthe Regularized Leader (FTRL), which optimizes the objective function of the\\nfollowing form: arg min L¿ x + R(x)\\n\\n(1)\\nx2K\\nwhere K is the decision set, L is (an estimate of) the cumulative loss vector,\\n\\nand R is a regularizer, a convex function with suitable curvature to stabilize the\\nobjective. The choice of regularizer R is 1\\n\\ncritical to the algorithm?s performance. For example, the EXP3 algorithm\\n(Auer, 2003) regularizes with the entropy function and achieves a nearly optimal\\nregret bound when K is the probability simplex. For a general convex set,\\nhowever, other regularizers such as self-concordant barrier functions (Abernethy\\net al., 2012) have tighter regret bounds. Another class of algorithms for the full\\ninformation setting is Follow the Perturbed Leader (FTPL) (Kalai and Vempala,\\n2005) whose foundations date back to the earliest work in adversarial online\\nlearning (Hannan, 1957). Here we choose a distribution D on RN , sample a\\nrandom vector Z ? D, and solve the following linear optimization problem arg\\nmin(L + Z)¿ x.\\n\\n(2)\\nx2K\\n\\n2\\n\\n\\n\\nFTPL is computationally simpler than FTRL due to the linearity of the\\nobjective, but it is analytically much more complex due to the randomness. For\\nevery different choice of D, an entirely new set of techniques had to be developed\\n(Devroye et al., 2013; Van Erven et al., 2014). Rakhlin et al. (2012) and Aber-\\nnethy et al. (2014) made some progress towards unifying the analysis framework.\\nTheir techniques, however, are limited to the full-information setting. In this\\npaper, we propose a new analysis framework for the multi-armed bandit problem\\nthat unifies the regularization and perturbation algorithms. The key element is\\na new kind of smoothness property, which we call differential consistency. It al-\\nlows us to generate a wide class of both optimal and near-optimal algorithms for\\nthe adversarial multi-armed bandit problem. We summarize our main results: 1.\\nWe show that regularization via the Tsallis entropy leads to the state-of-the-art\\nadversarial MAB algorithm, matching the minimax regret rate of Audibert and\\nBubeck (2009) with a tighter constant. Interestingly, our algorithm fully gener-\\nalizes EXP3. 2. We show that a wide array of well-studied noise distributions\\nlead to near-optimal regret bounds (matching those of EXP3). Furthermore,\\nour analysis reveals a strikingly simple and appealing p sufficient condition for\\nachieving O( T ) regret: the hazard rate function of the noise distribution must\\nbe bounded by a constant. We conjecture that this requirement is in fact both\\nnecessary and sufficient.\\n\\n2\\nGradient-Based Prediction Algorithms for the Multi-Armed Bandit\\nLet us now introduce the adversarial multi-armed bandit problem. On each\\n\\nround t = 1, . . . , T , a learner must choose a distribution pt 2 N over the set\\nof N available actions. The adversary (Nature) chooses a vector gt 2 [ 1, 0]N\\nof losses, the learner samples it ? pt , and plays action it . After selecting this\\naction, the learner observes only the value gt,it , and receives no information as\\nto the values gt,j for j 6= it . This limited information feedback is what makes\\nthe bandit problem much more challenging than the full-information setting in\\nwhich the entire gt is observed. The learner?s goal is to minimize the regret.\\nRegret is defined to be the difference in the realized loss and the loss of the best\\nfixed action in hindsight: RegretT := max\\n\\ni2[N ]\\nT X\\n(gt,i\\ngt,it ).\\n(3)\\nt=1\\nTo be precise, we consider the expected regret, where the expectation is\\n\\ntaken with respect to the learner?s randomization. Loss vs. Gain Note: We use\\nthe term ?loss? to refer to g, although the maximization in (3) would imply that\\ng should be thought of as a ?gain? instead. We use the former term, however,\\nas we impose the assumption that gt 2 [ 1, 0]N throughout the paper. 2.1\\n\\nThe Gradient-Based Algorithmic Template\\nOur results focus on a particular algorithmic template described in Frame-\\n\\nwork 1, which is a slight variation of the Gradient Based Prediction Algorithm\\n\\n3\\n\\n\\n\\n(GBPA) of Abernethy et al. (2014). Note that 2\\n? t , (ii) updates G ? t by the algorithm (i) maintains an unbiased estimate\\n\\nof the cumulative losses G adding a single round estimate g?t that has only one\\nnon-zero coordinate, and (iii) uses the gradient of a convex function ? as sam-\\npling distribution pt . The choice of ? is flexible but ? must be a differentiable\\nconvex function and its derivatives must always be a probability distribution.\\nFramework 1 may appear restrictive but it has served as the basis for much of\\nthe published work on adversarial MAB algorithms (Auer et al., 2003; Kujala\\nand Elomaa, 2005; Neu and Bart?ok, 2013). First, the GBPA framework es-\\nsentially encompasses all FTRL and FTPL algorithms (Abernethy et al., 2014),\\nwhich are the core techniques not only for the full information settings, but\\nalso for ? t remains an unbiased estimate of the bandit settings. Second, the\\nestimation scheme ensures that G Gt . Although there is some flexibility, any\\nunbiased estimation scheme would require some kind of inverse-probability scal-\\ning?information theory tells us that the unbiased estimates of a quantity that is\\nobserved with only probabilty p must necessarily involve fluctuations that scale\\nas O(1/p). Framework 1: Gradient-Based Prediction Alg. (GBPA) Template\\nfor Multi-Armed Bandit GBPA( ? ): ? is a differentiable convex function such\\nthat r ? 2 N and ri ? ¿ 0 for all i. ?0 = 0 Initialize G for t = 1 to T do Nature:\\nA loss vector gt 2 [ 1, 0]N is chosen by the Adversary ? t 1 ) = r t (G ?t 1)\\nSampling: Learner chooses it according to the distribution p(G Cost: Learner\\n?gains? loss gt,it gt,it Estimation: Learner ?guesses? g?t := p (G e ? ) it ?t =\\nG ?t Update: G\\n\\nit\\n1\\nt\\n1\\n+ g?t\\nLemma 2.1. Define (G) ? maxi Gi so that we can write the expected regret\\n\\nof GBPA( ? ) as PT ? ? ERegretT = (GT ) t=1 hr (Gt 1 ), gt i. Then, the\\nexpected regret of the GBPA( ? ) can be written as: ? T X ? ? T ) ? (G ?T )\\n+ ?t, G ?t ERegretT ? (0) (0) +Ei1 ,...,it 1 (G Ei [D ? (G — {z } — {z } t=1\\n— t {z overestimation penalty\\n\\nunderestimation penalty\\nwhere the expectations are over the sampling of it .\\n?\\n1 )—Gt 1 ]\\ndivergence penalty\\n}\\n, (4)\\nProof. Let ? be a valid convex function for the GBPA. Consider GBPA( ?\\n\\n) being run on the loss sequence g1 , . . . , gT . The algorithm produces a\\nsequence of estimated losses g?1 , . . . , g?T . Now consider GBPA-NE( ? ),\\nwhich is GBPA( ? ) run with the full information on the deterministic loss ? t\\ndirectly). The regret of sequence g?1 , . . . , g?T (there is no estimation step,\\nand the learner updates G this run can be written as ? T ) PT hr ? (G ? t\\n\\n4\\n\\n\\n\\n1 ), g?t i, (G t=1 ? T ) by the convexity of . Hence, it suffices to show that\\nthe GBPA-NE( ? ) has and (GT ) ? (G regret at most the righthand side of\\nEquation 4, which is a fairly well-known result in online learning literature; see,\\nfor example, (Cesa-Bianchi and Lugosi, 2006, Theorem 11.6) or (Abernethy et\\nal., 2014, Section 2). For completeness, we included the full proof in Appendix\\nA. 2.2\\n\\nA New Kind of Smoothness\\nWhat has emerged as a guiding principle throughout machine learning is that\\n\\nenforcing stability of an algorithm can often lead immediately to performance\\nguarantees?that is, small modifications of the input data should not dramatically\\nalter the output. In the context of GBPA, algorithmic stability is guaranteed\\nas long as the dervative r ? (?) is Lipschitz. Abernethy et al. (2014) explored\\na set of conditions on r2 ? (?) that lead to optimal regret guarantees for the\\nfull-information setting. Indeed, 3\\n\\nthis work discussed different settings where the regret depends on an upper\\nbound on either the nuclear norm or the operator norm of this hessian. In short,\\nregret in the full information setting relies on the smoothness of the choice of ? .\\nIn the bandit setting, however, merely a uniform bound on the magnitude of r2\\n? is insufficient to guar? t 1 + g?t , G ? t 1 ), where antee low regret; the regret\\n(Lemma 2.1) involves terms of the form D ? (G ? t 1 ). the incremental quantity\\ng?t can scale as large as the inverse of the smallest probability of p(G 2? What\\nis needed is a stronger notion of the smoothness that bounds r in correspondence\\nwith r ? , and we propose the following definition: Definition 2.2 (Differential\\nConsistency). For constants , C ¿ 0, we say that a convex function ? (?) is (\\n, C)-differentially-consistent if for all G 2 ( 1, 0]N , r2ii ? (G) ? C(ri ? (G))\\n. We now prove a useful bound that emerges from differential consistency, and\\nin the following two sections we shall show how this leads to regret guarantees.\\nTheorem 2.3. Suppose ? is ( , C)-differentially-consistent for constants C, ¿ 0.\\nThen divergence penalty at time t in Lemma 2.1 can be upper bounded as: ?t,\\nG ?t Eit [D ? (G\\n\\n?\\n1 )—Gt 1 ]\\n?C\\nN ? X i=1\\n?t ri ? ( G\\n1)\\n?\\n1\\n.\\n? to denote the cumulative estimate Proof. For the sake of clarity, we drop\\n\\nthe subscripts; we use G ? t 1 , g? to denote the marginal estimate g?t = G\\n?t G ? t 1 , and g to denote the true loss gt . G Note that by definition of\\nAlgorithm 1, g? is a sparse vector with one non-zero and non-positive ? Plus,\\nit is conditionally independent given G. ? For a fixed it , Let coordinate g?it =\\ngt,i /ri ? (G). ? + r? ? = D ? (G ? + rei , G), ? h(r) := D ? (G g /k? g k, G)\\nt ? ? ? ? 2? ? + t? ? tei ei . Now we can so that h00 (r) = (? g /k? g k)¿\\n\\n5\\n\\n\\n\\nr2 ? G g /k? g k (? g /k? g k) = e¿ G t t it r write R R ? + g?, G)— ? G] ?\\n= PN P[it = i] k?gk s h00 (r) dr ds Eit [D ? (G i=1 0 0 ? ? R R PN ? k?gk s\\ne¿ r2 ? G ? rei ei dr ds = i=1 ri ? (G) 0 0 i ? ? R R PN ? k?gk s C ri ? (G\\n? rei ) ? i=1 ri ? (G) dr ds 0 0 ? ? R R PN ? k?gk s C ri ? (G) ? ? i=1 ri ?\\n(G) dr ds 0 0 ? ? 1+ R k?gk R s PN ? = C i=1 ri ? (G) dr ds 0 0 ? ? ? 1 1\\nPN PN ? ? ? = C2 i=1 ri ? (G) gi2 ? C i=1 ri ? (G) . The first inequality is\\nby the supposition and the second inequality is due to the convexity of ? which\\nguarantees that ri is an increasing function in the i-th coordinate. Interestingly,\\nthis part of the proof critically depends on the fact that the we are in the ?loss?\\nsetting where g is always non-positive.\\n\\n3\\nA Minimax Bandit Algorithm via Tsallis Smoothing\\nThe design of a multi-armed bandit algorithm in the adversarial setting\\n\\nproved to be a challenging task. Ignoring the dependence on N for the moment,\\nwe note that the initial published work on EXP3 provided only an O(T 2/3 )\\nguarantee (Auer et al., 1995), and it was not p until the final version of this\\nwork (Auer et al., 2003) that the authors obtained the optimal O( T ) rate. For\\nthe more 4\\n\\ngeneral setting of online linear optimization, several sub-optimal rates were\\nachieved (Dani and p Hayes, 2006; Flaxman et al., 2005; McMahan and Blum,\\n2004) before the desired T was obtained (Abernethy et al., 2012; Dani et al.,\\n2008). We can view EXP3 as an instance of GBPA where the potential function\\n? (?) is the Fenchel conjugate of the P Shannon entropy. For any p 2 N , the\\n?(negative) Shannon entropy is defined as H(p) := Gi ?H(p)}. In i pi log pi , and\\nits Fenchel conjugate is H (G) = supp2 N {hp, P 1 ? fact, we have a closed-form\\nexpression for the supremum: H (G) = ? log ( i exp(?Gi )) . By inspecting\\nthe gradient of the above expression, it is easy to see that EXP3 chooses the\\ndistribution pt = rH ? (G) every round. p The tighter EXP3 bound given by\\nAuer et al. (2003) scaled p according to O( T N log N ) and the authors provided\\na matching lower bound of the form ?( T N ). It remained an open question\\nfor some time whether there exists a minimax optimal algorithm that does not\\ncontain the log term until Audibert and Bubeck (2009) proposed the Implicitly\\nNormalized Forecaster (INF). The INF is implicitly defined via a specially-\\ndesigned potential function with certain properties. It was not immediately\\nclear from this result how to define a minimax-optimal algorithm using the\\nnow-standard tools of regularization and Bregman divergence. More recently,\\nAudibert et al. (2011) improved upon Audibert and Bubeck (2009), extending\\nthe results to the combinatorial setting, and they also discovered that INF can\\nbe interpreted in terms of Bregman divergences. We give here a reformulation\\nof INF that leads to a very simple analysis in terms of our notion of differential\\nconsistency. Our reformulation can be viewed as a variation of EXP3, where\\nthe key modification is to replace the Shannon entropy function with the Tsallis\\nentropy1 for parameter 0 ¡ ? ¡ 1: X ? 1 ? S? (p) = 1 p? . i 1 ? This particular\\nfunction, proposed by Tsallis (1988), possesses a number of natural properties.\\nThe Tsallis entropy is in fact a generalization of the Shannon entropy, as one\\nobtains the latter as a special case of the former asymptotically. That is, it\\n\\n6\\n\\n\\n\\nis easy to prove the following uniform convergence: S? (?) ! H(?) as ? ! 1.\\nWe emphasize again that one can easily show that Tsallis-smoothing bandit\\nalgorithm is indeed identical to INF using the appropriate parameter mapping,\\nalthough our analysis is simpler due to the notion of differential consistency\\n(Definition 2.2). Theorem 3.1. Let ? (G) = maxp2 N {hp, Gi ?S? (p)}. Then\\nthe GBPA( ? ) has regret at most ERegret ? ?\\n\\nN1 1\\n?\\n1\\n?\\n+\\nN ?T . ??\\n(5)\\nBefore proving the theorem, we note that it immediately recovers the EXP3\\n\\nupper bound as a special 1 ? case ? ! 1. An easy application of L?H?opital?s\\nrule shows that as ? ! 1, N 1 ? 1 ! log N and p Np? /? ! N . Choosing ? =\\n(N log N )/T , we see that the right-hand side of (5) tends to 2 T N log N .\\nHowever the choice ? ! 1 is clearly not the optimal choice, as we show in the\\nfollowing statement, which directly follows from the theorem once we see that\\nN 1 ? 1 ¡ N 1 ? . q 1 2? Corollary 3.2. For any ? 2 (0, 1), if we choose ?\\n= ?N (1 ?)T then we have q NT ERegret ? 2 ?(1 ?) . p In particular, the\\nchoice of ? = 12 gives a regret of no more than 4 N T . Proof of Theorem 3.1.\\nWe will bound each penalty term in Lemma 2.1. Since S? is non-positive, the\\nunderestimation penalty is upper bounded by 0 and the overestimation penalty\\nis at most ( min S? ). The minimum of S? occurs at (1/N, . . . , 1/N ). Hence,\\n! N X ? 1 (overestimation penalty) ? 1 ? ?(N 1 ? 1). (6) ? 1 ? N i=1 1\\n\\nMore precisely, the function we give here is the negative Tsallis entropy\\naccording to its original definition.\\n\\n5\\nNow it remains to upper bound the divergence penalty with (??) 1 N ? T .\\n\\nWe observe that straight2 2 forward calculus gives r2 S? (p) = ??diag(p? , . . .\\n, p? 1 N ). Let I N (?) be the indicator function of N ; that is, I N (x) = 0 for x\\n2 N and I N (x) = 1 for x 2 / N . It is clear that ? (?) is the dual of the function\\nS? (?) + I N (?), and moreover we observe that r2 S? (p) is a sub-hessian of S?\\n(?) + I N (?) at p(G), following the setup of Penot (1994). Taking advantage\\nof Proposition 3.2 in the latter reference, we conclude that r 2 S? (p(G)) is a\\nsuper-hessian of ? = S?? at G. Hence, r2 ? (G)\\n\\n1\\n(??)\\ndiag(p21\\n?\\n(G), . . . , p2N ? (G))\\nfor any G. What we have stated, indeed, is that ? is (2 thus applying\\n\\nTheorem 2.3 gives ?t, G ?t D ? (G\\n1)\\n? (??)\\n\\n7\\n\\n\\n\\n1\\n?, (??)\\nN ? X\\n?t pi ( G\\ni=1\\n1)\\n1\\n)-differentially-consistent, and\\n?1\\n?\\n.\\nNoting that the ?1 -norm and the 1 1 ? -norm are dual to each other, we\\n\\ncan apply H?older?s inequality to any probability distribution p1 , . . . , pN to\\nobtain !1 ? N !? N N N 1 ? X X X X 1 p1i ? = p1i ? ? 1 ? pi1 ? 1? = (1)1 ?\\nN ? = N ? . i=1\\n\\ni=1\\ni=1\\nSo, the divergence penalty is at most (??)\\n4\\ni=1\\n1\\nN , which completes the proof. ?\\nNear-Optimal Bandit Algorithms via Stochastic Smoothing\\nLet D be a continuous distribution over an unbounded support with proba-\\n\\nbility density function f and cumulative density function F . Consider the GBPA(\\n? (G; D)) where ? (G; D) = E\\n\\niid\\nZ1 ,...,ZN ?D\\nmax{Gi + Zi } i\\nwhich is a stochastic smoothing of (maxi Gi ) function. Since the max\\n\\nfunction is convex, ? is also convex. By Bertsekas (1973), we can swap the\\norder of differentiation and expectation: ? (G; D) = E\\n\\niid\\nZ1 ,...,ZN ?D\\nei? , where i? = arg max{Gi + Zi }.\\n(7)\\ni=1,...,N\\nEven if the function is not differentiable everywhere, the swapping is still\\n\\npossible with any subgradient as long as they are bounded. Hence, the ties\\nbetween coordinates (which happen with probability zero anyways) can be re-\\nsolved in an arbitrary manner. It is clear that r ? is in the probability simplex,\\nand note that @? = EZ1 ,...,ZN 1{Gi + Zi ¿ Gj + Zj , 8j 6= i} @Gi ? j ? Gi ]]\\n= E ? [1 F (G ? j? = EG? j? [PZi [Zi ¿ G Gj ?\\n\\nGi )]\\n(8)\\n\\n8\\n\\n\\n\\n? j ? = maxj6=i Gj + Zj . The unbounded support condition guarantees\\nthat this partial where G derivative is non-zero for all i given any G. So, ? (G;\\nD) satisfies the requirements of Algorithm 1. 4.1\\n\\nConnection to Follow the Perturbed Leader\\nThere is a straightforward way to efficiently implement the sampling step\\n\\nof the bandit GBPA (Algorithm 1) with a stochastically smoothed function.\\nInstead of evaluating the expectation of Equation 7, we simply take a random\\nsample. In fact, this is equivalent to Follow the Perturbed Leader Algorithm\\n(FTPL) (Kalai and Vempala, 2005) for bandit settings. On the other hand, im-\\nplementing the estimation step is hard because generally there is no closed-form\\nexpression for r ? . To address this issue, Neu and Bart?ok (2013) proposed Ge-\\nometric Resampling (GR). GR uses an iterative resampling process to estimate\\nri ? . This process gives an unbiased estimate when allowed 6\\n\\nto run for an unbounded number of iterations. Even when we truncate the\\nresampling process after T M iterations, the extra regret due to the estimation\\nbias is at most N Since the eM (additive term). p p lower bound for the multi-\\narmed bandit problem is O( N T ), any choice of M = O( N T ) does not\\naffect the asymptotic regret of the algorithm. In summary, all our GBPA regret\\nbounds in this T section hold for the corresponding FTPL algorithm with an\\nextra additive N eM term in the bound. Despite the fact that perturbation-\\nbased algorithms provide a natural randomized decision strategy, they have seen\\nlittle applications mostly because they are hard to analyze. But one should\\nexpect general results to be within reach: the EXP3 algorithm, for example,\\ncan be viewed through the lens of perturbations, where the noise is distributed\\naccording to the Gumbel distribution. Indeed, an early result of Kujala and\\nElomaa (2005) showed that a near-optimal MAB strategy comes about through\\nthe use of exponentially-distributed noise, and the same perturbation strategy\\nhas more recently been utilized in the work of Neu and Bart?ok (2013) and\\nKoc?ak et al. (2014). However, a more general understanding of perturbation\\nmethods has remained elusive. For example, would Gaussian noise be sufficient\\nfor a guarantee? What about, say, the Weibull distribution? 4.2\\n\\nHazard Rate analysis\\nIn this section, we show that the performance of the GBPA( ? (G; D))\\n\\ncan be characterized by the hazard function of the smoothing distribution D.\\nThe hazard rate is a standard tool in survival analysis to describe failures due\\nto aging; for example, an increasing hazard rate models units that deteriorate\\nwith age while a decreasing hazard rate models units that improve with age\\n(a counter intuitive but not illogical possibility). To the best of our knowledge,\\nthe connection between hazard rates and design of adversarial bandit algorithms\\nhas not been made before. Definition 4.1 (Hazard rate function). Hazard rate\\nfunction of a distribution D is f (x) 1 F (x) For the rest of the section, we assume\\nthat D is unbounded in the direction of +1, so that the hazard function is well-\\ndefined everywhere. This assumption is for the clarity of presentation and can\\nbe easily removed (Appendix B). Theorem 4.2. The regret of the GBPA on ?\\n(L) = EZ ,...,Z ?D maxi {Gi + ?Zi } is at most: hD (x) :=\\n\\n1\\n\\n9\\n\\n\\n\\nn\\nh i N (sup hD ) T + ?EZ1 ,...,Zn ?D max Zi i ?\\nProof. We analyze each penalty term in Lemma 2.1. Due to the convexity\\n\\nof , the underestimation penalty is non-positive. The overestimation penalty is\\nclearly at most EZ1 ,...,Zn ?D [maxi Zi ], and Lemma 4.3 proves the N (sup\\nhD ) upper bound on the divergence penalty. It remains to provide the tuning\\nparameter ?. Suppose we scale the perturbation Z by ? ¿ 0, i.e., we add ?Zi to\\neach coordinate. It is easy to see that E[maxi=1,...,n ?Xi ] = ?E[maxi=1,...,n Xi\\n]. For the divergence penalty, let F? be the CDF of the scaled random variable.\\nObserve that F? (t) = F (t/?) and thus f? (t) = ?1 f (t/?). Hence, the hazard\\nrate scales by 1/?, which completes the proof.\\n\\nLemma 4.3. The divergence penalty of the GBPA with ? (G) = EZ?D maxi\\n{Gi + Zi } is at most N (sup hD ) each round. Proof. Recall the gradient\\nexpression in Equation 8. The i-th diagonal entry of the Hessian is: ? @ @ 2 ?\\n? ? j ? Gi )) = E ? f (G ? j ? Gi ) rii (G) = EG? j? [1 F (Gj ? Gi )] = EG? j?\\n(1 F (G Gj ? @Gi @Gi ? j ? Gi )(1 F (G ? j ? Gi ))] = EG? j? [h(G (9) ? (sup\\nh)EG? j? [1\\n\\n? j? F (G\\nGi )]\\n= (sup h)ri (G)\\n? j ? = maxj6=i {Gj + Zj } which is a random variable independent of Zi\\n\\n. We now apply where G Theorem 2.3 with = 1 and C = (sup h) to complete\\nthe proof. 7\\n\\nDistribution Gumbel(? = 1, = 1) Frechet (? ¿ 1) Weibull*( = 1, k ? 1)\\nPareto*(xm = 1, ?) Gamma(? 1, )\\n\\nsupx hD (x) 1 as x ! 0 at most 2? k at x = 0 ? at x = 0 as x ! 1\\nE[maxN i=1 Zi ] log N + 0 N 1/? (1 1/?) 1 O( k1 !(log N ) k ) ?N 1/? /(?\\n\\n1) log N +(? 1) log log N log (?) + 1 0\\np O( T N log N ) Param. N/A ? = log N k = 1 (Exponential) ? = log N =\\n\\n? = 1 (Exponential)\\np Table 1: Distributions that give O( T N log N ) regret FTPL algorithm.\\n\\nThe parameterization follows Wikipedia pages for easy lookup. We denote the\\nEuler constant (? 0.58) by 0 . Distributions marked with (*) need to be slightly\\nmodified using the conditioning trick explained in Appendix B.2. The maximum\\nof Frechet hazard function has to be computed numerically (Elsayed, 2012, p.\\n47) but elementary calculations show that it is bounded by 2? (Appendix D).\\n\\nCorollary 4.4. Follow the Perturbed Leader Algorithm with distributions in\\nTable 1 (restricted p to a certain range of parameters), combined with Geometric\\nResampling (Section 4.1) with M = NT, p has an expected regret of order O( T\\nN log N ). Table 1 provides the two terms we need to bound. We derive the third\\ncolumn of the table in Appendix C using Extreme Value Theory (Embrechts et\\nal., 1997). Note that our analysis in the proof of Lemma 4.3 is quite tight;\\nthe only place we have an inequality is when we upper bound the hazard rate.\\nIt is thus reasonable to pose the following conjecture: Conjecture 4.5. If a\\ndistribution D has a monotonically increasing hazard rate hD (x) that does not\\nconverge as x ! +1 (e.g., Gaussian), then there is a sequence of losses that will\\n\\n10\\n\\n\\n\\nincur at least a linear regret. The intuition is that if adversary keeps incurring\\na high loss for the i-th arm, then with high prob? j ? Gi will be large. So,\\nthe expectation in Equation 9 will be dominated by the hazard ability G ? j ?\\nGi . function evaluated at large values of G Acknowledgments. J. Abernethy\\nacknowledges the support of NSF under CAREER grant IIS1453304. A. Tewari\\nacknowledges the support of NSF under CAREER grant IIS-1452099.\\n\\n2 References\\n\\nJ. Abernethy, E. Hazan, and A. Rakhlin. Interior-point methods for full-\\ninformation and bandit online learning. IEEE Transactions on Information\\nTheory, 58(7):4164?4175, 2012. J. Abernethy, C. Lee, A. Sinha, and A. Tewari.\\nOnline linear optimization via smoothing. In COLT, pages 807?823, 2014. J.-Y.\\nAudibert and S. Bubeck. Minimax policies for adversarial and stochastic ban-\\ndits. In COLT, pages 217?226, 2009. J.-Y. Audibert, S. Bubeck, and G. Lugosi.\\nMinimax policies for combinatorial prediction games. In COLT, 2011. P. Auer.\\nUsing confidence bounds for exploitation-exploration trade-offs. The Journal\\nof Machine Learning Research, 3:397?422, 2003. P. Auer, N. Cesa-Bianchi, Y.\\nFreund, and R. E. Schapire. Gambling in a rigged casino: The adversarial\\nmulti-arm bandit problem. In FOCS, 1995. P. Auer, N. Cesa-Bianchi, and\\nP. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine\\nlearning, 47(2-3):235?256, 2002. P. Auer, N. Cesa-Bianchi, Y. Freund, and R.\\nE. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal of\\nComputuataion, 32(1):48?77, 2003. ISSN 0097-5397. D. P. Bertsekas. Stochas-\\ntic optimization problems with nondifferentiable cost functionals. Journal of\\nOptimization Theory and Applications, 12(2):218?231, 1973. ISSN 0022-3239.\\n8\\n\\nN. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cam-\\nbridge University Press, 2006. V. Dani and T. P. Hayes. Robbing the bandit:\\nless regret in online geometric optimization against an adaptive adversary. In\\nSODA, pages 937?943, 2006. V. Dani, T. Hayes, and S. Kakade. The price\\nof bandit information for online optimization. In NIPS, 2008. L. Devroye, G.\\nLugosi, and G. Neu. Prediction by random-walk perturbation. In Conference\\non Learning Theory, pages 460?473, 2013. E. Elsayed. Reliability Engineering.\\nWiley Series in Systems Engineering and Management. Wiley, 2012. ISBN\\n9781118309544. URL https://books.google.com/books?id= NdjF5G6tfLQC. P.\\nEmbrechts, C. Kl?uppelberg, and T. Mikosch. Modelling Extremal Events: For\\nInsurance and Finance. Applications of mathematics. Springer, 1997. ISBN\\n9783540609315. URL https: //books.google.com/books?id=BXOI2pICfJUC.\\nA. D. Flaxman, A. T. Kalai, and H. B. McMahan. Online convex optimization\\nin the bandit setting: gradient descent without a gradient. In SODA, pages\\n385?394, 2005. ISBN 0-89871-585-7. J. Gittins. Quantitative methods in the\\nplanning of pharmaceutical research. Drug Information Journal, 30(2):479?487,\\n1996. J. Gittins, K. Glazebrook, and R. Weber. Multi-armed bandit alloca-\\ntion indices. John Wiley & Sons, 2011. J. Hannan. Approximation to bayes\\n\\n11\\n\\n\\n\\nrisk in repeated play. In M. Dresher, A. W. Tucker, and P. Wolfe, editors,\\nContributions to the Theory of Games, volume III, pages 97?139, 1957. A.\\nKalai and S. Vempala. Efficient algorithms for online decision problems. Jour-\\nnal of Computer and System Sciences, 71(3):291?307, 2005. T. Koc?ak, G.\\nNeu, M. Valko, and R. Munos. Efficient learning by implicit exploration in\\nbandit problems with side observations. In NIPS, pages 613?621. Curran As-\\nsociates, Inc., 2014. J. Kujala and T. Elomaa. On following the perturbed\\nleader in the bandit setting. In Algorithmic Learning Theory, pages 371?385.\\nSpringer, 2005. T. L. Lai and H. Robbins. Asymptotically efficient adaptive\\nallocation rules. Advances in Applied Mathematics, 6(1):4?22, 1985. N. Lit-\\ntlestone and M. K. Warmuth. The weighted majority algorithm. Information\\nand Computation, 108(2):212?261, 1994. ISSN 0890-5401. H. B. McMahan\\nand A. Blum. Online geometric optimization in the bandit setting against an\\nadaptive adversary. In COLT, pages 109?123, 2004. G. Neu and G. Bart?ok.\\nAn efficient algorithm for learning with semi-bandit feedback. In Algorithmic\\nLearning Theory, pages 234?248. Springer, 2013. M. Pacula, J. Ansel, S. Ama-\\nrasinghe, and U.-M. OReilly. Hyperparameter tuning in bandit-based adaptive\\noperator selection. In Applications of Evolutionary Computation, pages 73?82.\\nSpringer, 2012. J.-P. Penot. Sub-hessians, super-hessians and conjugation.\\nNonlinear Analysis: Theory, Methods & Applications, 23(6):689?702, 1994.\\nURL http://www.sciencedirect.com/ science/article/pii/0362546X94902127. S.\\nRakhlin, O. Shamir, and K. Sridharan. Relax and randomize: From value\\nto algorithms. In Advances in Neural Information Processing Systems, pages\\n2141?2149, 2012. H. Robbins. Some aspects of the sequential design of exper-\\niments. Bull. Amer. Math. Soc., 58(5): 527?535, 1952. C. Tsallis. Possible\\ngeneralization of boltzmann-gibbs statistics. Journal of Statistical Physics, 52\\n(1-2):479?487, 1988. G. Van den Broeck, K. Driessens, and J. Ramon. Monte-\\ncarlo tree search in poker using expected reward distributions. In Advances in\\nMachine Learning, pages 367?381. Springer, 2009. T. Van Erven, W. Kotlowski,\\nand M. K. Warmuth. Follow the leader with dropout perturbations. In COLT,\\n2014. 9\\n\\n12\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# changes the working directory to the required path\n",
        "os.chdir(\"/content/drive/MyDrive\")\n",
        "# path for the files used in the assignment\n",
        "file_path = os.getcwd()+ \"/Asessment1/Task2\"\n",
        "# dictonary to store file name as key and the text contained as values\n",
        "file_dict = {}\n",
        "# loops through all the files stored in the specified directory\n",
        "for files in os.listdir(file_path):\n",
        "  # parser object to read and store data in pdf files\n",
        "  file_data = parser.from_file(file_path + '/'+ files)\n",
        "  # indexing the parser_object content to retrieve the text in the files\n",
        "  paper_content = file_data[\"content\"]\n",
        "  # storing the parsed content in dictionary\n",
        "  file_dict[re.sub(r'\\.pdf', '', files)] = paper_content\n",
        "file_dict[\"PP6030\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the extreacted data is processed to retrieve the paper body using regex.\n"
      ],
      "metadata": {
        "id": "ZJVaUZ5YGnZ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dFb39OiLLpv4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "ba5fd299-8f2c-4aed-f4fc-de809a1ee3e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Sparseness is being regarded as one of the key features in machine learning [15] and biology [16]. Sparse models are appealing since they provide an intuitive interpretation of a task at hand by singling out relevant pieces of information. Such automatic complexity reduction facilitates efficient training algorithms, and the resulting models are distinguished by small capacity. The interpretability is one of the main reasons for the popularity of sparse methods in complex domains such as computational biology, and consequently building sparse models from data has received a significant amount of recent attention. Unfortunately, sparse models do not always perform well in practice [7, 15]. This holds particularly for learning sparse linear combinations of data sources [15], an abstraction of which is known as multiple kernel learning (MKL) [10]. The data sources give rise to a set of (possibly correlated) P kernel matrices K1 , . . . , KM , and the task is to learn the optimal mixture K = m ?m Km for the problem at hand. Previous MKL research aims at finding sparse mixtures to effectively simplify the underlying data representation. For instance, [10] study semi definite matrices K 0 inducing sparseness by bounding the trace tr(K) ? c; unfortunately, the resulting semi definite optimization problems are computationally too expensive for large scale deployment. Recent approaches to MKL promote sparse solutions either by Tikhonov regularization over the mixing coefficients [25] or by incorporating an additional constraint k?k ? 1 [18, 27] requiring solutions on the standard simplex, known as Ivanov regularization. Based on the one or the other, efficient optimization strategies have been proposed for solving ‘1  norm MKL using semi infinite linear programming [21], second order approaches [6], gradient based optimization [19], and levelset methods [26]. Other variants of ‘1  norm MKL have been proposed in subsequent work addressing practical algorithms for multi class [18, 27] and multi label [9] problems. 1 Previous approaches to MKL successfully identify sparse kernel mixtures, however, the solutions found, frequently suffer fromP poor generalization performances. Often, trivial baselines using unweighted sum kernels K = m Km are observed to outperform the sparse mixture [7]. One reason for the collapse of ‘1  norm MKL is that kernels deployed in real world tasks are usually highly sophisticated and effectively capture relevant aspects of the data. In contrast, sparse approaches to MKL rely on the assumption that some kernels are irrelevant for solving the problem. Enforcing sparse mixtures in these situations may lead to degenerate models. As a remedy, we propose to sacrifice sparseness in these situations and deploy non sparse mixtures instead. After submission of this paper, we learned about a related approach, in which the sum of an ‘1 and an ‘2  regularizer are used [12]. Although non sparse solutions are not as easy to interpret, they account for (even small) contributions of all available kernels to live up to practical applications. In this paper, we first show the equivalence of the most common approaches to ‘1  norm MKL [18, 25, 27]. Our theorem allows for a generalized view of recent strands of multiple kernel learning research. Based on the detached view, we extend the MKL framework to arbitrary ‘p  norm MKL with p ? 1. Our approach can either be motivated by additionally regularizing over the mixing coefficients k?kpp , or equivalently by incorporating the constraint k?kpp ? 1. We propose two alternative optimization strategies based on Newton descent and cutting planes, respectively. Empirically, we demonstrate the efficiency and accuracy of none sparse MKL. Large scale experiments on gene start detection show a significant improvement of predictive accuracy compared to ‘1   and ‘?  norm MKL. The rest of the paper is structured as follows. We present our main contributions in Section 2, the theoretical analysis of existing approaches to MKL, our ‘p  norm MKL generalization with two highly efficient optimization strategies, and relations to ‘1  norm MKL. We report on our empirical results in Section 3 and Section 4 concludes. 2 2.1 Generalized Multiple Kernel Learning Preliminaries In the standard supervised learning setup, a labeled sample D = {(xi , yi )}i=1...,n is given, where the x lie in some input space X and y ? Y ? R. The goal is to find a hypothesis f ? H, that generalizes well on new and unseen data. Applying regularized risk minimization returns the minimizer f ? , f ? = argminf Remp (f ) + ??(f ), P n where Remp (f ) = n1 i=1 V (f (xi ), yi ) is the empirical risk of hypothesis f w.r.t. to the loss V : R ? Y ? R, regularizer ? : H ? R, and trade off parameter ? ¿ 0. In this paper, we focus on ? 22 and on linear models of the form ?(f ) = 21 kwk ? ¿ ?(x) + b, fw,b ? (x) = w (1) together with a (possibly non linear) mapping ? : X ? H to a Hilbert space H [20]. We will later make use of kernel functions K(x, x0 ) = h?(x), ?(x0 )iH to compute inner products in H. 2.2 Learning with Multiple Kernels When learning with multiple kernels, we are given M different feature map pings ?m : X ? Hm , m = 1, . . . M , each giving rise to a reproducing kernel P Km of Hm . Approaches to multiple kernel learning consider linear kernel mixtures K? = ?m Km , ?m ? 0. Compared to Eq. (1), the primal model for learning with multiple kernels is extended to ? ¿ ?? (xi ) + b = fw,b,? (x) = w ? M p X ?¿ ?m w m ?m (x) + b, (2) m=1 ??and the composite ? = where the weight vector w feature map ?? have a block structure w ? ¿ ¿ ?¿ ? (w , . . . , w ) and ? = ? ? ? . . . ? ? ? , respectively. ? 1 1 M M 1 M The idea in learning with P multiple kernels is to minimize the loss on the training data w.r.t. to optimal kernel mixture ?m Km in addition to regularizing ? to avoid overfitting. Hence, in terms 2 of regularized risk minimization, the optimization problem becomes inf ? w,b,??0 n M 1X ? X ? ? m k22 + ? kw ??[?]. V (fw,b,? (xi ), yi ) + n i=1 2 m=1 (3) ? Previous approaches to multiple kernel learning employ regularizers of the form ?(?) = ——?——1 to promote sparse kernel mixtures. By contrast, we propose to use smooth convex regularizers of the ? form ?(?) = ——?——pp , 1 ¡ p ¡ ?, allowing for non sparse solutions. The non convexity of the ? ? m. resulting optimization problem is not inherent and can be resolved by substituting wm ? ?m w 1 Furthermore, regularization parameter and sample size can be decoupled by introducing C? = n? ? ? (and adjusting ? ? ? ) which has favorable scaling properties in practice. We obtain the following convex optimization problem [5] that has also been considered by [25] for hinge loss and p = 1, ! M n M X X 1 X kwm k22 ¿ ? inf C + ?——?——pp , (4) V wm ?m (xi ) + b, yi + w,b,??0 2 ? m m=1 m=1 i=1 where we use the convention that 0t = 0 if t = 0 and ? otherwise. An alternative approach has been studied by [18, 27] (again using hinge loss and p = 1). They upper bound the value of the regularizer k?k1 ? 1 and incorporate the latter as an additional constraint into the optimization problem. For C ¿ 0, they arrive at ! M n M X X 1 X ——wm ——22 ¿ s.t. ——?——pp ? 1. (5) inf C V wm ?m (xi ) + b, yi + w,b,??0 2 ? m m=1 m=1 i=1 Our first contribution shows that both, the Tikhonov regularization in Eq. (4) and the Ivanov regularization in Eq. (5), are equivalent. ? ?) there exists C ¿ 0 such that for each optimal Theorem 1 Let be p ? 1. For each pair (C, ? ?), we have that (w? , b? , ? ? ? ) is also an optimal solution solution (w? , b? , ? ? ) of Eq. (4) using (C, of Eq. (5) using C, and vice versa, where ? ¿ 0 is some multiplicative constant. Proof. The proof is shown in the supplementary material for lack of space. Sketch of the proof: We incorporate the regularizer of (4) into the constraints and show that the resulting upper bound is tight. A variable substitution completes the proof. 2 Zien and Ong [27] showed that the MKL optimization problems by Bach et al. [3], Sonnenburg et al. [21], and their own formulation are equivalent. As a main implication of Theorem 1 and by using the result of Zien and Ong it follows that the optimization problem of Varma and Ray [25] and the ones from [3, 18, 21, 27] all are equivalent. In addition, our result shows the coupling between trade off parameter C and the regularization parameter ? in Eq. (4): tweaking one also changes the other and vice versa. Moreover, Theorem 1 implies that optimizing C in Eq. (5) implicitly searches the regularization path for the parameter ? of Eq. (4). In the remainder, we will therefore focus on the formulation in Eq. (5), as a single parameter is preferable in terms of model selection. Furthermore, we will focus on binary classification problems with Y = {?1, +1}, equipped with the hinge loss V (f (x), y) = max{0, 1 ? yf (x)}. However note, that all our results can easily be transferred to regression and multi class settings using appropriate convex loss functions and joint kernel extensions. 2.3 Non Sparse Multiple Kernel Learning We now extend the existing MKL framework to allow for non sparse kernel mixtures ?, see also [13]. Let us begin with rewriting Eq. (5) by expanding the hinge loss into the slack variables as follows M 1 X ——wm ——22 + Ck?k1 (6) min ?,w,b,? 2 m=1 ?m ! M X 0 s.t. ?i : yi wm ?m (xi ) + b ? 1 ? ?i ; ? ? 0 ; ? ? 0 ; k?kpp ? 1. m=1 3 Applying Lagrange?s theorem incorporates the constraints into the objective by introducing nonnegative Lagrangian multipliers ?, ? ? Rn , ? ? RM , ? ? R (including a pre factor of p1 for the ? Term). Resubstitution of optimality conditions w.r.t. to w, b, ?, and ? removes the dependency of the Lagrangian on the primal variables. After some additional algebra (e.g., the terms associated with ? cancel), the Lagrangian can be written as p ! p?1 M X 1 1 1 ¿ p ? 1 ? p?1 ¿ L=1 ?? ?? ? ? Qm ? , (7) p p 2 m=1 where Qm = diag(y)Km diag(y). Eq. (7) now has to be maximized w.r.t. to the dual variables ?, ?, subject to ?¿ y = 0, 0 ? ?i ? C for 1 ? i ? n, and ? ? 0. Let us ignore for a moment the non negativity ? ? 0 and solve ?L/?? = 0 for the unbounded ?. Setting the partial derivative to zero yields ?= p?1 p ! p p?1 M X 1 ¿ . ? Qm ? 2 m=1 (8) Interestingly, at optimality, we always have ? ? 0 because the quadratic term in ? is non negative. Plugging the optimal ? into Eq. (7), we arrive at the following optimization problem which solely depends on ?. max ? 1 1¿ ? ? 2 M X ?¿ Qm ? p p?1 ! p?1 p s.t. 0 ? ? ? C1; ?¿ y = 0. (9) m=1 P In the limit p ? ?, the above problem reduces to the SVM dual (with Q = m Qm ), while p ? 1 gives rise to a QCQP ‘1  MKL variant. However, optimizing the dual efficiently is difficult and will cause numerical problems in the limits p ? 1 and p ? ?. 2.4 Two Efficient Second Order Optimization Strategies Many recent MKL solvers (e.g., [19, 24, 26]) are based on wrapping linear programs around SVMs. From an optimization standpoint, our work is most closely related to the SILP approach [21] and the simpleMKL method [19, 24]. Both of these methods also aim at efficient large scale MKL algorithms. The two alternative approaches proposed for ‘p  norm MKL proposed in this paper are largely inspired by these methods and extend them in two aspects: customization to arbitrary norms and a tight coupling with minor iterations of an SVM solver, respectively. Our first strategy interleaves maximizing the Lagrangian of (6) w.r.t. ? with minor precision and Newton descent on ?. For the second strategy, we devise a semi infinite convex program, which we solve by column generation with nested sequential quadratically constrained linear programming (SQCLP). In both cases, the maximization step w.r.t. ? is performed by chunking optimization with minor iterations. The Newton approach can be applied without a common purpose QCQP solver, however, convergence can only be guaranteed for the SQCLP [8]. 2.4.1 Newton Descent For a Newton descent on the mixing coefficients, we first compute the partial derivatives 1 w¿ ?L m wm p?1 = ? + ??m 2 ??m 2 ?m — {z } and =:??m ?2L w¿ wm p?2 = m3 + (p ? 1)??m 2 ? ?m ?m — {z } =:hm of the original Lagrangian. Fortunately, the Hessian H is diagonal, i.e. given by H = diag(h). The m th element sm of the corresponding Netwon step, defined as s := ?H ?1 ?? , is thus computed by sm = 1 2 p+2 2 ?m ——w m —— ? ??m p+1 , ——wm ——2 + (p ? 1)??m 4 where ? is defined in Eq. (8). However, a Newton step ? t+1 = ? t + s might lead to non positive ?. To avoid this awkward situation, we take the Newton steps in the space of log(?) by adjusting the derivatives according to the chain rule. We obtain t+1 log(?m ) = t log(?m )? t ?t?m /?m , t )2 ? ?t /(? t )2 htm /(?m m ?m (10) which corresponds to multiplicative update of ?: t+1 ?m = t ?m ? exp t ?t?m ?m ?t?m ? htm ! . (11) Furthermore we additionally enhance the Newton step by a line search. 2.4.2 Cutting Planes In order to obtain an alternative optimization strategy, we fix ? and build the partial Lagrangian w.r.t. all other primal variables w, b, ?. The derivation is analogous to [18, 27] and we omit details for lack of space. The resulting dual problem is a min max problem of the form min max ? ? M X 1 ?m Qm ? 1¿ ? ? ?¿ 2 m=1 s.t. 0 ? ? ? C1; y ¿ ? = 0; ? ? 0; k?kpp ? 1. The above optimization problem is a saddle point problem and can be solved by alternating ? and ? optimization step. While the former can simply be carried out by a support vector machine for a fixed mixture ?, the latter has been optimized for p = 1 by reduced gradients [18]. We take a different approach and translate the min max problem into an equivalent semi infinite program (SIP) as follows. Denote the value of the target function by t(?, ?) and suppose ?? is optimal. Then, according to the max min inequality [5], we have t(?? , ?) ? t(?, ?) for all ? and ?. Hence, we can equivalently minimize an upper bound ? on the optimal value and arrive at min ? ?,? s.t. M X 1 ?m Qm ? ? ? 1¿ ? ? ?¿ 2 m=1 (12) for all ? ? Rn with 0 ? ? ? C1, and y ¿ ? = 0 as well as k?kpp ? 1 and ? ? 0. [21] optimize the above SIP for p ? 1 with interleaving cutting plane algorithms. The solution of a quadratic program (here the regular SVM) generates the most strongly violated constraint for the actual mixture ?. The optimal (? ? , ?) is then identified by solving a linear program with respect to the set of active constraints. The optimal mixture is then used for computing a new constraint and so on. Unfortunately, for p ¿ 1, a non linearity is introduced by requiring k?kpp ? 1 and such constraint is unlikely to be found in standard optimization toolboxes that often handle only linear and quadratic constraints. As a remedy, we propose to approximate k?kpp ? 1 by sequential second order Taylor expansion of the form ——?——pp ? 1 + M M p(p ? 3) X p(p ? 1) X ?p?2 2 ? p(p ? 2)(??m )p?1 ?m + ?m ?m , 2 2 m=1 m=1 p where ? p is defined element wise, that is ? p := (?1p , ..., ?M ). The sequence (? 0 , ? 1 , ? ? ? ) is initialized with a uniform mixture satisfying k? 0 kpp = 1 as a starting point. Successively ? t+1 is computed ? = ? t . Note that the quadratic term in the approximation is diagonal wherefore the subseusing ? quent quadratically constrained problem can be solved efficiently. Finally note, that this approach can be further sped up by an additional projection onto the level sets in the ? optimization phase similar to [26]. In our case, the level set projection is a convex quadratic problem with ‘p  norm constraints and can again be approximated by successive second order Taylor expansions. 5 2 10 2 10 time in seconds time in seconds 1 10 0 10 ?1 1 10 0 10 10 ?2 10 ?1 2 3 10 10 sample size 10 1 10 2 10 number of kernels 3 10 Figure 1: Execution times of SVM Training, ‘p  norm MKL based on inter leaved optimization via the Newton, the cutting plane algorithm (CPA), and the SimpleMKL wrapper. (left) Training using fixed number of 50 kernels varying training set size. (right) For 500 examples and varying numbers of kernels. Our proposed Newton and CPA obtain speedups of over an order of magnitude. Notice the tiny error bars. 3 Computational Experiments In this section we study non sparse MKL in terms of efficiency and accuracy.1 We apply the method of [21] for ‘1  norm results as it is contained as a special case of our cuttingPplane strategy. We write ‘?  norm MKL for a regular SVM with the unweighted sum kernel K = m Km . 3.1 Execution Time We demonstrate the efficiency of our implementations of non sparse MKL. We experiment on the MNIST data set where the task is to separate odd vs. even digits. We compare our ‘p  norm MKL with two methods for ‘1  norm MKL, simpleMKL [19] and SILP based chunking [21], and to SVMs using the unweighted sum kernel (‘?  norm MKL) as additional baseline. We optimize all methods up to a precision of 10?3 for the outer SVM ? and 10?5 for the ?inner? SIP precision and computed relative duality gaps. To provide a fair stopping criterion to simpleMKL, we set the stopping criterion of simpleMKL to the relative duality gap of its ‘1  norm counterpart. This way, the deviations of relative objective values of ‘1  norm MKL variants are guaranteed to be smaller than 10?4 . SVM trade off parameters are set to C = 1 for all methods. Figure 1 (left) displays the results for varying sample sizes and 50 precomputed Gaussian kernels with different bandwidths. Error bars indicate standard error over 5 repetitions. Unsurprisingly, the SVM with the unweighted sum kernel is the fastest method. Non sparse MKL scales similarly as ‘1  norm chunking; the Newton strategy (Section 2.4.1) is slightly faster than the cutting plane variant (Section 2.4.2) that needs additional Taylor expansions within each ? step. SimpleMKL suffers from training an SVM to full precision for each gradient evaluation and performs worst.2 Figure 1 (right) shows the results for varying the number of precomputed RBF kernels for a fixed sample size of 500. The SVM with the unweighted sum kernel is hardly affected by this setup and performs constantly. The ‘1  norm MKL by [21] handles the increasing number of kernels best and is the fastest MKL method. Non sparse approaches to MKL show reasonable runtimes, the Newtonbased ‘p  norm MKL being again slightly faster than its peer. Simple MKL performs again worst. Overall, our proposed Newton and cutting plane based optimization strategies achieve a speedup of often more than one order of magnitude. 3.2 Protein Subcellular Localization The prediction of the subcellular localization of proteins is one of the rare empirical success stories of ‘1  norm regularized MKL [17, 27]: after defining 69 kernels that capture diverse aspects of 1 2 Available at http://www.shogun toolbox.org/ SimpleMKL could not be evaluated for 2000 instances (ran out of memory on a 4GB machine). 6 Table 1: Results for Protein Subcellular Localization ‘p  norm 1   MCC [%] 1 9.13 32/31 9.12 16/15 9.64 8/7 9.84 4/3 9.56 2 10.18 4 10.08 ? 10.41 protein sequences, ‘1  norm MKL could raise the predictive accuracy signif icantly above that of the unweighted sum of kernels (thereby also improving on established prediction systems for this problem). Here we investigate the performance of non sparse MKL. We download the kernel matrices of the dataset plant3 and follow the experimental setup of [17] with the following changes: instead of a genuine multiclass SVM, we use the 1 vs rest decomposition; instead of performing cross validation for model selection, we report results for the best models, as we are only interested in the relative performance of the MKL regularizers. Specifically, for each C ? {1/32, 1/8, 1/2, 1, 2, 4, 8, 32, 128}, we compute the average Mathews correlation coefficient (MCC) on the test data. For each norm, the best average MCC is recorded. Table 1 shows the averages over several splits of the data. The results indicate that, indeed, with proper choice of a non sparse regularizer, the accuracy of ‘1  norm can be recovered. This is remarkable, as this dataset is particular in that it fullfills the rare condition that ‘1  norm MKL performs better than ‘?  norm MKL. In other words, selecting these data may imply a bias towards ‘1  norm. Nevertheless our novel non sparse MKL can keep up with this, essentially by approximating ‘1  norm. 3.3 Gene Start Recognition This experiment aims at detecting transcription start sites (TSS) of RNA Polymerase II binding genes in genomic DNA sequences. Accurate detection of the transcription start site is crucial to identify genes and their promoter regions and can be regarded as a first step in deciphering the key regulatory elements in the promoter region that determine transcription. For our experiments we use the dataset from [22] which contains a curated set of 8,508 TSS annotated genes built from dbTSS version 4 [23] and refseq genes. These are translated into positive training instances by extracting windows of size [?1000, +1000] around the TSS. Similar to [4], 85,042 negative instances are generated from the interior of the gene using the same window size. Following [22], we employ five different kernels representing the TSS signal (weighted degree with shift), the promoter (spectrum), the 1st exon (spectrum), angles (linear), and energies (linear). Optimal kernel parameters are determined by model selection in [22]. Every kernel is normalized such that all points have unit length in feature space. We reserve 13,000 and 20,000 randomly drawn instances for holdout and test sets, respectively, and use the remaining 60,000 as the training pool. Figure 2 shows test errors for varying training set sizes drawn from the pool; training sets of the same size are disjoint. Error bars indicate standard errors of repetitions for small training set sizes. Regardless of the sample size, ‘1  MKL is significantly outperformed by the sum kernel. On the contrary, non sparse MKL significantly achieves higher AUC values than the ‘?  MKL for sample sizes up to 20k. The scenario is well suited for ‘2  norm MKL which performs best. Finally, for 60k training instances, all methods but ‘1  norm MKL yield the same performance. Again, the superior performance of non sparse MKL is remarkable, and of significance for the application domain: the method using the unweighted sum of kernels [22] has recently been confirmed to be the leading in a comparison of 19 state of the art promoter prediction programs [1], and our experiments suggest that its accuracy can be further elevated by non sparse MKL. 4 Conclusion and Discussion We presented an efficient and accurate approach to non sparse multiple ker nel learning and showed that our ‘p  norm MKL can be motivated as Tikhonov and Ivanov regularization of the mixing coefficients, respectively. Applied to previous MKL research, our result allows for a unified view as so far seemingly different approaches turned out to be equivalent. Furthermore, we devised two efficient approaches to non sparse multiple kernel learning for arbitrary ‘p norms, p ¿ 1. The resulting 3 from http://www.fml.tuebingen.mpg.de/raetsch/suppl/protsubloc/ 7 4?norm unw.?sum n=5k 1?norm 4/3?norm 2?norm 0.93 n=20k 0.91 1?norm MKL 4/3?norm MKL 2?norm MKL 4?norm MKL SVM 0.9 0.89 0.88 0 10K 20K 30K 40K 50K n=60k AUC 0.92 60K sample size Figure 2: Left: Area under ROC curve (AUC) on test data for TSS recog nition as a function of the training set size. Notice the tiny bars indicating standard errors w.r.t. repetitions on disjoint training sets. Right: Corresponding kernel mixtures. For p = 1 consistent sparse solutios are obtained while the optimal p = 2 distributes wheights on the weighted degree and the 2 spectrum kernels in good agreement to [22]. optimization strategies are based on semi infinite programming and Newton descent, both interleaved with chunking based SVM training. Execution times moreover revealed that our interleaved optimization vastly outperforms commonly used wrapper approaches. We would like to note that there is a certain preference/obsession for sparse models in the scientific community due to various reasons. The present paper, however, shows clearly that sparsity by itself is not the ultimate virtue to be strived for. Rather on the contrary: non sparse model may improve quite impressively over sparse ones. The reason for this is less obvious and its theoretical exploration goes well beyond the scope of its submissions. We remark nevertheless that some interesting asymptotic results exist that show model selection consistency of sparse MKL (or the closely related group lasso) [2, 14], in other words in the limit n ? ? MKL is guaranteed to find the correct subset of kernels. However, also the rate of convergence to the true estimator needs to be considered, thus ? we conjecture that the rate slower than n which is common to sparse estimators [11] may be one of the reasons for finding excellent (nonasymptotic) results in non sparse MKL. In addition to the convergence rate the variance properties of MKL estimators may play an important role to elucidate the performance seen in our various simulation experiments. Intuitively speaking, we observe clearly that in some cases all features even though they may contain redundant information are to be kept, since putting their contributions to zero does not improve prediction. I.e. all of them are informative to our MKL models. Note however that this result is also class specific, i.e. for some classes we may sparsify. Cross validation based model building that includes the choice of p will however inevitably tell us which classes should be treated sparse and which non sparse. Large scale experiments on TSS recognition even raised the bar for ‘1  norm MKL: non sparse MKL proved consistently better than its sparse counterparts which were outperformed by an unweightedsum kernel. This exemplifies how the unprecedented combination of accuracy and scalability of our MKL approach and methods paves the way for progress in other real world applications of machine learning. Authors? Contributions The authors contributed in the following way: MK and UB had the initial idea. MK, UB, SS, and AZ each contributed substantially to both mathematical modelling, design and implementation of algorithms, conception and execution of experiments, and writing of the manuscript. PL had some shares in the initial phase and KRM contributed to the text. Most of the work was done at previous affiliations of several authors: Fraunhofer Institute FIRST (Berlin), Technical University Berlin, and the Friedrich Miescher Laboratory (T?ubingen). Acknowledgments This work was supported in part by the German BMBF grant REMIND (FKZ 01 IS07007A) and by the European Community under the PASCAL2 Network of Excellence (ICT 216886). '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# dictionary to store the paper body as the values and file name as the key\n",
        "paper_content_dict = {}\n",
        "# loops through the file dictionary\n",
        "for keys, values in file_dict.items():\n",
        "  # regex function to match the pattern to the string\n",
        "  match = re.search(r\"((Paper Body?)(.*)(References?))\",values,re.DOTALL)\n",
        "  # retrives the body content of the paper\n",
        "  paper_body = match.group(3)\n",
        "  # removes the page numbers and the empty spaces before the page num\n",
        "  paper_body = re.sub(r'\\n\\n\\d*\\n\\n\\n\\n',' ',paper_body)\n",
        "  # concatenates the hyphenated split words at the end of each line\n",
        "  paper_body = re.sub(r'-\\n','',paper_body)\n",
        "  # removes - from the hyphented words and replaces it with empty space\n",
        "  paper_body = re.sub(r'-',' ',paper_body)\n",
        "  # removes the extra empty spaces between different lines of the paper body\n",
        "  paper_body = re.sub(r'\\n+',' ',paper_body)\n",
        "  paper_body=re.sub(r'ﬁ','fi',paper_body)\n",
        "  # removes the html tags from the paper bdy\n",
        "  paper_body=re.sub(r'&#\\d','',paper_body)\n",
        "  paper_body=re.sub(r'.\\s2\\s$','',paper_body)\n",
        "  # stores the paper body content as value in the dictionary\n",
        "  paper_content_dict[keys] = paper_body\n",
        "paper_content_dict[\"PP3675\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The regex pattern looks matches the data in three group:\n",
        "\n",
        "\n",
        "1.   Paper Body: Group 2\n",
        "2.   Anything in between i.e. the paper body content: Group 3\n",
        "3.   Reference: Group 4\n",
        "\n",
        "The whole pattern is stored as Group 4. The paper body content is fetched by using group function.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "S3IeTvT20jVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then the paper body is processed for removing all the unwanted tags and spaces."
      ],
      "metadata": {
        "id": "ldv4Iex5HE1o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uh9oUXAlDEd"
      },
      "source": [
        "Let's examine the dictionary generated. For counting the total number of papers extracted. Using length function it is clear we have to process 200 paper that have been downloaded. The content dictionary stores tghe paper id as the key and the content as the value."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(paper_content_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohjMW8582hG7",
        "outputId": "aa59024d-6c95-4171-cf79-a2442005b1c4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "l4NrtBmT4Ens",
        "outputId": "fff62218-df63-4a3c-f7a4-2f309ad70dae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Sparseness is being regarded as one of the key features in machine learning [15] and biology [16]. Sparse models are appealing since they provide an intuitive interpretation of a task at hand by singling out relevant pieces of information. Such automatic complexity reduction facilitates efficient training algorithms, and the resulting models are distinguished by small capacity. The interpretability is one of the main reasons for the popularity of sparse methods in complex domains such as computational biology, and consequently building sparse models from data has received a significant amount of recent attention. Unfortunately, sparse models do not always perform well in practice [7, 15]. This holds particularly for learning sparse linear combinations of data sources [15], an abstraction of which is known as multiple kernel learning (MKL) [10]. The data sources give rise to a set of (possibly correlated) P kernel matrices K1 , . . . , KM , and the task is to learn the optimal mixture K = m ?m Km for the problem at hand. Previous MKL research aims at finding sparse mixtures to effectively simplify the underlying data representation. For instance, [10] study semi definite matrices K 0 inducing sparseness by bounding the trace tr(K) ? c; unfortunately, the resulting semi definite optimization problems are computationally too expensive for large scale deployment. Recent approaches to MKL promote sparse solutions either by Tikhonov regularization over the mixing coefficients [25] or by incorporating an additional constraint k?k ? 1 [18, 27] requiring solutions on the standard simplex, known as Ivanov regularization. Based on the one or the other, efficient optimization strategies have been proposed for solving ‘1  norm MKL using semi infinite linear programming [21], second order approaches [6], gradient based optimization [19], and levelset methods [26]. Other variants of ‘1  norm MKL have been proposed in subsequent work addressing practical algorithms for multi class [18, 27] and multi label [9] problems. 1 Previous approaches to MKL successfully identify sparse kernel mixtures, however, the solutions found, frequently suffer fromP poor generalization performances. Often, trivial baselines using unweighted sum kernels K = m Km are observed to outperform the sparse mixture [7]. One reason for the collapse of ‘1  norm MKL is that kernels deployed in real world tasks are usually highly sophisticated and effectively capture relevant aspects of the data. In contrast, sparse approaches to MKL rely on the assumption that some kernels are irrelevant for solving the problem. Enforcing sparse mixtures in these situations may lead to degenerate models. As a remedy, we propose to sacrifice sparseness in these situations and deploy non sparse mixtures instead. After submission of this paper, we learned about a related approach, in which the sum of an ‘1 and an ‘2  regularizer are used [12]. Although non sparse solutions are not as easy to interpret, they account for (even small) contributions of all available kernels to live up to practical applications. In this paper, we first show the equivalence of the most common approaches to ‘1  norm MKL [18, 25, 27]. Our theorem allows for a generalized view of recent strands of multiple kernel learning research. Based on the detached view, we extend the MKL framework to arbitrary ‘p  norm MKL with p ? 1. Our approach can either be motivated by additionally regularizing over the mixing coefficients k?kpp , or equivalently by incorporating the constraint k?kpp ? 1. We propose two alternative optimization strategies based on Newton descent and cutting planes, respectively. Empirically, we demonstrate the efficiency and accuracy of none sparse MKL. Large scale experiments on gene start detection show a significant improvement of predictive accuracy compared to ‘1   and ‘?  norm MKL. The rest of the paper is structured as follows. We present our main contributions in Section 2, the theoretical analysis of existing approaches to MKL, our ‘p  norm MKL generalization with two highly efficient optimization strategies, and relations to ‘1  norm MKL. We report on our empirical results in Section 3 and Section 4 concludes. 2 2.1 Generalized Multiple Kernel Learning Preliminaries In the standard supervised learning setup, a labeled sample D = {(xi , yi )}i=1...,n is given, where the x lie in some input space X and y ? Y ? R. The goal is to find a hypothesis f ? H, that generalizes well on new and unseen data. Applying regularized risk minimization returns the minimizer f ? , f ? = argminf Remp (f ) + ??(f ), P n where Remp (f ) = n1 i=1 V (f (xi ), yi ) is the empirical risk of hypothesis f w.r.t. to the loss V : R ? Y ? R, regularizer ? : H ? R, and trade off parameter ? ¿ 0. In this paper, we focus on ? 22 and on linear models of the form ?(f ) = 21 kwk ? ¿ ?(x) + b, fw,b ? (x) = w (1) together with a (possibly non linear) mapping ? : X ? H to a Hilbert space H [20]. We will later make use of kernel functions K(x, x0 ) = h?(x), ?(x0 )iH to compute inner products in H. 2.2 Learning with Multiple Kernels When learning with multiple kernels, we are given M different feature map pings ?m : X ? Hm , m = 1, . . . M , each giving rise to a reproducing kernel P Km of Hm . Approaches to multiple kernel learning consider linear kernel mixtures K? = ?m Km , ?m ? 0. Compared to Eq. (1), the primal model for learning with multiple kernels is extended to ? ¿ ?? (xi ) + b = fw,b,? (x) = w ? M p X ?¿ ?m w m ?m (x) + b, (2) m=1 ??and the composite ? = where the weight vector w feature map ?? have a block structure w ? ¿ ¿ ?¿ ? (w , . . . , w ) and ? = ? ? ? . . . ? ? ? , respectively. ? 1 1 M M 1 M The idea in learning with P multiple kernels is to minimize the loss on the training data w.r.t. to optimal kernel mixture ?m Km in addition to regularizing ? to avoid overfitting. Hence, in terms 2 of regularized risk minimization, the optimization problem becomes inf ? w,b,??0 n M 1X ? X ? ? m k22 + ? kw ??[?]. V (fw,b,? (xi ), yi ) + n i=1 2 m=1 (3) ? Previous approaches to multiple kernel learning employ regularizers of the form ?(?) = ——?——1 to promote sparse kernel mixtures. By contrast, we propose to use smooth convex regularizers of the ? form ?(?) = ——?——pp , 1 ¡ p ¡ ?, allowing for non sparse solutions. The non convexity of the ? ? m. resulting optimization problem is not inherent and can be resolved by substituting wm ? ?m w 1 Furthermore, regularization parameter and sample size can be decoupled by introducing C? = n? ? ? (and adjusting ? ? ? ) which has favorable scaling properties in practice. We obtain the following convex optimization problem [5] that has also been considered by [25] for hinge loss and p = 1, ! M n M X X 1 X kwm k22 ¿ ? inf C + ?——?——pp , (4) V wm ?m (xi ) + b, yi + w,b,??0 2 ? m m=1 m=1 i=1 where we use the convention that 0t = 0 if t = 0 and ? otherwise. An alternative approach has been studied by [18, 27] (again using hinge loss and p = 1). They upper bound the value of the regularizer k?k1 ? 1 and incorporate the latter as an additional constraint into the optimization problem. For C ¿ 0, they arrive at ! M n M X X 1 X ——wm ——22 ¿ s.t. ——?——pp ? 1. (5) inf C V wm ?m (xi ) + b, yi + w,b,??0 2 ? m m=1 m=1 i=1 Our first contribution shows that both, the Tikhonov regularization in Eq. (4) and the Ivanov regularization in Eq. (5), are equivalent. ? ?) there exists C ¿ 0 such that for each optimal Theorem 1 Let be p ? 1. For each pair (C, ? ?), we have that (w? , b? , ? ? ? ) is also an optimal solution solution (w? , b? , ? ? ) of Eq. (4) using (C, of Eq. (5) using C, and vice versa, where ? ¿ 0 is some multiplicative constant. Proof. The proof is shown in the supplementary material for lack of space. Sketch of the proof: We incorporate the regularizer of (4) into the constraints and show that the resulting upper bound is tight. A variable substitution completes the proof. 2 Zien and Ong [27] showed that the MKL optimization problems by Bach et al. [3], Sonnenburg et al. [21], and their own formulation are equivalent. As a main implication of Theorem 1 and by using the result of Zien and Ong it follows that the optimization problem of Varma and Ray [25] and the ones from [3, 18, 21, 27] all are equivalent. In addition, our result shows the coupling between trade off parameter C and the regularization parameter ? in Eq. (4): tweaking one also changes the other and vice versa. Moreover, Theorem 1 implies that optimizing C in Eq. (5) implicitly searches the regularization path for the parameter ? of Eq. (4). In the remainder, we will therefore focus on the formulation in Eq. (5), as a single parameter is preferable in terms of model selection. Furthermore, we will focus on binary classification problems with Y = {?1, +1}, equipped with the hinge loss V (f (x), y) = max{0, 1 ? yf (x)}. However note, that all our results can easily be transferred to regression and multi class settings using appropriate convex loss functions and joint kernel extensions. 2.3 Non Sparse Multiple Kernel Learning We now extend the existing MKL framework to allow for non sparse kernel mixtures ?, see also [13]. Let us begin with rewriting Eq. (5) by expanding the hinge loss into the slack variables as follows M 1 X ——wm ——22 + Ck?k1 (6) min ?,w,b,? 2 m=1 ?m ! M X 0 s.t. ?i : yi wm ?m (xi ) + b ? 1 ? ?i ; ? ? 0 ; ? ? 0 ; k?kpp ? 1. m=1 3 Applying Lagrange?s theorem incorporates the constraints into the objective by introducing nonnegative Lagrangian multipliers ?, ? ? Rn , ? ? RM , ? ? R (including a pre factor of p1 for the ? Term). Resubstitution of optimality conditions w.r.t. to w, b, ?, and ? removes the dependency of the Lagrangian on the primal variables. After some additional algebra (e.g., the terms associated with ? cancel), the Lagrangian can be written as p ! p?1 M X 1 1 1 ¿ p ? 1 ? p?1 ¿ L=1 ?? ?? ? ? Qm ? , (7) p p 2 m=1 where Qm = diag(y)Km diag(y). Eq. (7) now has to be maximized w.r.t. to the dual variables ?, ?, subject to ?¿ y = 0, 0 ? ?i ? C for 1 ? i ? n, and ? ? 0. Let us ignore for a moment the non negativity ? ? 0 and solve ?L/?? = 0 for the unbounded ?. Setting the partial derivative to zero yields ?= p?1 p ! p p?1 M X 1 ¿ . ? Qm ? 2 m=1 (8) Interestingly, at optimality, we always have ? ? 0 because the quadratic term in ? is non negative. Plugging the optimal ? into Eq. (7), we arrive at the following optimization problem which solely depends on ?. max ? 1 1¿ ? ? 2 M X ?¿ Qm ? p p?1 ! p?1 p s.t. 0 ? ? ? C1; ?¿ y = 0. (9) m=1 P In the limit p ? ?, the above problem reduces to the SVM dual (with Q = m Qm ), while p ? 1 gives rise to a QCQP ‘1  MKL variant. However, optimizing the dual efficiently is difficult and will cause numerical problems in the limits p ? 1 and p ? ?. 2.4 Two Efficient Second Order Optimization Strategies Many recent MKL solvers (e.g., [19, 24, 26]) are based on wrapping linear programs around SVMs. From an optimization standpoint, our work is most closely related to the SILP approach [21] and the simpleMKL method [19, 24]. Both of these methods also aim at efficient large scale MKL algorithms. The two alternative approaches proposed for ‘p  norm MKL proposed in this paper are largely inspired by these methods and extend them in two aspects: customization to arbitrary norms and a tight coupling with minor iterations of an SVM solver, respectively. Our first strategy interleaves maximizing the Lagrangian of (6) w.r.t. ? with minor precision and Newton descent on ?. For the second strategy, we devise a semi infinite convex program, which we solve by column generation with nested sequential quadratically constrained linear programming (SQCLP). In both cases, the maximization step w.r.t. ? is performed by chunking optimization with minor iterations. The Newton approach can be applied without a common purpose QCQP solver, however, convergence can only be guaranteed for the SQCLP [8]. 2.4.1 Newton Descent For a Newton descent on the mixing coefficients, we first compute the partial derivatives 1 w¿ ?L m wm p?1 = ? + ??m 2 ??m 2 ?m — {z } and =:??m ?2L w¿ wm p?2 = m3 + (p ? 1)??m 2 ? ?m ?m — {z } =:hm of the original Lagrangian. Fortunately, the Hessian H is diagonal, i.e. given by H = diag(h). The m th element sm of the corresponding Netwon step, defined as s := ?H ?1 ?? , is thus computed by sm = 1 2 p+2 2 ?m ——w m —— ? ??m p+1 , ——wm ——2 + (p ? 1)??m 4 where ? is defined in Eq. (8). However, a Newton step ? t+1 = ? t + s might lead to non positive ?. To avoid this awkward situation, we take the Newton steps in the space of log(?) by adjusting the derivatives according to the chain rule. We obtain t+1 log(?m ) = t log(?m )? t ?t?m /?m , t )2 ? ?t /(? t )2 htm /(?m m ?m (10) which corresponds to multiplicative update of ?: t+1 ?m = t ?m ? exp t ?t?m ?m ?t?m ? htm ! . (11) Furthermore we additionally enhance the Newton step by a line search. 2.4.2 Cutting Planes In order to obtain an alternative optimization strategy, we fix ? and build the partial Lagrangian w.r.t. all other primal variables w, b, ?. The derivation is analogous to [18, 27] and we omit details for lack of space. The resulting dual problem is a min max problem of the form min max ? ? M X 1 ?m Qm ? 1¿ ? ? ?¿ 2 m=1 s.t. 0 ? ? ? C1; y ¿ ? = 0; ? ? 0; k?kpp ? 1. The above optimization problem is a saddle point problem and can be solved by alternating ? and ? optimization step. While the former can simply be carried out by a support vector machine for a fixed mixture ?, the latter has been optimized for p = 1 by reduced gradients [18]. We take a different approach and translate the min max problem into an equivalent semi infinite program (SIP) as follows. Denote the value of the target function by t(?, ?) and suppose ?? is optimal. Then, according to the max min inequality [5], we have t(?? , ?) ? t(?, ?) for all ? and ?. Hence, we can equivalently minimize an upper bound ? on the optimal value and arrive at min ? ?,? s.t. M X 1 ?m Qm ? ? ? 1¿ ? ? ?¿ 2 m=1 (12) for all ? ? Rn with 0 ? ? ? C1, and y ¿ ? = 0 as well as k?kpp ? 1 and ? ? 0. [21] optimize the above SIP for p ? 1 with interleaving cutting plane algorithms. The solution of a quadratic program (here the regular SVM) generates the most strongly violated constraint for the actual mixture ?. The optimal (? ? , ?) is then identified by solving a linear program with respect to the set of active constraints. The optimal mixture is then used for computing a new constraint and so on. Unfortunately, for p ¿ 1, a non linearity is introduced by requiring k?kpp ? 1 and such constraint is unlikely to be found in standard optimization toolboxes that often handle only linear and quadratic constraints. As a remedy, we propose to approximate k?kpp ? 1 by sequential second order Taylor expansion of the form ——?——pp ? 1 + M M p(p ? 3) X p(p ? 1) X ?p?2 2 ? p(p ? 2)(??m )p?1 ?m + ?m ?m , 2 2 m=1 m=1 p where ? p is defined element wise, that is ? p := (?1p , ..., ?M ). The sequence (? 0 , ? 1 , ? ? ? ) is initialized with a uniform mixture satisfying k? 0 kpp = 1 as a starting point. Successively ? t+1 is computed ? = ? t . Note that the quadratic term in the approximation is diagonal wherefore the subseusing ? quent quadratically constrained problem can be solved efficiently. Finally note, that this approach can be further sped up by an additional projection onto the level sets in the ? optimization phase similar to [26]. In our case, the level set projection is a convex quadratic problem with ‘p  norm constraints and can again be approximated by successive second order Taylor expansions. 5 2 10 2 10 time in seconds time in seconds 1 10 0 10 ?1 1 10 0 10 10 ?2 10 ?1 2 3 10 10 sample size 10 1 10 2 10 number of kernels 3 10 Figure 1: Execution times of SVM Training, ‘p  norm MKL based on inter leaved optimization via the Newton, the cutting plane algorithm (CPA), and the SimpleMKL wrapper. (left) Training using fixed number of 50 kernels varying training set size. (right) For 500 examples and varying numbers of kernels. Our proposed Newton and CPA obtain speedups of over an order of magnitude. Notice the tiny error bars. 3 Computational Experiments In this section we study non sparse MKL in terms of efficiency and accuracy.1 We apply the method of [21] for ‘1  norm results as it is contained as a special case of our cuttingPplane strategy. We write ‘?  norm MKL for a regular SVM with the unweighted sum kernel K = m Km . 3.1 Execution Time We demonstrate the efficiency of our implementations of non sparse MKL. We experiment on the MNIST data set where the task is to separate odd vs. even digits. We compare our ‘p  norm MKL with two methods for ‘1  norm MKL, simpleMKL [19] and SILP based chunking [21], and to SVMs using the unweighted sum kernel (‘?  norm MKL) as additional baseline. We optimize all methods up to a precision of 10?3 for the outer SVM ? and 10?5 for the ?inner? SIP precision and computed relative duality gaps. To provide a fair stopping criterion to simpleMKL, we set the stopping criterion of simpleMKL to the relative duality gap of its ‘1  norm counterpart. This way, the deviations of relative objective values of ‘1  norm MKL variants are guaranteed to be smaller than 10?4 . SVM trade off parameters are set to C = 1 for all methods. Figure 1 (left) displays the results for varying sample sizes and 50 precomputed Gaussian kernels with different bandwidths. Error bars indicate standard error over 5 repetitions. Unsurprisingly, the SVM with the unweighted sum kernel is the fastest method. Non sparse MKL scales similarly as ‘1  norm chunking; the Newton strategy (Section 2.4.1) is slightly faster than the cutting plane variant (Section 2.4.2) that needs additional Taylor expansions within each ? step. SimpleMKL suffers from training an SVM to full precision for each gradient evaluation and performs worst.2 Figure 1 (right) shows the results for varying the number of precomputed RBF kernels for a fixed sample size of 500. The SVM with the unweighted sum kernel is hardly affected by this setup and performs constantly. The ‘1  norm MKL by [21] handles the increasing number of kernels best and is the fastest MKL method. Non sparse approaches to MKL show reasonable runtimes, the Newtonbased ‘p  norm MKL being again slightly faster than its peer. Simple MKL performs again worst. Overall, our proposed Newton and cutting plane based optimization strategies achieve a speedup of often more than one order of magnitude. 3.2 Protein Subcellular Localization The prediction of the subcellular localization of proteins is one of the rare empirical success stories of ‘1  norm regularized MKL [17, 27]: after defining 69 kernels that capture diverse aspects of 1 2 Available at http://www.shogun toolbox.org/ SimpleMKL could not be evaluated for 2000 instances (ran out of memory on a 4GB machine). 6 Table 1: Results for Protein Subcellular Localization ‘p  norm 1   MCC [%] 1 9.13 32/31 9.12 16/15 9.64 8/7 9.84 4/3 9.56 2 10.18 4 10.08 ? 10.41 protein sequences, ‘1  norm MKL could raise the predictive accuracy signif icantly above that of the unweighted sum of kernels (thereby also improving on established prediction systems for this problem). Here we investigate the performance of non sparse MKL. We download the kernel matrices of the dataset plant3 and follow the experimental setup of [17] with the following changes: instead of a genuine multiclass SVM, we use the 1 vs rest decomposition; instead of performing cross validation for model selection, we report results for the best models, as we are only interested in the relative performance of the MKL regularizers. Specifically, for each C ? {1/32, 1/8, 1/2, 1, 2, 4, 8, 32, 128}, we compute the average Mathews correlation coefficient (MCC) on the test data. For each norm, the best average MCC is recorded. Table 1 shows the averages over several splits of the data. The results indicate that, indeed, with proper choice of a non sparse regularizer, the accuracy of ‘1  norm can be recovered. This is remarkable, as this dataset is particular in that it fullfills the rare condition that ‘1  norm MKL performs better than ‘?  norm MKL. In other words, selecting these data may imply a bias towards ‘1  norm. Nevertheless our novel non sparse MKL can keep up with this, essentially by approximating ‘1  norm. 3.3 Gene Start Recognition This experiment aims at detecting transcription start sites (TSS) of RNA Polymerase II binding genes in genomic DNA sequences. Accurate detection of the transcription start site is crucial to identify genes and their promoter regions and can be regarded as a first step in deciphering the key regulatory elements in the promoter region that determine transcription. For our experiments we use the dataset from [22] which contains a curated set of 8,508 TSS annotated genes built from dbTSS version 4 [23] and refseq genes. These are translated into positive training instances by extracting windows of size [?1000, +1000] around the TSS. Similar to [4], 85,042 negative instances are generated from the interior of the gene using the same window size. Following [22], we employ five different kernels representing the TSS signal (weighted degree with shift), the promoter (spectrum), the 1st exon (spectrum), angles (linear), and energies (linear). Optimal kernel parameters are determined by model selection in [22]. Every kernel is normalized such that all points have unit length in feature space. We reserve 13,000 and 20,000 randomly drawn instances for holdout and test sets, respectively, and use the remaining 60,000 as the training pool. Figure 2 shows test errors for varying training set sizes drawn from the pool; training sets of the same size are disjoint. Error bars indicate standard errors of repetitions for small training set sizes. Regardless of the sample size, ‘1  MKL is significantly outperformed by the sum kernel. On the contrary, non sparse MKL significantly achieves higher AUC values than the ‘?  MKL for sample sizes up to 20k. The scenario is well suited for ‘2  norm MKL which performs best. Finally, for 60k training instances, all methods but ‘1  norm MKL yield the same performance. Again, the superior performance of non sparse MKL is remarkable, and of significance for the application domain: the method using the unweighted sum of kernels [22] has recently been confirmed to be the leading in a comparison of 19 state of the art promoter prediction programs [1], and our experiments suggest that its accuracy can be further elevated by non sparse MKL. 4 Conclusion and Discussion We presented an efficient and accurate approach to non sparse multiple ker nel learning and showed that our ‘p  norm MKL can be motivated as Tikhonov and Ivanov regularization of the mixing coefficients, respectively. Applied to previous MKL research, our result allows for a unified view as so far seemingly different approaches turned out to be equivalent. Furthermore, we devised two efficient approaches to non sparse multiple kernel learning for arbitrary ‘p norms, p ¿ 1. The resulting 3 from http://www.fml.tuebingen.mpg.de/raetsch/suppl/protsubloc/ 7 4?norm unw.?sum n=5k 1?norm 4/3?norm 2?norm 0.93 n=20k 0.91 1?norm MKL 4/3?norm MKL 2?norm MKL 4?norm MKL SVM 0.9 0.89 0.88 0 10K 20K 30K 40K 50K n=60k AUC 0.92 60K sample size Figure 2: Left: Area under ROC curve (AUC) on test data for TSS recog nition as a function of the training set size. Notice the tiny bars indicating standard errors w.r.t. repetitions on disjoint training sets. Right: Corresponding kernel mixtures. For p = 1 consistent sparse solutios are obtained while the optimal p = 2 distributes wheights on the weighted degree and the 2 spectrum kernels in good agreement to [22]. optimization strategies are based on semi infinite programming and Newton descent, both interleaved with chunking based SVM training. Execution times moreover revealed that our interleaved optimization vastly outperforms commonly used wrapper approaches. We would like to note that there is a certain preference/obsession for sparse models in the scientific community due to various reasons. The present paper, however, shows clearly that sparsity by itself is not the ultimate virtue to be strived for. Rather on the contrary: non sparse model may improve quite impressively over sparse ones. The reason for this is less obvious and its theoretical exploration goes well beyond the scope of its submissions. We remark nevertheless that some interesting asymptotic results exist that show model selection consistency of sparse MKL (or the closely related group lasso) [2, 14], in other words in the limit n ? ? MKL is guaranteed to find the correct subset of kernels. However, also the rate of convergence to the true estimator needs to be considered, thus ? we conjecture that the rate slower than n which is common to sparse estimators [11] may be one of the reasons for finding excellent (nonasymptotic) results in non sparse MKL. In addition to the convergence rate the variance properties of MKL estimators may play an important role to elucidate the performance seen in our various simulation experiments. Intuitively speaking, we observe clearly that in some cases all features even though they may contain redundant information are to be kept, since putting their contributions to zero does not improve prediction. I.e. all of them are informative to our MKL models. Note however that this result is also class specific, i.e. for some classes we may sparsify. Cross validation based model building that includes the choice of p will however inevitably tell us which classes should be treated sparse and which non sparse. Large scale experiments on TSS recognition even raised the bar for ‘1  norm MKL: non sparse MKL proved consistently better than its sparse counterparts which were outperformed by an unweightedsum kernel. This exemplifies how the unprecedented combination of accuracy and scalability of our MKL approach and methods paves the way for progress in other real world applications of machine learning. Authors? Contributions The authors contributed in the following way: MK and UB had the initial idea. MK, UB, SS, and AZ each contributed substantially to both mathematical modelling, design and implementation of algorithms, conception and execution of experiments, and writing of the manuscript. PL had some shares in the initial phase and KRM contributed to the text. Most of the work was done at previous affiliations of several authors: Fraunhofer Institute FIRST (Berlin), Technical University Berlin, and the Friedrich Miescher Laboratory (T?ubingen). Acknowledgments This work was supported in part by the German BMBF grant REMIND (FKZ 01 IS07007A) and by the European Community under the PASCAL2 Network of Excellence (ICT 216886). '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "paper_content_dict[\"PP3675\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "VIfQCD1VlDEe"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 4.1. Tokenization <a class=\"anchor\" name=\"tokenize\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gchByyjolDEf"
      },
      "source": [
        "Tokenization is a principal step in text processing and producing unigrams and bigrams. In this section, we start by tokenizing the sentences in ordere to achieve normalized sentences. As per required we are required to have the first letter of a word in a sentence to be in lowercase. punkt mdoule from nltk is used to achieve sentecne tokenization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xbv4i6FJY_LM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "429579b2-693e-4eab-b904-4503d81f4157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sparseness is being regarded as one of the key features in machine learning [15] and biology [16].sparse models are appealing since they provide an intuitive interpretation of a task at hand by singling out relevant pieces of information.such automatic complexity reduction facilitates efficient training algorithms, and the resulting models are distinguished by small capacity.the interpretability is one of the main reasons for the popularity of sparse methods in complex domains such as computational biology, and consequently building sparse models from data has received a significant amount of recent attention.unfortunately, sparse models do not always perform well in practice [7, 15].this holds particularly for learning sparse linear combinations of data sources [15], an abstraction of which is known as multiple kernel learning (MKL) [10].the data sources give rise to a set of (possibly correlated) P kernel matrices K1 , ..., KM , and the task is to learn the optimal mixture K = m ?m Km for the problem at hand.previous MKL research aims at finding sparse mixtures to effectively simplify the underlying data representation.for instance, [10] study semi definite matrices K 0 inducing sparseness by bounding the trace tr(K) ?c; unfortunately, the resulting semi definite optimization problems are computationally too expensive for large scale deployment.recent approaches to MKL promote sparse solutions either by Tikhonov regularization over the mixing coefficients [25] or by incorporating an additional constraint k?k ?1 [18, 27] requiring solutions on the standard simplex, known as Ivanov regularization.based on the one or the other, efficient optimization strategies have been proposed for solving ‘1  norm MKL using semi infinite linear programming [21], second order approaches [6], gradient based optimization [19], and levelset methods [26].other variants of ‘1  norm MKL have been proposed in subsequent work addressing practical algorithms for multi class [18, 27] and multi label [9] problems.1 Previous approaches to MKL successfully identify sparse kernel mixtures, however, the solutions found, frequently suffer fromP poor generalization performances.often, trivial baselines using unweighted sum kernels K = m Km are observed to outperform the sparse mixture [7].one reason for the collapse of ‘1  norm MKL is that kernels deployed in real world tasks are usually highly sophisticated and effectively capture relevant aspects of the data.in contrast, sparse approaches to MKL rely on the assumption that some kernels are irrelevant for solving the problem.enforcing sparse mixtures in these situations may lead to degenerate models.as a remedy, we propose to sacrifice sparseness in these situations and deploy non sparse mixtures instead.after submission of this paper, we learned about a related approach, in which the sum of an ‘1 and an ‘2  regularizer are used [12].although non sparse solutions are not as easy to interpret, they account for (even small) contributions of all available kernels to live up to practical applications.in this paper, we first show the equivalence of the most common approaches to ‘1  norm MKL [18, 25, 27].our theorem allows for a generalized view of recent strands of multiple kernel learning research.based on the detached view, we extend the MKL framework to arbitrary ‘p  norm MKL with p ?1.our approach can either be motivated by additionally regularizing over the mixing coefficients k?kpp , or equivalently by incorporating the constraint k?kpp ?1.we propose two alternative optimization strategies based on Newton descent and cutting planes, respectively.empirically, we demonstrate the efficiency and accuracy of none sparse MKL.large scale experiments on gene start detection show a significant improvement of predictive accuracy compared to ‘1   and ‘?norm MKL.the rest of the paper is structured as follows.we present our main contributions in Section 2, the theoretical analysis of existing approaches to MKL, our ‘p  norm MKL generalization with two highly efficient optimization strategies, and relations to ‘1  norm MKL.we report on our empirical results in Section 3 and Section 4 concludes.2 2.1 Generalized Multiple Kernel Learning Preliminaries In the standard supervised learning setup, a labeled sample D = {(xi , yi )}i=1...,n is given, where the x lie in some input space X and y ?y ?r. The goal is to find a hypothesis f ?h, that generalizes well on new and unseen data.applying regularized risk minimization returns the minimizer f ?, f ?= argminf Remp (f ) + ??(f ), P n where Remp (f ) = n1 i=1 V (f (xi ), yi ) is the empirical risk of hypothesis f w.r.t.to the loss V : R ?y ?r, regularizer ?: H ?r, and trade off parameter ?¿ 0.in this paper, we focus on ?22 and on linear models of the form ?(f ) = 21 kwk ?¿ ?(x) + b, fw,b ?(x) = w (1) together with a (possibly non linear) mapping ?: X ?h to a Hilbert space H [20].we will later make use of kernel functions K(x, x0 ) = h?(x), ?(x0 )iH to compute inner products in H. 2.2 Learning with Multiple Kernels When learning with multiple kernels, we are given M different feature map pings ?m : X ?hm , m = 1, ...m , each giving rise to a reproducing kernel P Km of Hm .approaches to multiple kernel learning consider linear kernel mixtures K?= ?m Km , ?m ?0.compared to Eq.(1), the primal model for learning with multiple kernels is extended to ?¿ ??(xi ) + b = fw,b,?(x) = w ?m p X ?¿ ?m w m ?m (x) + b, (2) m=1 ??and the composite ?= where the weight vector w feature map ??have a block structure w ?¿ ¿ ?¿ ?(w , ..., w ) and ?= ???...???, respectively.?1 1 M M 1 M The idea in learning with P multiple kernels is to minimize the loss on the training data w.r.t.to optimal kernel mixture ?m Km in addition to regularizing ?to avoid overfitting.hence, in terms 2 of regularized risk minimization, the optimization problem becomes inf ?w,b,??0 n M 1X ?x ??m k22 + ?kw ??[?].v (fw,b,?(xi ), yi ) + n i=1 2 m=1 (3) ?previous approaches to multiple kernel learning employ regularizers of the form ?(?)= ——?——1 to promote sparse kernel mixtures.by contrast, we propose to use smooth convex regularizers of the ?form ?(?)= ——?——pp , 1 ¡ p ¡ ?, allowing for non sparse solutions.the non convexity of the ??m. resulting optimization problem is not inherent and can be resolved by substituting wm ??m w 1 Furthermore, regularization parameter and sample size can be decoupled by introducing C?= n???(and adjusting ??? )which has favorable scaling properties in practice.we obtain the following convex optimization problem [5] that has also been considered by [25] for hinge loss and p = 1, !m n M X X 1 X kwm k22 ¿ ?inf C + ?——?——pp , (4) V wm ?m (xi ) + b, yi + w,b,??0 2 ?m m=1 m=1 i=1 where we use the convention that 0t = 0 if t = 0 and ?otherwise.an alternative approach has been studied by [18, 27] (again using hinge loss and p = 1).they upper bound the value of the regularizer k?k1 ?1 and incorporate the latter as an additional constraint into the optimization problem.for C ¿ 0, they arrive at !m n M X X 1 X ——wm ——22 ¿ s.t.——?——pp ?1.(5) inf C V wm ?m (xi ) + b, yi + w,b,??0 2 ?m m=1 m=1 i=1 Our first contribution shows that both, the Tikhonov regularization in Eq.(4) and the Ivanov regularization in Eq.(5), are equivalent.??)there exists C ¿ 0 such that for each optimal Theorem 1 Let be p ?1.for each pair (C, ??), we have that (w?, b?, ??? )is also an optimal solution solution (w?, b?, ?? )of Eq.(4) using (C, of Eq.(5) using C, and vice versa, where ?¿ 0 is some multiplicative constant.proof.the proof is shown in the supplementary material for lack of space.sketch of the proof: We incorporate the regularizer of (4) into the constraints and show that the resulting upper bound is tight.a variable substitution completes the proof.2 Zien and Ong [27] showed that the MKL optimization problems by Bach et al.[3], Sonnenburg et al.[21], and their own formulation are equivalent.as a main implication of Theorem 1 and by using the result of Zien and Ong it follows that the optimization problem of Varma and Ray [25] and the ones from [3, 18, 21, 27] all are equivalent.in addition, our result shows the coupling between trade off parameter C and the regularization parameter ?in Eq.(4): tweaking one also changes the other and vice versa.moreover, Theorem 1 implies that optimizing C in Eq.(5) implicitly searches the regularization path for the parameter ?of Eq.(4).in the remainder, we will therefore focus on the formulation in Eq.(5), as a single parameter is preferable in terms of model selection.furthermore, we will focus on binary classification problems with Y = {?1, +1}, equipped with the hinge loss V (f (x), y) = max{0, 1 ?yf (x)}.however note, that all our results can easily be transferred to regression and multi class settings using appropriate convex loss functions and joint kernel extensions.2.3 Non Sparse Multiple Kernel Learning We now extend the existing MKL framework to allow for non sparse kernel mixtures ?, see also [13].let us begin with rewriting Eq.(5) by expanding the hinge loss into the slack variables as follows M 1 X ——wm ——22 + Ck?k1 (6) min ?,w,b,?2 m=1 ?m !m X 0 s.t.?i : yi wm ?m (xi ) + b ?1 ??i ; ??0 ; ??0 ; k?kpp ?1. m=1 3 Applying Lagrange?s theorem incorporates the constraints into the objective by introducing nonnegative Lagrangian multipliers ?, ??rn , ??rM , ??r (including a pre factor of p1 for the ?term).resubstitution of optimality conditions w.r.t.to w, b, ?, and ?removes the dependency of the Lagrangian on the primal variables.after some additional algebra (e.g., the terms associated with ?cancel), the Lagrangian can be written as p !p?1 M X 1 1 1 ¿ p ?1 ?p?1 ¿ L=1 ??????qm ?, (7) p p 2 m=1 where Qm = diag(y)Km diag(y).eq.(7) now has to be maximized w.r.t.to the dual variables ?, ?, subject to ?¿ y = 0, 0 ??i ?c for 1 ?i ?n, and ??0.let us ignore for a moment the non negativity ??0 and solve ?L/??= 0 for the unbounded ?.setting the partial derivative to zero yields ?= p?1 p !p p?1 M X 1 ¿ .?qm ?2 m=1 (8) Interestingly, at optimality, we always have ??0 because the quadratic term in ?is non negative.plugging the optimal ?into Eq.(7), we arrive at the following optimization problem which solely depends on ?.max ?1 1¿ ??2 M X ?¿ Qm ?p p?1 !p?1 p s.t.0 ???c1; ?¿ y = 0.(9) m=1 P In the limit p ??, the above problem reduces to the SVM dual (with Q = m Qm ), while p ?1 gives rise to a QCQP ‘1  MKL variant.however, optimizing the dual efficiently is difficult and will cause numerical problems in the limits p ?1 and p ??.2.4 Two Efficient Second Order Optimization Strategies Many recent MKL solvers (e.g., [19, 24, 26]) are based on wrapping linear programs around SVMs.from an optimization standpoint, our work is most closely related to the SILP approach [21] and the simpleMKL method [19, 24].both of these methods also aim at efficient large scale MKL algorithms.the two alternative approaches proposed for ‘p  norm MKL proposed in this paper are largely inspired by these methods and extend them in two aspects: customization to arbitrary norms and a tight coupling with minor iterations of an SVM solver, respectively.our first strategy interleaves maximizing the Lagrangian of (6) w.r.t.?with minor precision and Newton descent on ?.for the second strategy, we devise a semi infinite convex program, which we solve by column generation with nested sequential quadratically constrained linear programming (SQCLP).in both cases, the maximization step w.r.t.?is performed by chunking optimization with minor iterations.the Newton approach can be applied without a common purpose QCQP solver, however, convergence can only be guaranteed for the SQCLP [8].2.4.1 Newton Descent For a Newton descent on the mixing coefficients, we first compute the partial derivatives 1 w¿ ?L m wm p?1 = ?+ ??m 2 ??m 2 ?m — {z } and =:??m ?2L w¿ wm p?2 = m3 + (p ?1)??m 2 ??m ?m — {z } =:hm of the original Lagrangian.fortunately, the Hessian H is diagonal, i.e.given by H = diag(h).the m th element sm of the corresponding Netwon step, defined as s := ?H ?1 ??, is thus computed by sm = 1 2 p+2 2 ?m ——w m —— ???m p+1 , ——wm ——2 + (p ?1)??m 4 where ?is defined in Eq.(8).however, a Newton step ?t+1 = ?t + s might lead to non positive ?.to avoid this awkward situation, we take the Newton steps in the space of log(?)by adjusting the derivatives according to the chain rule.we obtain t+1 log(?m ) = t log(?m )?t ?t?m /?m , t )2 ??t /(?t )2 htm /(?m m ?m (10) which corresponds to multiplicative update of ?: t+1 ?m = t ?m ?exp t ?t?m ?m ?t?m ?htm !.(11) Furthermore we additionally enhance the Newton step by a line search.2.4.2 Cutting Planes In order to obtain an alternative optimization strategy, we fix ?and build the partial Lagrangian w.r.t.all other primal variables w, b, ?.the derivation is analogous to [18, 27] and we omit details for lack of space.the resulting dual problem is a min max problem of the form min max ??m X 1 ?m Qm ?1¿ ???¿ 2 m=1 s.t.0 ???c1; y ¿ ?= 0; ??0; k?kpp ?1.the above optimization problem is a saddle point problem and can be solved by alternating ?and ?optimization step.while the former can simply be carried out by a support vector machine for a fixed mixture ?, the latter has been optimized for p = 1 by reduced gradients [18].we take a different approach and translate the min max problem into an equivalent semi infinite program (SIP) as follows.denote the value of the target function by t(?, ?)and suppose ??is optimal.then, according to the max min inequality [5], we have t(??, ?)?t(?, ?)for all ?and ?.hence, we can equivalently minimize an upper bound ?on the optimal value and arrive at min ??,?s.t.m X 1 ?m Qm ???1¿ ???¿ 2 m=1 (12) for all ??rn with 0 ???c1, and y ¿ ?= 0 as well as k?kpp ?1 and ??0.[21] optimize the above SIP for p ?1 with interleaving cutting plane algorithms.the solution of a quadratic program (here the regular SVM) generates the most strongly violated constraint for the actual mixture ?.the optimal (??, ?)is then identified by solving a linear program with respect to the set of active constraints.the optimal mixture is then used for computing a new constraint and so on.unfortunately, for p ¿ 1, a non linearity is introduced by requiring k?kpp ?1 and such constraint is unlikely to be found in standard optimization toolboxes that often handle only linear and quadratic constraints.as a remedy, we propose to approximate k?kpp ?1 by sequential second order Taylor expansion of the form ——?——pp ?1 + M M p(p ?3) X p(p ?1) X ?p?2 2 ?p(p ?2)(??m )p?1 ?m + ?m ?m , 2 2 m=1 m=1 p where ?p is defined element wise, that is ?p := (?1p , ..., ?M ).the sequence (?0 , ?1 , ??? )is initialized with a uniform mixture satisfying k?0 kpp = 1 as a starting point.successively ?t+1 is computed ?= ?t .note that the quadratic term in the approximation is diagonal wherefore the subseusing ?quent quadratically constrained problem can be solved efficiently.finally note, that this approach can be further sped up by an additional projection onto the level sets in the ?optimization phase similar to [26].in our case, the level set projection is a convex quadratic problem with ‘p  norm constraints and can again be approximated by successive second order Taylor expansions.5 2 10 2 10 time in seconds time in seconds 1 10 0 10 ?1 1 10 0 10 10 ?2 10 ?1 2 3 10 10 sample size 10 1 10 2 10 number of kernels 3 10 Figure 1: Execution times of SVM Training, ‘p  norm MKL based on inter leaved optimization via the Newton, the cutting plane algorithm (CPA), and the SimpleMKL wrapper.(left) Training using fixed number of 50 kernels varying training set size.(right) For 500 examples and varying numbers of kernels.our proposed Newton and CPA obtain speedups of over an order of magnitude.notice the tiny error bars.3 Computational Experiments In this section we study non sparse MKL in terms of efficiency and accuracy.1 We apply the method of [21] for ‘1  norm results as it is contained as a special case of our cuttingPplane strategy.we write ‘?norm MKL for a regular SVM with the unweighted sum kernel K = m Km .3.1 Execution Time We demonstrate the efficiency of our implementations of non sparse MKL.we experiment on the MNIST data set where the task is to separate odd vs. even digits.we compare our ‘p  norm MKL with two methods for ‘1  norm MKL, simpleMKL [19] and SILP based chunking [21], and to SVMs using the unweighted sum kernel (‘?norm MKL) as additional baseline.we optimize all methods up to a precision of 10?3 for the outer SVM ?and 10?5 for the ?inner?sIP precision and computed relative duality gaps.to provide a fair stopping criterion to simpleMKL, we set the stopping criterion of simpleMKL to the relative duality gap of its ‘1  norm counterpart.this way, the deviations of relative objective values of ‘1  norm MKL variants are guaranteed to be smaller than 10?4 .sVM trade off parameters are set to C = 1 for all methods.figure 1 (left) displays the results for varying sample sizes and 50 precomputed Gaussian kernels with different bandwidths.error bars indicate standard error over 5 repetitions.unsurprisingly, the SVM with the unweighted sum kernel is the fastest method.non sparse MKL scales similarly as ‘1  norm chunking; the Newton strategy (Section 2.4.1) is slightly faster than the cutting plane variant (Section 2.4.2) that needs additional Taylor expansions within each ?step.simpleMKL suffers from training an SVM to full precision for each gradient evaluation and performs worst.2 Figure 1 (right) shows the results for varying the number of precomputed RBF kernels for a fixed sample size of 500.the SVM with the unweighted sum kernel is hardly affected by this setup and performs constantly.the ‘1  norm MKL by [21] handles the increasing number of kernels best and is the fastest MKL method.non sparse approaches to MKL show reasonable runtimes, the Newtonbased ‘p  norm MKL being again slightly faster than its peer.simple MKL performs again worst.overall, our proposed Newton and cutting plane based optimization strategies achieve a speedup of often more than one order of magnitude.3.2 Protein Subcellular Localization The prediction of the subcellular localization of proteins is one of the rare empirical success stories of ‘1  norm regularized MKL [17, 27]: after defining 69 kernels that capture diverse aspects of 1 2 Available at http://www.shogun toolbox.org/ SimpleMKL could not be evaluated for 2000 instances (ran out of memory on a 4GB machine).6 Table 1: Results for Protein Subcellular Localization ‘p  norm 1   MCC [%] 1 9.13 32/31 9.12 16/15 9.64 8/7 9.84 4/3 9.56 2 10.18 4 10.08 ?10.41 protein sequences, ‘1  norm MKL could raise the predictive accuracy signif icantly above that of the unweighted sum of kernels (thereby also improving on established prediction systems for this problem).here we investigate the performance of non sparse MKL.we download the kernel matrices of the dataset plant3 and follow the experimental setup of [17] with the following changes: instead of a genuine multiclass SVM, we use the 1 vs rest decomposition; instead of performing cross validation for model selection, we report results for the best models, as we are only interested in the relative performance of the MKL regularizers.specifically, for each C ?{1/32, 1/8, 1/2, 1, 2, 4, 8, 32, 128}, we compute the average Mathews correlation coefficient (MCC) on the test data.for each norm, the best average MCC is recorded.table 1 shows the averages over several splits of the data.the results indicate that, indeed, with proper choice of a non sparse regularizer, the accuracy of ‘1  norm can be recovered.this is remarkable, as this dataset is particular in that it fullfills the rare condition that ‘1  norm MKL performs better than ‘?norm MKL.in other words, selecting these data may imply a bias towards ‘1  norm.nevertheless our novel non sparse MKL can keep up with this, essentially by approximating ‘1  norm.3.3 Gene Start Recognition This experiment aims at detecting transcription start sites (TSS) of RNA Polymerase II binding genes in genomic DNA sequences.accurate detection of the transcription start site is crucial to identify genes and their promoter regions and can be regarded as a first step in deciphering the key regulatory elements in the promoter region that determine transcription.for our experiments we use the dataset from [22] which contains a curated set of 8,508 TSS annotated genes built from dbTSS version 4 [23] and refseq genes.these are translated into positive training instances by extracting windows of size [?1000, +1000] around the TSS.similar to [4], 85,042 negative instances are generated from the interior of the gene using the same window size.following [22], we employ five different kernels representing the TSS signal (weighted degree with shift), the promoter (spectrum), the 1st exon (spectrum), angles (linear), and energies (linear).optimal kernel parameters are determined by model selection in [22].every kernel is normalized such that all points have unit length in feature space.we reserve 13,000 and 20,000 randomly drawn instances for holdout and test sets, respectively, and use the remaining 60,000 as the training pool.figure 2 shows test errors for varying training set sizes drawn from the pool; training sets of the same size are disjoint.error bars indicate standard errors of repetitions for small training set sizes.regardless of the sample size, ‘1  MKL is significantly outperformed by the sum kernel.on the contrary, non sparse MKL significantly achieves higher AUC values than the ‘?mKL for sample sizes up to 20k.the scenario is well suited for ‘2  norm MKL which performs best.finally, for 60k training instances, all methods but ‘1  norm MKL yield the same performance.again, the superior performance of non sparse MKL is remarkable, and of significance for the application domain: the method using the unweighted sum of kernels [22] has recently been confirmed to be the leading in a comparison of 19 state of the art promoter prediction programs [1], and our experiments suggest that its accuracy can be further elevated by non sparse MKL.4 Conclusion and Discussion We presented an efficient and accurate approach to non sparse multiple ker nel learning and showed that our ‘p  norm MKL can be motivated as Tikhonov and Ivanov regularization of the mixing coefficients, respectively.applied to previous MKL research, our result allows for a unified view as so far seemingly different approaches turned out to be equivalent.furthermore, we devised two efficient approaches to non sparse multiple kernel learning for arbitrary ‘p norms, p ¿ 1.the resulting 3 from http://www.fml.tuebingen.mpg.de/raetsch/suppl/protsubloc/ 7 4?norm unw.?sum n=5k 1?norm 4/3?norm 2?norm 0.93 n=20k 0.91 1?norm MKL 4/3?norm MKL 2?norm MKL 4?norm MKL SVM 0.9 0.89 0.88 0 10K 20K 30K 40K 50K n=60k AUC 0.92 60K sample size Figure 2: Left: Area under ROC curve (AUC) on test data for TSS recog nition as a function of the training set size.notice the tiny bars indicating standard errors w.r.t.repetitions on disjoint training sets.right: Corresponding kernel mixtures.for p = 1 consistent sparse solutios are obtained while the optimal p = 2 distributes wheights on the weighted degree and the 2 spectrum kernels in good agreement to [22].optimization strategies are based on semi infinite programming and Newton descent, both interleaved with chunking based SVM training.execution times moreover revealed that our interleaved optimization vastly outperforms commonly used wrapper approaches.we would like to note that there is a certain preference/obsession for sparse models in the scientific community due to various reasons.the present paper, however, shows clearly that sparsity by itself is not the ultimate virtue to be strived for.rather on the contrary: non sparse model may improve quite impressively over sparse ones.the reason for this is less obvious and its theoretical exploration goes well beyond the scope of its submissions.we remark nevertheless that some interesting asymptotic results exist that show model selection consistency of sparse MKL (or the closely related group lasso) [2, 14], in other words in the limit n ??mKL is guaranteed to find the correct subset of kernels.however, also the rate of convergence to the true estimator needs to be considered, thus ?we conjecture that the rate slower than n which is common to sparse estimators [11] may be one of the reasons for finding excellent (nonasymptotic) results in non sparse MKL.in addition to the convergence rate the variance properties of MKL estimators may play an important role to elucidate the performance seen in our various simulation experiments.intuitively speaking, we observe clearly that in some cases all features even though they may contain redundant information are to be kept, since putting their contributions to zero does not improve prediction.i.e.all of them are informative to our MKL models.note however that this result is also class specific, i.e.for some classes we may sparsify.cross validation based model building that includes the choice of p will however inevitably tell us which classes should be treated sparse and which non sparse.large scale experiments on TSS recognition even raised the bar for ‘1  norm MKL: non sparse MKL proved consistently better than its sparse counterparts which were outperformed by an unweightedsum kernel.this exemplifies how the unprecedented combination of accuracy and scalability of our MKL approach and methods paves the way for progress in other real world applications of machine learning.authors?contributions The authors contributed in the following way: MK and UB had the initial idea.mK, UB, SS, and AZ each contributed substantially to both mathematical modelling, design and implementation of algorithms, conception and execution of experiments, and writing of the manuscript.pL had some shares in the initial phase and KRM contributed to the text.most of the work was done at previous affiliations of several authors: Fraunhofer Institute FIRST (Berlin), Technical University Berlin, and the Friedrich Miescher Laboratory (T?ubingen).acknowledgments This work was supported in part by the German BMBF grant REMIND (FKZ 01 IS07007A) and by the European Community under the PASCAL2 Network of Excellence (ICT 216886).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# unsupervised algorithm that helops to achieve sentence segmentation\n",
        "nltk.download('punkt')\n",
        "# dictionary to the segemented list for all the paper id as keys\n",
        "sentence_seg = {}\n",
        "# loops through the content dictionary\n",
        "for key, value in paper_content_dict.items():\n",
        "  # segements the entire paper body into different sentences \n",
        "  paper_segment = \"\"\n",
        "  segment_list = []\n",
        "  sentence = nltk.sent_tokenize(value)\n",
        "  # sentence normaliation\n",
        "  for i in sentence:\n",
        "    segment = i.strip()\n",
        "    paper_segment += segment[0].lower() + segment[1:]\n",
        "  sentence_seg[key] = ''.join(paper_segment)\n",
        "sentence_seg[\"PP3675\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we tokenize all the paper body contents to have a list of all the token in each paper. After the tokens are created we add them in a list and set it as a value in the dictionary having the paper id as the key."
      ],
      "metadata": {
        "id": "24A74JZ_H4Rl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8zT4N0RlDEf",
        "outputId": "81bb999f-2065-4af9-bacf-4a2f46261e81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sparseness',\n",
              " 'is',\n",
              " 'being',\n",
              " 'regarded',\n",
              " 'as',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'key',\n",
              " 'features',\n",
              " 'in',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'and',\n",
              " 'biology',\n",
              " 'sparse',\n",
              " 'models',\n",
              " 'are',\n",
              " 'appealing',\n",
              " 'since',\n",
              " 'they',\n",
              " 'provide',\n",
              " 'an',\n",
              " 'intuitive',\n",
              " 'interpretation',\n",
              " 'of',\n",
              " 'task',\n",
              " 'at',\n",
              " 'hand',\n",
              " 'by',\n",
              " 'singling',\n",
              " 'out',\n",
              " 'relevant',\n",
              " 'pieces',\n",
              " 'of',\n",
              " 'information',\n",
              " 'such',\n",
              " 'automatic',\n",
              " 'complexity',\n",
              " 'reduction',\n",
              " 'facilitates',\n",
              " 'efficient',\n",
              " 'training',\n",
              " 'algorithms',\n",
              " 'and',\n",
              " 'the',\n",
              " 'resulting',\n",
              " 'models',\n",
              " 'are',\n",
              " 'distinguished',\n",
              " 'by',\n",
              " 'small',\n",
              " 'capacity',\n",
              " 'the',\n",
              " 'interpretability',\n",
              " 'is',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'main',\n",
              " 'reasons',\n",
              " 'for',\n",
              " 'the',\n",
              " 'popularity',\n",
              " 'of',\n",
              " 'sparse',\n",
              " 'methods',\n",
              " 'in',\n",
              " 'complex',\n",
              " 'domains',\n",
              " 'such',\n",
              " 'as',\n",
              " 'computational',\n",
              " 'biology',\n",
              " 'and',\n",
              " 'consequently',\n",
              " 'building',\n",
              " 'sparse',\n",
              " 'models',\n",
              " 'from',\n",
              " 'data',\n",
              " 'has',\n",
              " 'received',\n",
              " 'significant',\n",
              " 'amount',\n",
              " 'of',\n",
              " 'recent',\n",
              " 'attention',\n",
              " 'unfortunately',\n",
              " 'sparse',\n",
              " 'models',\n",
              " 'do',\n",
              " 'not',\n",
              " 'always',\n",
              " 'perform',\n",
              " 'well',\n",
              " 'in',\n",
              " 'practice',\n",
              " 'this',\n",
              " 'holds',\n",
              " 'particularly',\n",
              " 'for',\n",
              " 'learning',\n",
              " 'sparse',\n",
              " 'linear',\n",
              " 'combinations',\n",
              " 'of',\n",
              " 'data',\n",
              " 'sources',\n",
              " 'an',\n",
              " 'abstraction',\n",
              " 'of',\n",
              " 'which',\n",
              " 'is',\n",
              " 'known',\n",
              " 'as',\n",
              " 'multiple',\n",
              " 'kernel',\n",
              " 'learning',\n",
              " 'MKL',\n",
              " 'the',\n",
              " 'data',\n",
              " 'sources',\n",
              " 'give',\n",
              " 'rise',\n",
              " 'to',\n",
              " 'set',\n",
              " 'of',\n",
              " 'possibly',\n",
              " 'correlated',\n",
              " 'kernel',\n",
              " 'matrices',\n",
              " 'K1',\n",
              " 'KM',\n",
              " 'and',\n",
              " 'the',\n",
              " 'task',\n",
              " 'is',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'the',\n",
              " 'optimal',\n",
              " 'mixture',\n",
              " 'Km',\n",
              " 'for',\n",
              " 'the',\n",
              " 'problem',\n",
              " 'at',\n",
              " 'hand',\n",
              " 'previous',\n",
              " 'MKL',\n",
              " 'research',\n",
              " 'aims',\n",
              " 'at',\n",
              " 'finding',\n",
              " 'sparse',\n",
              " 'mixtures',\n",
              " 'to',\n",
              " 'effectively',\n",
              " 'simplify',\n",
              " 'the',\n",
              " 'underlying',\n",
              " 'data',\n",
              " 'representation',\n",
              " 'for',\n",
              " 'instance',\n",
              " 'study',\n",
              " 'semi',\n",
              " 'definite',\n",
              " 'matrices',\n",
              " 'inducing',\n",
              " 'sparseness',\n",
              " 'by',\n",
              " 'bounding',\n",
              " 'the',\n",
              " 'trace',\n",
              " 'tr',\n",
              " 'unfortunately',\n",
              " 'the',\n",
              " 'resulting',\n",
              " 'semi',\n",
              " 'definite',\n",
              " 'optimization',\n",
              " 'problems',\n",
              " 'are',\n",
              " 'computationally',\n",
              " 'too',\n",
              " 'expensive',\n",
              " 'for',\n",
              " 'large',\n",
              " 'scale',\n",
              " 'deployment',\n",
              " 'recent',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'MKL',\n",
              " 'promote',\n",
              " 'sparse',\n",
              " 'solutions',\n",
              " 'either',\n",
              " 'by',\n",
              " 'Tikhonov',\n",
              " 'regularization',\n",
              " 'over',\n",
              " 'the',\n",
              " 'mixing',\n",
              " 'coefficients',\n",
              " 'or',\n",
              " 'by',\n",
              " 'incorporating',\n",
              " 'an',\n",
              " 'additional',\n",
              " 'constraint',\n",
              " 'requiring',\n",
              " 'solutions',\n",
              " 'on',\n",
              " 'the',\n",
              " 'standard',\n",
              " 'simplex',\n",
              " 'known',\n",
              " 'as',\n",
              " 'Ivanov',\n",
              " 'regularization',\n",
              " 'based',\n",
              " 'on',\n",
              " 'the',\n",
              " 'one',\n",
              " 'or',\n",
              " 'the',\n",
              " 'other',\n",
              " 'efficient',\n",
              " 'optimization',\n",
              " 'strategies',\n",
              " 'have',\n",
              " 'been',\n",
              " 'proposed',\n",
              " 'for',\n",
              " 'solving',\n",
              " 'norm',\n",
              " 'MKL',\n",
              " 'using',\n",
              " 'semi',\n",
              " 'infinite',\n",
              " 'linear',\n",
              " 'programming',\n",
              " 'second',\n",
              " 'order',\n",
              " 'approaches',\n",
              " 'gradient',\n",
              " 'based',\n",
              " 'optimization',\n",
              " 'and',\n",
              " 'levelset',\n",
              " 'methods',\n",
              " 'other',\n",
              " 'variants',\n",
              " 'of',\n",
              " 'norm',\n",
              " 'MKL',\n",
              " 'have',\n",
              " 'been',\n",
              " 'proposed',\n",
              " 'in',\n",
              " 'subsequent',\n",
              " 'work',\n",
              " 'addressing',\n",
              " 'practical',\n",
              " 'algorithms',\n",
              " 'for',\n",
              " 'multi',\n",
              " 'class',\n",
              " 'and',\n",
              " 'multi',\n",
              " 'label',\n",
              " 'problems',\n",
              " 'Previous',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'MKL',\n",
              " 'successfully',\n",
              " 'identify',\n",
              " 'sparse',\n",
              " 'kernel',\n",
              " 'mixtures',\n",
              " 'however',\n",
              " 'the',\n",
              " 'solutions',\n",
              " 'found',\n",
              " 'frequently',\n",
              " 'suffer',\n",
              " 'fromP',\n",
              " 'poor',\n",
              " 'generalization',\n",
              " 'performances',\n",
              " 'often',\n",
              " 'trivial',\n",
              " 'baselines',\n",
              " 'using',\n",
              " 'unweighted',\n",
              " 'sum',\n",
              " 'kernels',\n",
              " 'Km',\n",
              " 'are',\n",
              " 'observed',\n",
              " 'to',\n",
              " 'outperform',\n",
              " 'the',\n",
              " 'sparse',\n",
              " 'mixture',\n",
              " 'one',\n",
              " 'reason',\n",
              " 'for',\n",
              " 'the',\n",
              " 'collapse',\n",
              " 'of',\n",
              " 'norm',\n",
              " 'MKL',\n",
              " 'is',\n",
              " 'that',\n",
              " 'kernels',\n",
              " 'deployed',\n",
              " 'in',\n",
              " 'real',\n",
              " 'world',\n",
              " 'tasks',\n",
              " 'are',\n",
              " 'usually',\n",
              " 'highly',\n",
              " 'sophisticated',\n",
              " 'and',\n",
              " 'effectively',\n",
              " 'capture',\n",
              " 'relevant',\n",
              " 'aspects',\n",
              " 'of',\n",
              " 'the',\n",
              " 'data',\n",
              " 'in',\n",
              " 'contrast',\n",
              " 'sparse',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'MKL',\n",
              " 'rely',\n",
              " 'on',\n",
              " 'the',\n",
              " 'assumption',\n",
              " 'that',\n",
              " 'some',\n",
              " 'kernels',\n",
              " 'are',\n",
              " 'irrelevant',\n",
              " 'for',\n",
              " 'solving',\n",
              " 'the',\n",
              " 'problem',\n",
              " 'enforcing',\n",
              " 'sparse',\n",
              " 'mixtures',\n",
              " 'in',\n",
              " 'these',\n",
              " 'situations',\n",
              " 'may',\n",
              " 'lead',\n",
              " 'to',\n",
              " 'degenerate',\n",
              " 'models',\n",
              " 'as',\n",
              " 'remedy',\n",
              " 'we',\n",
              " 'propose',\n",
              " 'to',\n",
              " 'sacrifice',\n",
              " 'sparseness',\n",
              " 'in',\n",
              " 'these',\n",
              " 'situations',\n",
              " 'and',\n",
              " 'deploy',\n",
              " 'non',\n",
              " 'sparse',\n",
              " 'mixtures',\n",
              " 'instead',\n",
              " 'after',\n",
              " 'submission',\n",
              " 'of',\n",
              " 'this',\n",
              " 'paper',\n",
              " 'we',\n",
              " 'learned',\n",
              " 'about',\n",
              " 'related',\n",
              " 'approach',\n",
              " 'in',\n",
              " 'which',\n",
              " 'the',\n",
              " 'sum',\n",
              " 'of',\n",
              " 'an',\n",
              " 'and',\n",
              " 'an',\n",
              " 'regularizer',\n",
              " 'are',\n",
              " 'used',\n",
              " 'although',\n",
              " 'non',\n",
              " 'sparse',\n",
              " 'solutions',\n",
              " 'are',\n",
              " 'not',\n",
              " 'as',\n",
              " 'easy',\n",
              " 'to',\n",
              " 'interpret',\n",
              " 'they',\n",
              " 'account',\n",
              " 'for',\n",
              " 'even',\n",
              " 'small',\n",
              " 'contributions',\n",
              " 'of',\n",
              " 'all',\n",
              " 'available',\n",
              " 'kernels',\n",
              " 'to',\n",
              " 'live',\n",
              " 'up',\n",
              " 'to',\n",
              " 'practical',\n",
              " 'applications',\n",
              " 'in',\n",
              " 'this',\n",
              " 'paper',\n",
              " 'we',\n",
              " 'first',\n",
              " 'show',\n",
              " 'the',\n",
              " 'equivalence',\n",
              " 'of',\n",
              " 'the',\n",
              " 'most',\n",
              " 'common',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'norm',\n",
              " 'MKL',\n",
              " 'our',\n",
              " 'theorem',\n",
              " 'allows',\n",
              " 'for',\n",
              " 'generalized',\n",
              " 'view',\n",
              " 'of',\n",
              " 'recent',\n",
              " 'strands',\n",
              " 'of',\n",
              " 'multiple',\n",
              " 'kernel',\n",
              " 'learning',\n",
              " 'research',\n",
              " 'based',\n",
              " 'on',\n",
              " 'the',\n",
              " 'detached',\n",
              " 'view',\n",
              " 'we',\n",
              " 'extend',\n",
              " 'the',\n",
              " 'MKL',\n",
              " 'framework',\n",
              " 'to',\n",
              " 'arbitrary',\n",
              " 'norm',\n",
              " 'MKL',\n",
              " 'with',\n",
              " 'our',\n",
              " 'approach',\n",
              " 'can',\n",
              " 'either',\n",
              " 'be',\n",
              " 'motivated',\n",
              " 'by',\n",
              " 'additionally',\n",
              " 'regularizing',\n",
              " 'over',\n",
              " 'the',\n",
              " 'mixing',\n",
              " 'coefficients',\n",
              " 'kpp',\n",
              " 'or',\n",
              " 'equivalently',\n",
              " 'by',\n",
              " 'incorporating',\n",
              " 'the',\n",
              " 'constraint',\n",
              " 'kpp',\n",
              " 'we',\n",
              " 'propose',\n",
              " 'two',\n",
              " 'alternative',\n",
              " 'optimization',\n",
              " 'strategies',\n",
              " 'based',\n",
              " 'on',\n",
              " 'Newton',\n",
              " 'descent',\n",
              " 'and',\n",
              " 'cutting',\n",
              " 'planes',\n",
              " 'respectively',\n",
              " 'empirically',\n",
              " 'we',\n",
              " 'demonstrate',\n",
              " 'the',\n",
              " 'efficiency',\n",
              " 'and',\n",
              " 'accuracy',\n",
              " 'of',\n",
              " 'none',\n",
              " 'sparse',\n",
              " 'MKL',\n",
              " 'large',\n",
              " 'scale',\n",
              " 'experiments',\n",
              " 'on',\n",
              " 'gene',\n",
              " 'start',\n",
              " 'detection',\n",
              " 'show',\n",
              " 'significant',\n",
              " 'improvement',\n",
              " 'of',\n",
              " 'predictive',\n",
              " 'accuracy',\n",
              " 'compared',\n",
              " 'to',\n",
              " 'and',\n",
              " 'norm',\n",
              " 'MKL',\n",
              " 'the',\n",
              " 'rest',\n",
              " 'of',\n",
              " 'the',\n",
              " 'paper',\n",
              " 'is',\n",
              " 'structured',\n",
              " 'as',\n",
              " 'follows',\n",
              " 'we',\n",
              " 'present',\n",
              " 'our',\n",
              " 'main',\n",
              " 'contributions',\n",
              " 'in',\n",
              " 'Section',\n",
              " 'the',\n",
              " 'theoretical',\n",
              " 'analysis',\n",
              " 'of',\n",
              " 'existing',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'MKL',\n",
              " 'our',\n",
              " 'norm',\n",
              " 'MKL',\n",
              " 'generalization',\n",
              " 'with',\n",
              " 'two',\n",
              " 'highly',\n",
              " 'efficient',\n",
              " 'optimization',\n",
              " 'strategies',\n",
              " 'and',\n",
              " 'relations',\n",
              " 'to',\n",
              " 'norm',\n",
              " 'MKL',\n",
              " 'we',\n",
              " 'report',\n",
              " 'on',\n",
              " 'our',\n",
              " 'empirical',\n",
              " 'results',\n",
              " 'in',\n",
              " 'Section',\n",
              " 'and',\n",
              " 'Section',\n",
              " 'concludes',\n",
              " 'Generalized',\n",
              " 'Multiple',\n",
              " 'Kernel',\n",
              " 'Learning',\n",
              " 'Preliminaries',\n",
              " 'In',\n",
              " 'the',\n",
              " 'standard',\n",
              " 'supervised',\n",
              " 'learning',\n",
              " 'setup',\n",
              " 'labeled',\n",
              " 'sample',\n",
              " 'xi',\n",
              " 'yi',\n",
              " 'is',\n",
              " 'given',\n",
              " 'where',\n",
              " 'the',\n",
              " 'lie',\n",
              " 'in',\n",
              " 'some',\n",
              " 'input',\n",
              " 'space',\n",
              " 'and',\n",
              " 'The',\n",
              " 'goal',\n",
              " 'is',\n",
              " 'to',\n",
              " 'find',\n",
              " 'hypothesis',\n",
              " 'that',\n",
              " 'generalizes',\n",
              " 'well',\n",
              " 'on',\n",
              " 'new',\n",
              " 'and',\n",
              " 'unseen',\n",
              " 'data',\n",
              " 'applying',\n",
              " 'regularized',\n",
              " 'risk',\n",
              " 'minimization',\n",
              " 'returns',\n",
              " 'the',\n",
              " 'minimizer',\n",
              " 'argminf',\n",
              " 'Remp',\n",
              " 'where',\n",
              " 'Remp',\n",
              " 'n1',\n",
              " 'xi',\n",
              " 'yi',\n",
              " 'is',\n",
              " 'the',\n",
              " 'empirical',\n",
              " 'risk',\n",
              " 'of',\n",
              " 'hypothesis',\n",
              " 'to',\n",
              " 'the',\n",
              " 'loss',\n",
              " 'regularizer',\n",
              " 'and',\n",
              " 'trade',\n",
              " 'off',\n",
              " 'parameter',\n",
              " 'in',\n",
              " 'this',\n",
              " 'paper',\n",
              " 'we',\n",
              " 'focus',\n",
              " 'on',\n",
              " 'and',\n",
              " 'on',\n",
              " 'linear',\n",
              " 'models',\n",
              " 'of',\n",
              " 'the',\n",
              " 'form',\n",
              " 'kwk',\n",
              " 'fw',\n",
              " 'together',\n",
              " 'with',\n",
              " 'possibly',\n",
              " 'non',\n",
              " 'linear',\n",
              " 'mapping',\n",
              " 'to',\n",
              " 'Hilbert',\n",
              " 'space',\n",
              " 'we',\n",
              " 'will',\n",
              " 'later',\n",
              " 'make',\n",
              " 'use',\n",
              " 'of',\n",
              " 'kernel',\n",
              " 'functions',\n",
              " 'x0',\n",
              " 'x0',\n",
              " 'iH',\n",
              " 'to',\n",
              " 'compute',\n",
              " 'inner',\n",
              " 'products',\n",
              " 'in',\n",
              " 'Learning',\n",
              " 'with',\n",
              " 'Multiple',\n",
              " 'Kernels',\n",
              " 'When',\n",
              " 'learning',\n",
              " 'with',\n",
              " 'multiple',\n",
              " 'kernels',\n",
              " 'we',\n",
              " 'are',\n",
              " 'given',\n",
              " 'different',\n",
              " 'feature',\n",
              " 'map',\n",
              " 'pings',\n",
              " 'hm',\n",
              " 'each',\n",
              " 'giving',\n",
              " 'rise',\n",
              " 'to',\n",
              " 'reproducing',\n",
              " 'kernel',\n",
              " 'Km',\n",
              " 'of',\n",
              " 'Hm',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'multiple',\n",
              " 'kernel',\n",
              " 'learning',\n",
              " 'consider',\n",
              " 'linear',\n",
              " 'kernel',\n",
              " 'mixtures',\n",
              " 'Km',\n",
              " 'compared',\n",
              " 'to',\n",
              " 'Eq',\n",
              " 'the',\n",
              " 'primal',\n",
              " 'model',\n",
              " 'for',\n",
              " 'learning',\n",
              " 'with',\n",
              " 'multiple',\n",
              " 'kernels',\n",
              " 'is',\n",
              " 'extended',\n",
              " 'to',\n",
              " 'xi',\n",
              " 'fw',\n",
              " 'and',\n",
              " 'the',\n",
              " 'composite',\n",
              " 'where',\n",
              " 'the',\n",
              " 'weight',\n",
              " 'vector',\n",
              " 'feature',\n",
              " 'map',\n",
              " 'have',\n",
              " 'block',\n",
              " 'structure',\n",
              " 'and',\n",
              " 'respectively',\n",
              " 'The',\n",
              " 'idea',\n",
              " 'in',\n",
              " 'learning',\n",
              " 'with',\n",
              " 'multiple',\n",
              " 'kernels',\n",
              " 'is',\n",
              " 'to',\n",
              " 'minimize',\n",
              " 'the',\n",
              " 'loss',\n",
              " 'on',\n",
              " 'the',\n",
              " 'training',\n",
              " 'data',\n",
              " 'to',\n",
              " 'optimal',\n",
              " 'kernel',\n",
              " 'mixture',\n",
              " 'Km',\n",
              " 'in',\n",
              " 'addition',\n",
              " 'to',\n",
              " 'regularizing',\n",
              " 'to',\n",
              " 'avoid',\n",
              " 'overfitting',\n",
              " 'hence',\n",
              " 'in',\n",
              " 'terms',\n",
              " 'of',\n",
              " 'regularized',\n",
              " 'risk',\n",
              " 'minimization',\n",
              " 'the',\n",
              " 'optimization',\n",
              " 'problem',\n",
              " 'becomes',\n",
              " 'inf',\n",
              " 'k22',\n",
              " 'kw',\n",
              " 'fw',\n",
              " 'xi',\n",
              " 'yi',\n",
              " 'previous',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'multiple',\n",
              " 'kernel',\n",
              " 'learning',\n",
              " 'employ',\n",
              " 'regularizers',\n",
              " 'of',\n",
              " 'the',\n",
              " 'form',\n",
              " 'to',\n",
              " 'promote',\n",
              " 'sparse',\n",
              " 'kernel',\n",
              " 'mixtures',\n",
              " 'by',\n",
              " 'contrast',\n",
              " 'we',\n",
              " 'propose',\n",
              " 'to',\n",
              " 'use',\n",
              " 'smooth',\n",
              " 'convex',\n",
              " 'regularizers',\n",
              " 'of',\n",
              " 'the',\n",
              " 'form',\n",
              " 'pp',\n",
              " 'allowing',\n",
              " 'for',\n",
              " 'non',\n",
              " 'sparse',\n",
              " 'solutions',\n",
              " 'the',\n",
              " 'non',\n",
              " 'convexity',\n",
              " 'of',\n",
              " 'the',\n",
              " 'resulting',\n",
              " 'optimization',\n",
              " 'problem',\n",
              " 'is',\n",
              " 'not',\n",
              " 'inherent',\n",
              " 'and',\n",
              " 'can',\n",
              " 'be',\n",
              " 'resolved',\n",
              " 'by',\n",
              " 'substituting',\n",
              " 'wm',\n",
              " 'Furthermore',\n",
              " 'regularization',\n",
              " 'parameter',\n",
              " 'and',\n",
              " 'sample',\n",
              " 'size',\n",
              " 'can',\n",
              " 'be',\n",
              " 'decoupled',\n",
              " 'by',\n",
              " 'introducing',\n",
              " 'and',\n",
              " 'adjusting',\n",
              " 'which',\n",
              " 'has',\n",
              " 'favorable',\n",
              " 'scaling',\n",
              " 'properties',\n",
              " 'in',\n",
              " 'practice',\n",
              " 'we',\n",
              " 'obtain',\n",
              " 'the',\n",
              " 'following',\n",
              " 'convex',\n",
              " 'optimization',\n",
              " 'problem',\n",
              " 'that',\n",
              " 'has',\n",
              " 'also',\n",
              " 'been',\n",
              " 'considered',\n",
              " 'by',\n",
              " 'for',\n",
              " 'hinge',\n",
              " 'loss',\n",
              " 'and',\n",
              " 'kwm',\n",
              " 'k22',\n",
              " 'inf',\n",
              " 'pp',\n",
              " 'wm',\n",
              " 'xi',\n",
              " 'yi',\n",
              " 'where',\n",
              " 'we',\n",
              " 'use',\n",
              " 'the',\n",
              " 'convention',\n",
              " 'that',\n",
              " 'if',\n",
              " 'and',\n",
              " 'otherwise',\n",
              " 'an',\n",
              " 'alternative',\n",
              " 'approach',\n",
              " 'has',\n",
              " 'been',\n",
              " 'studied',\n",
              " 'by',\n",
              " 'again',\n",
              " 'using',\n",
              " 'hinge',\n",
              " 'loss',\n",
              " 'and',\n",
              " 'they',\n",
              " 'upper',\n",
              " 'bound',\n",
              " 'the',\n",
              " 'value',\n",
              " 'of',\n",
              " 'the',\n",
              " 'regularizer',\n",
              " 'k1',\n",
              " 'and',\n",
              " 'incorporate',\n",
              " 'the',\n",
              " 'latter',\n",
              " 'as',\n",
              " 'an',\n",
              " 'additional',\n",
              " 'constraint',\n",
              " 'into',\n",
              " 'the',\n",
              " 'optimization',\n",
              " 'problem',\n",
              " 'for',\n",
              " 'they',\n",
              " 'arrive',\n",
              " 'at',\n",
              " 'wm',\n",
              " 'pp',\n",
              " 'inf',\n",
              " 'wm',\n",
              " 'xi',\n",
              " 'yi',\n",
              " 'Our',\n",
              " 'first',\n",
              " 'contribution',\n",
              " 'shows',\n",
              " 'that',\n",
              " 'both',\n",
              " 'the',\n",
              " 'Tikhonov',\n",
              " 'regularization',\n",
              " 'in',\n",
              " 'Eq',\n",
              " 'and',\n",
              " 'the',\n",
              " 'Ivanov',\n",
              " 'regularization',\n",
              " 'in',\n",
              " 'Eq',\n",
              " 'are',\n",
              " 'equivalent',\n",
              " 'there',\n",
              " 'exists',\n",
              " 'such',\n",
              " 'that',\n",
              " 'for',\n",
              " 'each',\n",
              " 'optimal',\n",
              " 'Theorem',\n",
              " 'Let',\n",
              " 'be',\n",
              " 'for',\n",
              " 'each',\n",
              " 'pair',\n",
              " 'we',\n",
              " 'have',\n",
              " 'that',\n",
              " 'is',\n",
              " 'also',\n",
              " 'an',\n",
              " 'optimal',\n",
              " 'solution',\n",
              " 'solution',\n",
              " 'of',\n",
              " 'Eq',\n",
              " 'using',\n",
              " 'of',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# dictionary to store the tokens as the values and paper id as the key\n",
        "token_dict = {}\n",
        "# instansiates tokenizer accroding to the regex pattern passed\n",
        "tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
        "# loops through the normalized content \n",
        "for key,value in sentence_seg.items():\n",
        "  # tokenizes the paper body content for each individual paper\n",
        "  tokens = tokenizer.tokenize(value)\n",
        "  # stores the generated tokens\n",
        "  token_dict[key] = tokens\n",
        "token_dict[\"PP3675\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqZos1q6lDEf"
      },
      "source": [
        "The above operation results in a dictionary with paper id as the key and all the paper content tokenized and stored as values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OPBNTTq6lDEg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f966a73a-898d-48d7-9219-61f2f99c4181"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'inference'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# generates a list of all the tokens from the dictionary for all the papers\n",
        "token_list = list(chain.from_iterable(token_dict.values()))\n",
        "token_list[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NglwwiJRnPZd"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 4.2. Sparse Feature Generation <a class=\"anchor\" name=\"whetev\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopword/ stop tokens are those tokens which we don't need in our vocablary. These are extracted from the file provided in the sharedrive. We store these stopwords in a list."
      ],
      "metadata": {
        "id": "drEN_aOLIRpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list to store all the stop words\n",
        "stopwords_list = []\n",
        "# opens the file containing the stop words\n",
        "with open('/content/drive/Shareddrives/FIT5196_S1_2023/Assessment1/stopwords_en.txt') as f:\n",
        "  # adds the stop words to the list\n",
        "  stopwords_list = f.read().splitlines() \n",
        "stopwords_list[5]"
      ],
      "metadata": {
        "id": "YYDeYaP05qFW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3eacff57-c15e-4bd7-fe84-f1f7cf2bcf76"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'according'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Ve6IZ2I-lDEg"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 4.3. Finding First 200 Bigrams <a class=\"anchor\" name=\"bigrams\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erGhUY2UlDEg"
      },
      "source": [
        "One of the tasks is to find the first 200 bigrams based on frequency. These bigrams should also be included in our vocabulary list. We need to make sure that the bigrams are not in the stopwords. After we have our bigrams we check if they are present in the stopwaords list or not. If our bigrams are not part of stop word only then we add them to our vocabulary. Also we need to make sure we have only the 200 most frequent bigrams in our vocab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Y2W3jnURPodX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4009e344-6765-4da2-d636-79fb28a21412"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('probability', 'distributions')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# generates a list containing set of bigrams from the token list\n",
        "bigram_list = list(nltk.bigrams(token_list))\n",
        "# list to store the bigrams that are not included in the stop words list\n",
        "fltrd_bgrms = []\n",
        "for bigrams in bigram_list:\n",
        "  # checks if the words in bigrams are contained in teh stop words\n",
        "  if bigrams[0] not in stopwords_list and bigrams[1] not in stopwords_list:\n",
        "    fltrd_bgrms.append(bigrams)\n",
        "fltrd_bgrms[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Q3nwMWzBXfMm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a162214-9f85-4061-ae02-6ccdfbe4e655"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('x1', 'x2')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# creates a iter object of Counter type containing bigrams ad their frequencies of occurances\n",
        "bgrms_frq = Counter(fltrd_bgrms)\n",
        "# returns a list of most frequently occured 200 bigrams along with their frequencies\n",
        "most_frq_bgrms = bgrms_frq.most_common(200)\n",
        "top_200_bgrms = []\n",
        "for values in most_frq_bgrms:\n",
        "  top_200_bgrms.append(values[0])\n",
        "top_200_bgrms[7]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We add the bigrams to our existing tokens list by concatenating the bigram with __ using MWETokenizer. We make our vocab list by converting the list of the updated tokens into a set which eleminated the duplicates and gives us unique tokens."
      ],
      "metadata": {
        "id": "nDVqjYHnJXiq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-hDrmwEylDEg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ecd9104e-485c-4447-c32c-63c0d842ba57"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'finalized'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# creates a new tokenizer object with the bigrams concatenated with __\n",
        "mwetokenizer = MWETokenizer(top_200_bgrms, separator = \"__\")\n",
        "# creates a new dict object from the previos tokens and updates them with the new bigrams as tokens\n",
        "colloc_dict =  dict((paper_id, mwetokenizer.tokenize(values)) for paper_id,values in token_dict.items())\n",
        "# creates a list of all the updated tokens unigrams and top 200 bigrams\n",
        "all_words_colloc = list(chain.from_iterable(colloc_dict.values()))\n",
        "# creates a list of all unique tokens\n",
        "colloc_voc = list(set(all_words_colloc))\n",
        "colloc_voc[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO1PJO-dlDEh"
      },
      "source": [
        "At this stage, we have a dictionary of tokenized words, whose keys are indicative of file they are contained in.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VfjoJRgdUMcT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c728cf-b9c2-4e34-e363-c4d06b41f504"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sparseness',\n",
              " 'is',\n",
              " 'being',\n",
              " 'regarded',\n",
              " 'as',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'key',\n",
              " 'features',\n",
              " 'in',\n",
              " 'machine__learning',\n",
              " 'and',\n",
              " 'biology',\n",
              " 'sparse',\n",
              " 'models',\n",
              " 'are',\n",
              " 'appealing',\n",
              " 'since',\n",
              " 'they',\n",
              " 'provide',\n",
              " 'an',\n",
              " 'intuitive',\n",
              " 'interpretation',\n",
              " 'of',\n",
              " 'task',\n",
              " 'at',\n",
              " 'hand',\n",
              " 'by',\n",
              " 'singling',\n",
              " 'out',\n",
              " 'relevant',\n",
              " 'pieces',\n",
              " 'of',\n",
              " 'information',\n",
              " 'such',\n",
              " 'automatic',\n",
              " 'complexity',\n",
              " 'reduction',\n",
              " 'facilitates',\n",
              " 'efficient',\n",
              " 'training',\n",
              " 'algorithms',\n",
              " 'and',\n",
              " 'the',\n",
              " 'resulting',\n",
              " 'models',\n",
              " 'are',\n",
              " 'distinguished',\n",
              " 'by',\n",
              " 'small',\n",
              " 'capacity',\n",
              " 'the',\n",
              " 'interpretability',\n",
              " 'is',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'main',\n",
              " 'reasons',\n",
              " 'for',\n",
              " 'the',\n",
              " 'popularity',\n",
              " 'of',\n",
              " 'sparse',\n",
              " 'methods',\n",
              " 'in',\n",
              " 'complex',\n",
              " 'domains',\n",
              " 'such',\n",
              " 'as',\n",
              " 'computational',\n",
              " 'biology',\n",
              " 'and',\n",
              " 'consequently',\n",
              " 'building',\n",
              " 'sparse',\n",
              " 'models',\n",
              " 'from',\n",
              " 'data',\n",
              " 'has',\n",
              " 'received',\n",
              " 'significant',\n",
              " 'amount',\n",
              " 'of',\n",
              " 'recent',\n",
              " 'attention',\n",
              " 'unfortunately',\n",
              " 'sparse',\n",
              " 'models',\n",
              " 'do',\n",
              " 'not',\n",
              " 'always',\n",
              " 'perform',\n",
              " 'well',\n",
              " 'in',\n",
              " 'practice',\n",
              " 'this',\n",
              " 'holds',\n",
              " 'particularly',\n",
              " 'for',\n",
              " 'learning',\n",
              " 'sparse',\n",
              " 'linear',\n",
              " 'combinations',\n",
              " 'of',\n",
              " 'data',\n",
              " 'sources',\n",
              " 'an',\n",
              " 'abstraction',\n",
              " 'of',\n",
              " 'which',\n",
              " 'is',\n",
              " 'known',\n",
              " 'as',\n",
              " 'multiple',\n",
              " 'kernel',\n",
              " 'learning',\n",
              " 'MKL',\n",
              " 'the',\n",
              " 'data',\n",
              " 'sources',\n",
              " 'give',\n",
              " 'rise',\n",
              " 'to',\n",
              " 'set',\n",
              " 'of',\n",
              " 'possibly',\n",
              " 'correlated',\n",
              " 'kernel',\n",
              " 'matrices',\n",
              " 'K1',\n",
              " 'KM',\n",
              " 'and',\n",
              " 'the',\n",
              " 'task',\n",
              " 'is',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'the',\n",
              " 'optimal',\n",
              " 'mixture',\n",
              " 'Km',\n",
              " 'for',\n",
              " 'the',\n",
              " 'problem',\n",
              " 'at',\n",
              " 'hand',\n",
              " 'previous',\n",
              " 'MKL',\n",
              " 'research',\n",
              " 'aims',\n",
              " 'at',\n",
              " 'finding',\n",
              " 'sparse',\n",
              " 'mixtures',\n",
              " 'to',\n",
              " 'effectively',\n",
              " 'simplify',\n",
              " 'the',\n",
              " 'underlying',\n",
              " 'data',\n",
              " 'representation',\n",
              " 'for',\n",
              " 'instance',\n",
              " 'study',\n",
              " 'semi',\n",
              " 'definite',\n",
              " 'matrices',\n",
              " 'inducing',\n",
              " 'sparseness',\n",
              " 'by',\n",
              " 'bounding',\n",
              " 'the',\n",
              " 'trace',\n",
              " 'tr',\n",
              " 'unfortunately',\n",
              " 'the',\n",
              " 'resulting',\n",
              " 'semi',\n",
              " 'definite',\n",
              " 'optimization',\n",
              " 'problems',\n",
              " 'are',\n",
              " 'computationally',\n",
              " 'too',\n",
              " 'expensive',\n",
              " 'for',\n",
              " 'large__scale',\n",
              " 'deployment',\n",
              " 'recent',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'MKL',\n",
              " 'promote',\n",
              " 'sparse',\n",
              " 'solutions',\n",
              " 'either',\n",
              " 'by',\n",
              " 'Tikhonov',\n",
              " 'regularization',\n",
              " 'over',\n",
              " 'the',\n",
              " 'mixing',\n",
              " 'coefficients',\n",
              " 'or',\n",
              " 'by',\n",
              " 'incorporating',\n",
              " 'an',\n",
              " 'additional',\n",
              " 'constraint',\n",
              " 'requiring',\n",
              " 'solutions',\n",
              " 'on',\n",
              " 'the',\n",
              " 'standard',\n",
              " 'simplex',\n",
              " 'known',\n",
              " 'as',\n",
              " 'Ivanov',\n",
              " 'regularization',\n",
              " 'based',\n",
              " 'on',\n",
              " 'the',\n",
              " 'one',\n",
              " 'or',\n",
              " 'the',\n",
              " 'other',\n",
              " 'efficient',\n",
              " 'optimization',\n",
              " 'strategies',\n",
              " 'have',\n",
              " 'been',\n",
              " 'proposed',\n",
              " 'for',\n",
              " 'solving',\n",
              " 'norm__MKL',\n",
              " 'using',\n",
              " 'semi',\n",
              " 'infinite',\n",
              " 'linear',\n",
              " 'programming',\n",
              " 'second',\n",
              " 'order',\n",
              " 'approaches',\n",
              " 'gradient',\n",
              " 'based',\n",
              " 'optimization',\n",
              " 'and',\n",
              " 'levelset',\n",
              " 'methods',\n",
              " 'other',\n",
              " 'variants',\n",
              " 'of',\n",
              " 'norm__MKL',\n",
              " 'have',\n",
              " 'been',\n",
              " 'proposed',\n",
              " 'in',\n",
              " 'subsequent',\n",
              " 'work',\n",
              " 'addressing',\n",
              " 'practical',\n",
              " 'algorithms',\n",
              " 'for',\n",
              " 'multi',\n",
              " 'class',\n",
              " 'and',\n",
              " 'multi',\n",
              " 'label',\n",
              " 'problems',\n",
              " 'Previous',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'MKL',\n",
              " 'successfully',\n",
              " 'identify',\n",
              " 'sparse',\n",
              " 'kernel',\n",
              " 'mixtures',\n",
              " 'however',\n",
              " 'the',\n",
              " 'solutions',\n",
              " 'found',\n",
              " 'frequently',\n",
              " 'suffer',\n",
              " 'fromP',\n",
              " 'poor',\n",
              " 'generalization',\n",
              " 'performances',\n",
              " 'often',\n",
              " 'trivial',\n",
              " 'baselines',\n",
              " 'using',\n",
              " 'unweighted',\n",
              " 'sum',\n",
              " 'kernels',\n",
              " 'Km',\n",
              " 'are',\n",
              " 'observed',\n",
              " 'to',\n",
              " 'outperform',\n",
              " 'the',\n",
              " 'sparse',\n",
              " 'mixture',\n",
              " 'one',\n",
              " 'reason',\n",
              " 'for',\n",
              " 'the',\n",
              " 'collapse',\n",
              " 'of',\n",
              " 'norm__MKL',\n",
              " 'is',\n",
              " 'that',\n",
              " 'kernels',\n",
              " 'deployed',\n",
              " 'in',\n",
              " 'real__world',\n",
              " 'tasks',\n",
              " 'are',\n",
              " 'usually',\n",
              " 'highly',\n",
              " 'sophisticated',\n",
              " 'and',\n",
              " 'effectively',\n",
              " 'capture',\n",
              " 'relevant',\n",
              " 'aspects',\n",
              " 'of',\n",
              " 'the',\n",
              " 'data',\n",
              " 'in',\n",
              " 'contrast',\n",
              " 'sparse',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'MKL',\n",
              " 'rely',\n",
              " 'on',\n",
              " 'the',\n",
              " 'assumption',\n",
              " 'that',\n",
              " 'some',\n",
              " 'kernels',\n",
              " 'are',\n",
              " 'irrelevant',\n",
              " 'for',\n",
              " 'solving',\n",
              " 'the',\n",
              " 'problem',\n",
              " 'enforcing',\n",
              " 'sparse',\n",
              " 'mixtures',\n",
              " 'in',\n",
              " 'these',\n",
              " 'situations',\n",
              " 'may',\n",
              " 'lead',\n",
              " 'to',\n",
              " 'degenerate',\n",
              " 'models',\n",
              " 'as',\n",
              " 'remedy',\n",
              " 'we',\n",
              " 'propose',\n",
              " 'to',\n",
              " 'sacrifice',\n",
              " 'sparseness',\n",
              " 'in',\n",
              " 'these',\n",
              " 'situations',\n",
              " 'and',\n",
              " 'deploy',\n",
              " 'non',\n",
              " 'sparse',\n",
              " 'mixtures',\n",
              " 'instead',\n",
              " 'after',\n",
              " 'submission',\n",
              " 'of',\n",
              " 'this',\n",
              " 'paper',\n",
              " 'we',\n",
              " 'learned',\n",
              " 'about',\n",
              " 'related',\n",
              " 'approach',\n",
              " 'in',\n",
              " 'which',\n",
              " 'the',\n",
              " 'sum',\n",
              " 'of',\n",
              " 'an',\n",
              " 'and',\n",
              " 'an',\n",
              " 'regularizer',\n",
              " 'are',\n",
              " 'used',\n",
              " 'although',\n",
              " 'non',\n",
              " 'sparse',\n",
              " 'solutions',\n",
              " 'are',\n",
              " 'not',\n",
              " 'as',\n",
              " 'easy',\n",
              " 'to',\n",
              " 'interpret',\n",
              " 'they',\n",
              " 'account',\n",
              " 'for',\n",
              " 'even',\n",
              " 'small',\n",
              " 'contributions',\n",
              " 'of',\n",
              " 'all',\n",
              " 'available',\n",
              " 'kernels',\n",
              " 'to',\n",
              " 'live',\n",
              " 'up',\n",
              " 'to',\n",
              " 'practical',\n",
              " 'applications',\n",
              " 'in',\n",
              " 'this',\n",
              " 'paper',\n",
              " 'we',\n",
              " 'first',\n",
              " 'show',\n",
              " 'the',\n",
              " 'equivalence',\n",
              " 'of',\n",
              " 'the',\n",
              " 'most',\n",
              " 'common',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'norm__MKL',\n",
              " 'our',\n",
              " 'theorem',\n",
              " 'allows',\n",
              " 'for',\n",
              " 'generalized',\n",
              " 'view',\n",
              " 'of',\n",
              " 'recent',\n",
              " 'strands',\n",
              " 'of',\n",
              " 'multiple',\n",
              " 'kernel',\n",
              " 'learning',\n",
              " 'research',\n",
              " 'based',\n",
              " 'on',\n",
              " 'the',\n",
              " 'detached',\n",
              " 'view',\n",
              " 'we',\n",
              " 'extend',\n",
              " 'the',\n",
              " 'MKL',\n",
              " 'framework',\n",
              " 'to',\n",
              " 'arbitrary',\n",
              " 'norm__MKL',\n",
              " 'with',\n",
              " 'our',\n",
              " 'approach',\n",
              " 'can',\n",
              " 'either',\n",
              " 'be',\n",
              " 'motivated',\n",
              " 'by',\n",
              " 'additionally',\n",
              " 'regularizing',\n",
              " 'over',\n",
              " 'the',\n",
              " 'mixing',\n",
              " 'coefficients',\n",
              " 'kpp',\n",
              " 'or',\n",
              " 'equivalently',\n",
              " 'by',\n",
              " 'incorporating',\n",
              " 'the',\n",
              " 'constraint',\n",
              " 'kpp',\n",
              " 'we',\n",
              " 'propose',\n",
              " 'two',\n",
              " 'alternative',\n",
              " 'optimization',\n",
              " 'strategies',\n",
              " 'based',\n",
              " 'on',\n",
              " 'Newton',\n",
              " 'descent',\n",
              " 'and',\n",
              " 'cutting',\n",
              " 'planes',\n",
              " 'respectively',\n",
              " 'empirically',\n",
              " 'we',\n",
              " 'demonstrate',\n",
              " 'the',\n",
              " 'efficiency',\n",
              " 'and',\n",
              " 'accuracy',\n",
              " 'of',\n",
              " 'none',\n",
              " 'sparse',\n",
              " 'MKL',\n",
              " 'large__scale',\n",
              " 'experiments',\n",
              " 'on',\n",
              " 'gene',\n",
              " 'start',\n",
              " 'detection',\n",
              " 'show',\n",
              " 'significant',\n",
              " 'improvement',\n",
              " 'of',\n",
              " 'predictive',\n",
              " 'accuracy',\n",
              " 'compared',\n",
              " 'to',\n",
              " 'and',\n",
              " 'norm__MKL',\n",
              " 'the',\n",
              " 'rest',\n",
              " 'of',\n",
              " 'the',\n",
              " 'paper',\n",
              " 'is',\n",
              " 'structured',\n",
              " 'as',\n",
              " 'follows',\n",
              " 'we',\n",
              " 'present',\n",
              " 'our',\n",
              " 'main',\n",
              " 'contributions',\n",
              " 'in',\n",
              " 'Section',\n",
              " 'the',\n",
              " 'theoretical__analysis',\n",
              " 'of',\n",
              " 'existing',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'MKL',\n",
              " 'our',\n",
              " 'norm__MKL',\n",
              " 'generalization',\n",
              " 'with',\n",
              " 'two',\n",
              " 'highly',\n",
              " 'efficient',\n",
              " 'optimization',\n",
              " 'strategies',\n",
              " 'and',\n",
              " 'relations',\n",
              " 'to',\n",
              " 'norm__MKL',\n",
              " 'we',\n",
              " 'report',\n",
              " 'on',\n",
              " 'our',\n",
              " 'empirical',\n",
              " 'results',\n",
              " 'in',\n",
              " 'Section',\n",
              " 'and',\n",
              " 'Section',\n",
              " 'concludes',\n",
              " 'Generalized',\n",
              " 'Multiple',\n",
              " 'Kernel',\n",
              " 'Learning',\n",
              " 'Preliminaries',\n",
              " 'In',\n",
              " 'the',\n",
              " 'standard',\n",
              " 'supervised__learning',\n",
              " 'setup',\n",
              " 'labeled',\n",
              " 'sample',\n",
              " 'xi__yi',\n",
              " 'is',\n",
              " 'given',\n",
              " 'where',\n",
              " 'the',\n",
              " 'lie',\n",
              " 'in',\n",
              " 'some',\n",
              " 'input',\n",
              " 'space',\n",
              " 'and',\n",
              " 'The',\n",
              " 'goal',\n",
              " 'is',\n",
              " 'to',\n",
              " 'find',\n",
              " 'hypothesis',\n",
              " 'that',\n",
              " 'generalizes',\n",
              " 'well',\n",
              " 'on',\n",
              " 'new',\n",
              " 'and',\n",
              " 'unseen',\n",
              " 'data',\n",
              " 'applying',\n",
              " 'regularized',\n",
              " 'risk',\n",
              " 'minimization',\n",
              " 'returns',\n",
              " 'the',\n",
              " 'minimizer',\n",
              " 'argminf',\n",
              " 'Remp',\n",
              " 'where',\n",
              " 'Remp',\n",
              " 'n1',\n",
              " 'xi__yi',\n",
              " 'is',\n",
              " 'the',\n",
              " 'empirical__risk',\n",
              " 'of',\n",
              " 'hypothesis',\n",
              " 'to',\n",
              " 'the',\n",
              " 'loss',\n",
              " 'regularizer',\n",
              " 'and',\n",
              " 'trade',\n",
              " 'off',\n",
              " 'parameter',\n",
              " 'in',\n",
              " 'this',\n",
              " 'paper',\n",
              " 'we',\n",
              " 'focus',\n",
              " 'on',\n",
              " 'and',\n",
              " 'on',\n",
              " 'linear__models',\n",
              " 'of',\n",
              " 'the',\n",
              " 'form',\n",
              " 'kwk',\n",
              " 'fw',\n",
              " 'together',\n",
              " 'with',\n",
              " 'possibly',\n",
              " 'non',\n",
              " 'linear',\n",
              " 'mapping',\n",
              " 'to',\n",
              " 'Hilbert',\n",
              " 'space',\n",
              " 'we',\n",
              " 'will',\n",
              " 'later',\n",
              " 'make',\n",
              " 'use',\n",
              " 'of',\n",
              " 'kernel',\n",
              " 'functions',\n",
              " 'x0__x0',\n",
              " 'iH',\n",
              " 'to',\n",
              " 'compute',\n",
              " 'inner',\n",
              " 'products',\n",
              " 'in',\n",
              " 'Learning',\n",
              " 'with',\n",
              " 'Multiple',\n",
              " 'Kernels',\n",
              " 'When',\n",
              " 'learning',\n",
              " 'with',\n",
              " 'multiple',\n",
              " 'kernels',\n",
              " 'we',\n",
              " 'are',\n",
              " 'given',\n",
              " 'different',\n",
              " 'feature',\n",
              " 'map',\n",
              " 'pings',\n",
              " 'hm',\n",
              " 'each',\n",
              " 'giving',\n",
              " 'rise',\n",
              " 'to',\n",
              " 'reproducing',\n",
              " 'kernel',\n",
              " 'Km',\n",
              " 'of',\n",
              " 'Hm',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'multiple',\n",
              " 'kernel',\n",
              " 'learning',\n",
              " 'consider',\n",
              " 'linear',\n",
              " 'kernel',\n",
              " 'mixtures',\n",
              " 'Km',\n",
              " 'compared',\n",
              " 'to',\n",
              " 'Eq',\n",
              " 'the',\n",
              " 'primal',\n",
              " 'model',\n",
              " 'for',\n",
              " 'learning',\n",
              " 'with',\n",
              " 'multiple',\n",
              " 'kernels',\n",
              " 'is',\n",
              " 'extended',\n",
              " 'to',\n",
              " 'xi',\n",
              " 'fw',\n",
              " 'and',\n",
              " 'the',\n",
              " 'composite',\n",
              " 'where',\n",
              " 'the',\n",
              " 'weight',\n",
              " 'vector',\n",
              " 'feature',\n",
              " 'map',\n",
              " 'have',\n",
              " 'block',\n",
              " 'structure',\n",
              " 'and',\n",
              " 'respectively',\n",
              " 'The',\n",
              " 'idea',\n",
              " 'in',\n",
              " 'learning',\n",
              " 'with',\n",
              " 'multiple',\n",
              " 'kernels',\n",
              " 'is',\n",
              " 'to',\n",
              " 'minimize',\n",
              " 'the',\n",
              " 'loss',\n",
              " 'on',\n",
              " 'the',\n",
              " 'training__data',\n",
              " 'to',\n",
              " 'optimal',\n",
              " 'kernel',\n",
              " 'mixture',\n",
              " 'Km',\n",
              " 'in',\n",
              " 'addition',\n",
              " 'to',\n",
              " 'regularizing',\n",
              " 'to',\n",
              " 'avoid',\n",
              " 'overfitting',\n",
              " 'hence',\n",
              " 'in',\n",
              " 'terms',\n",
              " 'of',\n",
              " 'regularized',\n",
              " 'risk',\n",
              " 'minimization',\n",
              " 'the',\n",
              " 'optimization__problem',\n",
              " 'becomes',\n",
              " 'inf',\n",
              " 'k22',\n",
              " 'kw',\n",
              " 'fw',\n",
              " 'xi__yi',\n",
              " 'previous',\n",
              " 'approaches',\n",
              " 'to',\n",
              " 'multiple',\n",
              " 'kernel',\n",
              " 'learning',\n",
              " 'employ',\n",
              " 'regularizers',\n",
              " 'of',\n",
              " 'the',\n",
              " 'form',\n",
              " 'to',\n",
              " 'promote',\n",
              " 'sparse',\n",
              " 'kernel',\n",
              " 'mixtures',\n",
              " 'by',\n",
              " 'contrast',\n",
              " 'we',\n",
              " 'propose',\n",
              " 'to',\n",
              " 'use',\n",
              " 'smooth',\n",
              " 'convex',\n",
              " 'regularizers',\n",
              " 'of',\n",
              " 'the',\n",
              " 'form',\n",
              " 'pp',\n",
              " 'allowing',\n",
              " 'for',\n",
              " 'non',\n",
              " 'sparse',\n",
              " 'solutions',\n",
              " 'the',\n",
              " 'non',\n",
              " 'convexity',\n",
              " 'of',\n",
              " 'the',\n",
              " 'resulting',\n",
              " 'optimization__problem',\n",
              " 'is',\n",
              " 'not',\n",
              " 'inherent',\n",
              " 'and',\n",
              " 'can',\n",
              " 'be',\n",
              " 'resolved',\n",
              " 'by',\n",
              " 'substituting',\n",
              " 'wm',\n",
              " 'Furthermore',\n",
              " 'regularization',\n",
              " 'parameter',\n",
              " 'and',\n",
              " 'sample__size',\n",
              " 'can',\n",
              " 'be',\n",
              " 'decoupled',\n",
              " 'by',\n",
              " 'introducing',\n",
              " 'and',\n",
              " 'adjusting',\n",
              " 'which',\n",
              " 'has',\n",
              " 'favorable',\n",
              " 'scaling',\n",
              " 'properties',\n",
              " 'in',\n",
              " 'practice',\n",
              " 'we',\n",
              " 'obtain',\n",
              " 'the',\n",
              " 'following',\n",
              " 'convex__optimization',\n",
              " 'problem',\n",
              " 'that',\n",
              " 'has',\n",
              " 'also',\n",
              " 'been',\n",
              " 'considered',\n",
              " 'by',\n",
              " 'for',\n",
              " 'hinge',\n",
              " 'loss',\n",
              " 'and',\n",
              " 'kwm',\n",
              " 'k22',\n",
              " 'inf',\n",
              " 'pp',\n",
              " 'wm',\n",
              " 'xi__yi',\n",
              " 'where',\n",
              " 'we',\n",
              " 'use',\n",
              " 'the',\n",
              " 'convention',\n",
              " 'that',\n",
              " 'if',\n",
              " 'and',\n",
              " 'otherwise',\n",
              " 'an',\n",
              " 'alternative',\n",
              " 'approach',\n",
              " 'has',\n",
              " 'been',\n",
              " 'studied',\n",
              " 'by',\n",
              " 'again',\n",
              " 'using',\n",
              " 'hinge',\n",
              " 'loss',\n",
              " 'and',\n",
              " 'they',\n",
              " 'upper__bound',\n",
              " 'the',\n",
              " 'value',\n",
              " 'of',\n",
              " 'the',\n",
              " 'regularizer',\n",
              " 'k1',\n",
              " 'and',\n",
              " 'incorporate',\n",
              " 'the',\n",
              " 'latter',\n",
              " 'as',\n",
              " 'an',\n",
              " 'additional',\n",
              " 'constraint',\n",
              " 'into',\n",
              " 'the',\n",
              " 'optimization__problem',\n",
              " 'for',\n",
              " 'they',\n",
              " 'arrive',\n",
              " 'at',\n",
              " 'wm',\n",
              " 'pp',\n",
              " 'inf',\n",
              " 'wm',\n",
              " 'xi__yi',\n",
              " 'Our',\n",
              " 'first',\n",
              " 'contribution',\n",
              " 'shows',\n",
              " 'that',\n",
              " 'both',\n",
              " 'the',\n",
              " 'Tikhonov',\n",
              " 'regularization',\n",
              " 'in',\n",
              " 'Eq',\n",
              " 'and',\n",
              " 'the',\n",
              " 'Ivanov',\n",
              " 'regularization',\n",
              " 'in',\n",
              " 'Eq',\n",
              " 'are',\n",
              " 'equivalent',\n",
              " 'there',\n",
              " 'exists',\n",
              " 'such',\n",
              " 'that',\n",
              " 'for',\n",
              " 'each',\n",
              " 'optimal',\n",
              " 'Theorem',\n",
              " 'Let',\n",
              " 'be',\n",
              " 'for',\n",
              " 'each',\n",
              " 'pair',\n",
              " 'we',\n",
              " 'have',\n",
              " 'that',\n",
              " 'is',\n",
              " 'also',\n",
              " 'an',\n",
              " 'optimal__solution',\n",
              " 'solution',\n",
              " 'of',\n",
              " 'Eq',\n",
              " 'using',\n",
              " 'of',\n",
              " 'Eq',\n",
              " 'using',\n",
              " 'and',\n",
              " 'vice',\n",
              " 'versa',\n",
              " 'where',\n",
              " 'is',\n",
              " 'some',\n",
              " 'multiplicative',\n",
              " 'constant',\n",
              " 'proof',\n",
              " 'the',\n",
              " 'proof',\n",
              " 'is',\n",
              " 'shown',\n",
              " 'in',\n",
              " 'the',\n",
              " 'supplementary__material',\n",
              " 'for',\n",
              " 'lack',\n",
              " 'of',\n",
              " 'space',\n",
              " 'sketch',\n",
              " 'of',\n",
              " 'the',\n",
              " 'proof',\n",
              " 'We',\n",
              " 'incorporate',\n",
              " 'the',\n",
              " 'regularizer',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# dictionary to store the tokens that are not in the stop words\n",
        "stopped_token_dict = {}\n",
        "# iterates through the updated dict containing bigrams\n",
        "for keys, values in colloc_dict.items():\n",
        "    stopped_tokens = []\n",
        "    for tokens in values:\n",
        "      if values not in stopwords_list:\n",
        "        stopped_tokens.append(tokens)\n",
        "    # adds the tokens which are not stop words to the dict\n",
        "    stopped_token_dict[keys] = stopped_tokens\n",
        "stopped_token_dict[\"PP3675\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We eleminate the rare tokens. For this we need to make sure the frequency of occurance of each token should be less than 95% and more than 3% and token is of more than 3 characters."
      ],
      "metadata": {
        "id": "ivzqtQPuKTV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stores the tokens which are not rare\n",
        "rare_token_dict = {}\n",
        "rare_tokens = []\n",
        "# list of all the tokens without stop words\n",
        "tokens = list(chain.from_iterable([set(value) for value in stopped_token_dict.values()]))\n",
        "# iterable object which stores the token as key and its frequency of occurance as value\n",
        "token_frq = FreqDist(tokens).most_common(len(colloc_voc))\n",
        "for tokens in token_frq:\n",
        "  # if not rare tokens\n",
        "  if len(tokens[0]) >= 3 and ((tokens[1]/len(stopped_token_dict)) <= 0.95) and ((tokens[1]/len(stopped_token_dict)) >= 0.03):\n",
        "    rare_tokens.append(tokens[0])\n",
        "for keys, values in stopped_token_dict.items():\n",
        "  rare_val = []\n",
        "  for tokens in values:\n",
        "    if tokens in rare_tokens:  \n",
        "      rare_val.append(tokens)\n",
        "  # updated dict storing the tokens that are not rare\n",
        "  rare_token_dict[keys] = rare_val\n",
        "rare_token_dict[\"PP3675\"]\n"
      ],
      "metadata": {
        "id": "s8V2AASEDjby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba22124c-c3d5-489c-df63-93c3aa0cc890"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['being',\n",
              " 'regarded',\n",
              " 'key',\n",
              " 'features',\n",
              " 'machine__learning',\n",
              " 'biology',\n",
              " 'sparse',\n",
              " 'models',\n",
              " 'appealing',\n",
              " 'since',\n",
              " 'they',\n",
              " 'provide',\n",
              " 'intuitive',\n",
              " 'interpretation',\n",
              " 'task',\n",
              " 'hand',\n",
              " 'out',\n",
              " 'relevant',\n",
              " 'pieces',\n",
              " 'information',\n",
              " 'automatic',\n",
              " 'complexity',\n",
              " 'reduction',\n",
              " 'facilitates',\n",
              " 'efficient',\n",
              " 'training',\n",
              " 'algorithms',\n",
              " 'resulting',\n",
              " 'models',\n",
              " 'small',\n",
              " 'capacity',\n",
              " 'interpretability',\n",
              " 'main',\n",
              " 'reasons',\n",
              " 'popularity',\n",
              " 'sparse',\n",
              " 'methods',\n",
              " 'complex',\n",
              " 'domains',\n",
              " 'computational',\n",
              " 'biology',\n",
              " 'consequently',\n",
              " 'building',\n",
              " 'sparse',\n",
              " 'models',\n",
              " 'data',\n",
              " 'received',\n",
              " 'significant',\n",
              " 'amount',\n",
              " 'recent',\n",
              " 'attention',\n",
              " 'unfortunately',\n",
              " 'sparse',\n",
              " 'models',\n",
              " 'always',\n",
              " 'perform',\n",
              " 'well',\n",
              " 'practice',\n",
              " 'holds',\n",
              " 'particularly',\n",
              " 'learning',\n",
              " 'sparse',\n",
              " 'linear',\n",
              " 'combinations',\n",
              " 'data',\n",
              " 'sources',\n",
              " 'abstraction',\n",
              " 'known',\n",
              " 'multiple',\n",
              " 'kernel',\n",
              " 'learning',\n",
              " 'data',\n",
              " 'sources',\n",
              " 'give',\n",
              " 'rise',\n",
              " 'possibly',\n",
              " 'correlated',\n",
              " 'kernel',\n",
              " 'matrices',\n",
              " 'task',\n",
              " 'learn',\n",
              " 'optimal',\n",
              " 'mixture',\n",
              " 'problem',\n",
              " 'hand',\n",
              " 'previous',\n",
              " 'research',\n",
              " 'aims',\n",
              " 'finding',\n",
              " 'sparse',\n",
              " 'mixtures',\n",
              " 'effectively',\n",
              " 'simplify',\n",
              " 'underlying',\n",
              " 'data',\n",
              " 'representation',\n",
              " 'instance',\n",
              " 'study',\n",
              " 'semi',\n",
              " 'definite',\n",
              " 'matrices',\n",
              " 'inducing',\n",
              " 'bounding',\n",
              " 'trace',\n",
              " 'unfortunately',\n",
              " 'resulting',\n",
              " 'semi',\n",
              " 'definite',\n",
              " 'optimization',\n",
              " 'problems',\n",
              " 'computationally',\n",
              " 'too',\n",
              " 'expensive',\n",
              " 'large__scale',\n",
              " 'recent',\n",
              " 'approaches',\n",
              " 'promote',\n",
              " 'sparse',\n",
              " 'solutions',\n",
              " 'either',\n",
              " 'regularization',\n",
              " 'mixing',\n",
              " 'coefficients',\n",
              " 'incorporating',\n",
              " 'additional',\n",
              " 'constraint',\n",
              " 'requiring',\n",
              " 'solutions',\n",
              " 'standard',\n",
              " 'simplex',\n",
              " 'known',\n",
              " 'regularization',\n",
              " 'based',\n",
              " 'efficient',\n",
              " 'optimization',\n",
              " 'strategies',\n",
              " 'been',\n",
              " 'proposed',\n",
              " 'solving',\n",
              " 'semi',\n",
              " 'infinite',\n",
              " 'linear',\n",
              " 'programming',\n",
              " 'second',\n",
              " 'order',\n",
              " 'approaches',\n",
              " 'gradient',\n",
              " 'based',\n",
              " 'optimization',\n",
              " 'methods',\n",
              " 'variants',\n",
              " 'been',\n",
              " 'proposed',\n",
              " 'subsequent',\n",
              " 'work',\n",
              " 'addressing',\n",
              " 'practical',\n",
              " 'algorithms',\n",
              " 'multi',\n",
              " 'class',\n",
              " 'multi',\n",
              " 'label',\n",
              " 'problems',\n",
              " 'Previous',\n",
              " 'approaches',\n",
              " 'successfully',\n",
              " 'identify',\n",
              " 'sparse',\n",
              " 'kernel',\n",
              " 'mixtures',\n",
              " 'however',\n",
              " 'solutions',\n",
              " 'found',\n",
              " 'frequently',\n",
              " 'suffer',\n",
              " 'poor',\n",
              " 'generalization',\n",
              " 'performances',\n",
              " 'often',\n",
              " 'trivial',\n",
              " 'baselines',\n",
              " 'sum',\n",
              " 'kernels',\n",
              " 'observed',\n",
              " 'outperform',\n",
              " 'sparse',\n",
              " 'mixture',\n",
              " 'reason',\n",
              " 'collapse',\n",
              " 'kernels',\n",
              " 'deployed',\n",
              " 'real__world',\n",
              " 'tasks',\n",
              " 'usually',\n",
              " 'highly',\n",
              " 'sophisticated',\n",
              " 'effectively',\n",
              " 'capture',\n",
              " 'relevant',\n",
              " 'aspects',\n",
              " 'data',\n",
              " 'contrast',\n",
              " 'sparse',\n",
              " 'approaches',\n",
              " 'rely',\n",
              " 'assumption',\n",
              " 'some',\n",
              " 'kernels',\n",
              " 'irrelevant',\n",
              " 'solving',\n",
              " 'problem',\n",
              " 'enforcing',\n",
              " 'sparse',\n",
              " 'mixtures',\n",
              " 'situations',\n",
              " 'may',\n",
              " 'lead',\n",
              " 'degenerate',\n",
              " 'models',\n",
              " 'remedy',\n",
              " 'propose',\n",
              " 'situations',\n",
              " 'non',\n",
              " 'sparse',\n",
              " 'mixtures',\n",
              " 'instead',\n",
              " 'after',\n",
              " 'paper',\n",
              " 'learned',\n",
              " 'about',\n",
              " 'related',\n",
              " 'approach',\n",
              " 'sum',\n",
              " 'regularizer',\n",
              " 'although',\n",
              " 'non',\n",
              " 'sparse',\n",
              " 'solutions',\n",
              " 'easy',\n",
              " 'interpret',\n",
              " 'they',\n",
              " 'account',\n",
              " 'even',\n",
              " 'small',\n",
              " 'contributions',\n",
              " 'available',\n",
              " 'kernels',\n",
              " 'live',\n",
              " 'practical',\n",
              " 'applications',\n",
              " 'paper',\n",
              " 'show',\n",
              " 'equivalence',\n",
              " 'most',\n",
              " 'common',\n",
              " 'approaches',\n",
              " 'theorem',\n",
              " 'allows',\n",
              " 'generalized',\n",
              " 'view',\n",
              " 'recent',\n",
              " 'multiple',\n",
              " 'kernel',\n",
              " 'learning',\n",
              " 'research',\n",
              " 'based',\n",
              " 'view',\n",
              " 'extend',\n",
              " 'framework',\n",
              " 'arbitrary',\n",
              " 'approach',\n",
              " 'either',\n",
              " 'motivated',\n",
              " 'additionally',\n",
              " 'regularizing',\n",
              " 'mixing',\n",
              " 'coefficients',\n",
              " 'equivalently',\n",
              " 'incorporating',\n",
              " 'constraint',\n",
              " 'propose',\n",
              " 'alternative',\n",
              " 'optimization',\n",
              " 'strategies',\n",
              " 'based',\n",
              " 'Newton',\n",
              " 'descent',\n",
              " 'respectively',\n",
              " 'empirically',\n",
              " 'demonstrate',\n",
              " 'efficiency',\n",
              " 'accuracy',\n",
              " 'none',\n",
              " 'sparse',\n",
              " 'large__scale',\n",
              " 'experiments',\n",
              " 'gene',\n",
              " 'start',\n",
              " 'detection',\n",
              " 'show',\n",
              " 'significant',\n",
              " 'improvement',\n",
              " 'predictive',\n",
              " 'accuracy',\n",
              " 'compared',\n",
              " 'rest',\n",
              " 'paper',\n",
              " 'structured',\n",
              " 'follows',\n",
              " 'present',\n",
              " 'main',\n",
              " 'contributions',\n",
              " 'Section',\n",
              " 'theoretical__analysis',\n",
              " 'existing',\n",
              " 'approaches',\n",
              " 'generalization',\n",
              " 'highly',\n",
              " 'efficient',\n",
              " 'optimization',\n",
              " 'strategies',\n",
              " 'relations',\n",
              " 'report',\n",
              " 'empirical',\n",
              " 'Section',\n",
              " 'Section',\n",
              " 'concludes',\n",
              " 'Generalized',\n",
              " 'Multiple',\n",
              " 'Kernel',\n",
              " 'Learning',\n",
              " 'Preliminaries',\n",
              " 'standard',\n",
              " 'supervised__learning',\n",
              " 'setup',\n",
              " 'labeled',\n",
              " 'sample',\n",
              " 'xi__yi',\n",
              " 'lie',\n",
              " 'some',\n",
              " 'input',\n",
              " 'space',\n",
              " 'goal',\n",
              " 'find',\n",
              " 'hypothesis',\n",
              " 'generalizes',\n",
              " 'well',\n",
              " 'new',\n",
              " 'unseen',\n",
              " 'data',\n",
              " 'applying',\n",
              " 'regularized',\n",
              " 'risk',\n",
              " 'minimization',\n",
              " 'returns',\n",
              " 'minimizer',\n",
              " 'xi__yi',\n",
              " 'empirical__risk',\n",
              " 'hypothesis',\n",
              " 'loss',\n",
              " 'regularizer',\n",
              " 'trade',\n",
              " 'off',\n",
              " 'parameter',\n",
              " 'paper',\n",
              " 'focus',\n",
              " 'linear__models',\n",
              " 'form',\n",
              " 'kwk',\n",
              " 'together',\n",
              " 'possibly',\n",
              " 'non',\n",
              " 'linear',\n",
              " 'mapping',\n",
              " 'Hilbert',\n",
              " 'space',\n",
              " 'will',\n",
              " 'later',\n",
              " 'make',\n",
              " 'kernel',\n",
              " 'functions',\n",
              " 'x0__x0',\n",
              " 'compute',\n",
              " 'inner',\n",
              " 'products',\n",
              " 'Learning',\n",
              " 'Multiple',\n",
              " 'When',\n",
              " 'learning',\n",
              " 'multiple',\n",
              " 'kernels',\n",
              " 'different',\n",
              " 'feature',\n",
              " 'map',\n",
              " 'giving',\n",
              " 'rise',\n",
              " 'reproducing',\n",
              " 'kernel',\n",
              " 'approaches',\n",
              " 'multiple',\n",
              " 'kernel',\n",
              " 'learning',\n",
              " 'consider',\n",
              " 'linear',\n",
              " 'kernel',\n",
              " 'mixtures',\n",
              " 'compared',\n",
              " 'primal',\n",
              " 'model',\n",
              " 'learning',\n",
              " 'multiple',\n",
              " 'kernels',\n",
              " 'extended',\n",
              " 'weight',\n",
              " 'vector',\n",
              " 'feature',\n",
              " 'map',\n",
              " 'block',\n",
              " 'structure',\n",
              " 'respectively',\n",
              " 'idea',\n",
              " 'learning',\n",
              " 'multiple',\n",
              " 'kernels',\n",
              " 'minimize',\n",
              " 'loss',\n",
              " 'training__data',\n",
              " 'optimal',\n",
              " 'kernel',\n",
              " 'mixture',\n",
              " 'addition',\n",
              " 'regularizing',\n",
              " 'avoid',\n",
              " 'overfitting',\n",
              " 'hence',\n",
              " 'terms',\n",
              " 'regularized',\n",
              " 'risk',\n",
              " 'minimization',\n",
              " 'optimization__problem',\n",
              " 'becomes',\n",
              " 'inf',\n",
              " 'k22',\n",
              " 'xi__yi',\n",
              " 'previous',\n",
              " 'approaches',\n",
              " 'multiple',\n",
              " 'kernel',\n",
              " 'learning',\n",
              " 'employ',\n",
              " 'regularizers',\n",
              " 'form',\n",
              " 'promote',\n",
              " 'sparse',\n",
              " 'kernel',\n",
              " 'mixtures',\n",
              " 'contrast',\n",
              " 'propose',\n",
              " 'smooth',\n",
              " 'convex',\n",
              " 'regularizers',\n",
              " 'form',\n",
              " 'allowing',\n",
              " 'non',\n",
              " 'sparse',\n",
              " 'solutions',\n",
              " 'non',\n",
              " 'convexity',\n",
              " 'resulting',\n",
              " 'optimization__problem',\n",
              " 'inherent',\n",
              " 'resolved',\n",
              " 'substituting',\n",
              " 'Furthermore',\n",
              " 'regularization',\n",
              " 'parameter',\n",
              " 'sample__size',\n",
              " 'introducing',\n",
              " 'adjusting',\n",
              " 'favorable',\n",
              " 'scaling',\n",
              " 'properties',\n",
              " 'practice',\n",
              " 'obtain',\n",
              " 'following',\n",
              " 'convex__optimization',\n",
              " 'problem',\n",
              " 'been',\n",
              " 'considered',\n",
              " 'hinge',\n",
              " 'loss',\n",
              " 'k22',\n",
              " 'inf',\n",
              " 'xi__yi',\n",
              " 'convention',\n",
              " 'otherwise',\n",
              " 'alternative',\n",
              " 'approach',\n",
              " 'been',\n",
              " 'studied',\n",
              " 'again',\n",
              " 'hinge',\n",
              " 'loss',\n",
              " 'they',\n",
              " 'upper__bound',\n",
              " 'value',\n",
              " 'regularizer',\n",
              " 'incorporate',\n",
              " 'latter',\n",
              " 'additional',\n",
              " 'constraint',\n",
              " 'into',\n",
              " 'optimization__problem',\n",
              " 'they',\n",
              " 'arrive',\n",
              " 'inf',\n",
              " 'xi__yi',\n",
              " 'Our',\n",
              " 'contribution',\n",
              " 'shows',\n",
              " 'both',\n",
              " 'regularization',\n",
              " 'regularization',\n",
              " 'equivalent',\n",
              " 'there',\n",
              " 'exists',\n",
              " 'optimal',\n",
              " 'Theorem',\n",
              " 'Let',\n",
              " 'pair',\n",
              " 'optimal__solution',\n",
              " 'solution',\n",
              " 'vice',\n",
              " 'versa',\n",
              " 'some',\n",
              " 'multiplicative',\n",
              " 'constant',\n",
              " 'proof',\n",
              " 'proof',\n",
              " 'shown',\n",
              " 'supplementary__material',\n",
              " 'lack',\n",
              " 'space',\n",
              " 'sketch',\n",
              " 'proof',\n",
              " 'incorporate',\n",
              " 'regularizer',\n",
              " 'into',\n",
              " 'constraints',\n",
              " 'show',\n",
              " 'resulting',\n",
              " 'upper__bound',\n",
              " 'tight',\n",
              " 'variable',\n",
              " 'completes',\n",
              " 'proof',\n",
              " 'showed',\n",
              " 'optimization',\n",
              " 'problems',\n",
              " 'their',\n",
              " 'own',\n",
              " 'formulation',\n",
              " 'equivalent',\n",
              " 'main',\n",
              " 'implication',\n",
              " 'Theorem',\n",
              " 'result',\n",
              " 'follows',\n",
              " 'optimization__problem',\n",
              " 'ones',\n",
              " 'equivalent',\n",
              " 'addition',\n",
              " 'result',\n",
              " 'shows',\n",
              " 'coupling',\n",
              " 'between',\n",
              " 'trade',\n",
              " 'off',\n",
              " 'parameter',\n",
              " 'regularization',\n",
              " 'parameter',\n",
              " 'changes',\n",
              " 'vice',\n",
              " 'versa',\n",
              " 'moreover',\n",
              " 'Theorem',\n",
              " 'implies',\n",
              " 'optimizing',\n",
              " 'implicitly',\n",
              " 'regularization',\n",
              " 'path',\n",
              " 'parameter',\n",
              " 'remainder',\n",
              " 'will',\n",
              " 'therefore',\n",
              " 'focus',\n",
              " 'formulation',\n",
              " 'single',\n",
              " 'parameter',\n",
              " 'terms',\n",
              " 'model',\n",
              " 'selection',\n",
              " 'furthermore',\n",
              " 'will',\n",
              " 'focus',\n",
              " 'binary',\n",
              " 'classification',\n",
              " 'problems',\n",
              " 'equipped',\n",
              " 'hinge',\n",
              " 'loss',\n",
              " 'max',\n",
              " 'however',\n",
              " 'note',\n",
              " 'easily',\n",
              " 'regression',\n",
              " 'multi',\n",
              " 'class',\n",
              " 'settings',\n",
              " 'appropriate',\n",
              " 'convex',\n",
              " 'loss__functions',\n",
              " 'joint',\n",
              " 'kernel',\n",
              " 'extensions',\n",
              " 'Non',\n",
              " 'Sparse',\n",
              " 'Multiple',\n",
              " 'Kernel',\n",
              " 'Learning',\n",
              " 'now',\n",
              " 'extend',\n",
              " 'existing',\n",
              " 'framework',\n",
              " 'allow',\n",
              " 'non',\n",
              " 'sparse',\n",
              " 'kernel',\n",
              " 'mixtures',\n",
              " 'see',\n",
              " 'let',\n",
              " 'begin',\n",
              " 'rewriting',\n",
              " 'hinge',\n",
              " 'loss',\n",
              " 'into',\n",
              " 'slack',\n",
              " 'variables',\n",
              " 'follows',\n",
              " 'min',\n",
              " 'Applying',\n",
              " 'theorem',\n",
              " 'incorporates',\n",
              " 'constraints',\n",
              " 'into',\n",
              " 'objective',\n",
              " 'introducing',\n",
              " 'nonnegative',\n",
              " 'Lagrangian',\n",
              " 'multipliers',\n",
              " 'including',\n",
              " 'pre',\n",
              " 'factor',\n",
              " 'term',\n",
              " 'optimality',\n",
              " 'conditions',\n",
              " 'removes',\n",
              " 'dependency',\n",
              " 'Lagrangian',\n",
              " 'primal',\n",
              " 'variables',\n",
              " 'after',\n",
              " 'some',\n",
              " 'additional',\n",
              " 'algebra',\n",
              " 'terms',\n",
              " 'associated',\n",
              " 'Lagrangian',\n",
              " 'written',\n",
              " 'diag',\n",
              " 'diag',\n",
              " 'now',\n",
              " 'maximized',\n",
              " 'dual',\n",
              " 'variables',\n",
              " 'subject',\n",
              " 'let',\n",
              " 'ignore',\n",
              " 'moment',\n",
              " 'non',\n",
              " 'negativity',\n",
              " 'solve',\n",
              " 'unbounded',\n",
              " 'setting',\n",
              " 'partial',\n",
              " 'derivative',\n",
              " 'zero',\n",
              " 'yields',\n",
              " 'Interestingly',\n",
              " 'optimality',\n",
              " 'always',\n",
              " 'because',\n",
              " 'quadratic',\n",
              " 'term',\n",
              " 'non',\n",
              " 'negative',\n",
              " 'plugging',\n",
              " 'optimal',\n",
              " 'into',\n",
              " 'arrive',\n",
              " 'following',\n",
              " 'optimization__problem',\n",
              " 'solely',\n",
              " 'depends',\n",
              " 'max',\n",
              " 'limit',\n",
              " 'above',\n",
              " 'problem',\n",
              " 'reduces',\n",
              " 'SVM',\n",
              " 'dual',\n",
              " 'while',\n",
              " 'gives',\n",
              " 'rise',\n",
              " 'variant',\n",
              " 'however',\n",
              " 'optimizing',\n",
              " 'dual',\n",
              " 'efficiently',\n",
              " 'difficult',\n",
              " 'will',\n",
              " 'cause',\n",
              " 'numerical',\n",
              " 'problems',\n",
              " 'limits',\n",
              " 'Two',\n",
              " 'Order',\n",
              " 'Optimization',\n",
              " 'Many',\n",
              " 'recent',\n",
              " 'solvers',\n",
              " 'based',\n",
              " 'linear',\n",
              " 'programs',\n",
              " 'around',\n",
              " 'SVMs',\n",
              " 'optimization',\n",
              " 'work',\n",
              " 'most',\n",
              " 'closely__related',\n",
              " 'approach',\n",
              " 'method',\n",
              " 'both',\n",
              " 'methods',\n",
              " 'aim',\n",
              " 'efficient',\n",
              " 'large__scale',\n",
              " 'algorithms',\n",
              " 'alternative',\n",
              " 'approaches',\n",
              " 'proposed',\n",
              " 'proposed',\n",
              " 'paper',\n",
              " 'largely',\n",
              " 'inspired',\n",
              " 'methods',\n",
              " 'extend',\n",
              " 'them',\n",
              " 'aspects',\n",
              " 'arbitrary',\n",
              " 'norms',\n",
              " 'tight',\n",
              " 'coupling',\n",
              " 'minor',\n",
              " 'iterations',\n",
              " 'SVM',\n",
              " 'solver',\n",
              " 'respectively',\n",
              " 'strategy',\n",
              " 'maximizing',\n",
              " 'Lagrangian',\n",
              " 'minor',\n",
              " 'precision',\n",
              " 'Newton',\n",
              " 'descent',\n",
              " 'second',\n",
              " 'strategy',\n",
              " 'semi',\n",
              " 'infinite',\n",
              " 'convex',\n",
              " 'program',\n",
              " 'solve',\n",
              " 'column',\n",
              " 'generation',\n",
              " 'sequential',\n",
              " 'quadratically',\n",
              " 'constrained',\n",
              " 'linear',\n",
              " 'programming',\n",
              " 'both',\n",
              " 'cases',\n",
              " 'maximization',\n",
              " 'step',\n",
              " 'performed',\n",
              " 'optimization',\n",
              " 'minor',\n",
              " 'iterations',\n",
              " 'Newton',\n",
              " 'approach',\n",
              " 'applied',\n",
              " 'without',\n",
              " 'common',\n",
              " 'purpose',\n",
              " 'solver',\n",
              " 'however',\n",
              " 'convergence',\n",
              " 'guaranteed',\n",
              " 'Newton',\n",
              " 'Descent',\n",
              " 'For',\n",
              " 'Newton',\n",
              " 'descent',\n",
              " 'mixing',\n",
              " 'coefficients',\n",
              " 'compute',\n",
              " 'partial',\n",
              " 'derivatives',\n",
              " 'original',\n",
              " 'Lagrangian',\n",
              " 'fortunately',\n",
              " 'Hessian',\n",
              " 'diagonal',\n",
              " 'diag',\n",
              " 'element',\n",
              " 'corresponding',\n",
              " 'step',\n",
              " 'defined',\n",
              " 'thus',\n",
              " 'computed',\n",
              " 'defined',\n",
              " 'however',\n",
              " 'Newton',\n",
              " 'step',\n",
              " 'might',\n",
              " 'lead',\n",
              " 'non',\n",
              " 'positive',\n",
              " 'avoid',\n",
              " 'situation',\n",
              " 'take',\n",
              " 'Newton',\n",
              " 'steps',\n",
              " 'space',\n",
              " 'log',\n",
              " 'adjusting',\n",
              " 'derivatives',\n",
              " 'according',\n",
              " 'chain',\n",
              " 'rule',\n",
              " 'obtain',\n",
              " 'log__log',\n",
              " 'corresponds',\n",
              " 'multiplicative',\n",
              " 'update',\n",
              " 'exp',\n",
              " 'Furthermore',\n",
              " 'additionally',\n",
              " 'Newton',\n",
              " 'step',\n",
              " 'line',\n",
              " 'search',\n",
              " 'order',\n",
              " 'obtain',\n",
              " 'alternative',\n",
              " 'optimization',\n",
              " 'strategy',\n",
              " 'fix',\n",
              " 'build',\n",
              " 'partial',\n",
              " 'Lagrangian',\n",
              " 'primal',\n",
              " 'variables',\n",
              " 'derivation',\n",
              " 'analogous',\n",
              " 'omit',\n",
              " 'details',\n",
              " 'lack',\n",
              " 'space',\n",
              " 'resulting',\n",
              " 'dual',\n",
              " 'problem',\n",
              " 'min__max',\n",
              " 'problem',\n",
              " 'form',\n",
              " 'min__max',\n",
              " 'above',\n",
              " 'optimization__problem',\n",
              " 'saddle',\n",
              " 'point',\n",
              " 'problem',\n",
              " 'solved',\n",
              " 'alternating',\n",
              " 'optimization',\n",
              " 'step',\n",
              " 'while',\n",
              " 'former',\n",
              " 'simply',\n",
              " 'carried',\n",
              " 'out',\n",
              " 'support',\n",
              " 'vector',\n",
              " 'machine',\n",
              " 'fixed',\n",
              " 'mixture',\n",
              " 'latter',\n",
              " 'been',\n",
              " 'optimized',\n",
              " 'reduced',\n",
              " 'gradients',\n",
              " 'take',\n",
              " 'different',\n",
              " 'approach',\n",
              " 'translate',\n",
              " 'min__max',\n",
              " 'problem',\n",
              " 'into',\n",
              " 'equivalent',\n",
              " 'semi',\n",
              " 'infinite',\n",
              " 'program',\n",
              " 'follows',\n",
              " 'denote',\n",
              " 'value',\n",
              " 'target',\n",
              " 'function',\n",
              " 'suppose',\n",
              " 'optimal',\n",
              " 'according',\n",
              " 'max',\n",
              " 'min',\n",
              " 'inequality',\n",
              " 'hence',\n",
              " 'equivalently',\n",
              " 'minimize',\n",
              " 'upper__bound',\n",
              " 'optimal',\n",
              " 'value',\n",
              " 'arrive',\n",
              " 'min',\n",
              " 'well',\n",
              " 'optimize',\n",
              " 'above',\n",
              " 'plane',\n",
              " 'algorithms',\n",
              " 'solution',\n",
              " 'quadratic',\n",
              " 'program',\n",
              " 'here',\n",
              " 'regular',\n",
              " 'SVM',\n",
              " 'generates',\n",
              " 'most',\n",
              " 'strongly',\n",
              " 'violated',\n",
              " 'constraint',\n",
              " 'actual',\n",
              " 'mixture',\n",
              " 'optimal',\n",
              " 'identified',\n",
              " 'solving',\n",
              " 'linear',\n",
              " 'program',\n",
              " 'respect',\n",
              " 'active',\n",
              " 'constraints',\n",
              " 'optimal',\n",
              " 'mixture',\n",
              " 'computing',\n",
              " 'new',\n",
              " 'constraint',\n",
              " 'unfortunately',\n",
              " 'non',\n",
              " 'linearity',\n",
              " 'introduced',\n",
              " 'requiring',\n",
              " 'constraint',\n",
              " 'unlikely',\n",
              " 'found',\n",
              " 'standard',\n",
              " 'optimization',\n",
              " 'often',\n",
              " 'handle',\n",
              " 'linear',\n",
              " 'quadratic',\n",
              " 'constraints',\n",
              " 'remedy',\n",
              " 'propose',\n",
              " 'approximate',\n",
              " 'sequential',\n",
              " 'second',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is the process reducing a word to it stek value. Here we try to achieve that using the PorterStemmer class."
      ],
      "metadata": {
        "id": "UKbLJDkPLNmW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "JO1s3TpEUzWq"
      },
      "outputs": [],
      "source": [
        "stemmed_dict = {}\n",
        "# instantiates a class object\n",
        "stemmer = PorterStemmer()\n",
        "for keys in rare_token_dict.keys():\n",
        "  st_token_list = []\n",
        "  for value in rare_token_dict[keys]:\n",
        "    if \"__\" in value: \n",
        "      # bigrans are not stemmed but used as is\n",
        "      st_token_list.append(value)\n",
        "    else:\n",
        "      # stems any other tokens than bigrams\n",
        "      st_token_list.append(stemmer.stem(value))\n",
        "  stemmed_dict[keys] = st_token_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsfda66jVBFj",
        "outputId": "54980233-2f88-4f87-f3b1-952fe47c69da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1870"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "len(stemmed_dict[\"PP3675\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmaGJYIJlDEl"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 5. Writing Output Files <a class=\"anchor\" name=\"write\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dj2JGI7JUKwU"
      },
      "outputs": [],
      "source": [
        "stemmed_list = list(chain.from_iterable(stemmed_dict.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjMBqRetlDEl"
      },
      "source": [
        "files need to be generated:\n",
        "* Vocabulary list\n",
        "* Sparse matrix (count_vectors)\n",
        "* Statistics matrix\n",
        "\n",
        "This is performed in the following sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc6tQ4ljlDEm"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 5.1. Vocabulary List <a class=\"anchor\" name=\"write-vocab\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDlbpGYilDEm"
      },
      "source": [
        "List of vocabulary should also be written to a file, sorted alphabetically, with their reference codes in front of them. This file also refers to the sparse matrix in the next file. For this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Y6OUXHlxlDEm"
      },
      "outputs": [],
      "source": [
        "# list of tokens sorted alphabetically\n",
        "vocab_list = sorted(set(stemmed_list))\n",
        "vocab_dict = {}\n",
        "\n",
        "for i in range(len(vocab_list)):\n",
        "  # getting the index position of the token\n",
        "  vocab_dict[vocab_list[i]] = i\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Asessment1/OutputFiles/Task2\")\n",
        "# creates a file containing the token and its index position\n",
        "with open('task2_31940757_vocab.txt','w') as vocab:\n",
        "  for key, value in vocab_dict.items():\n",
        "    vocab.write(key+\":\"+str(value)+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kraZc9ZuQruG",
        "outputId": "d9ae4053-9e34-4d44-ec01-c7b00f1c0eba"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['EM__algorithm',\n",
              " 'Experiments__We',\n",
              " 'Figure__The',\n",
              " 'Gaussian__distribution',\n",
              " 'Gaussian__noise',\n",
              " 'Gibbs__sampling',\n",
              " 'KL__divergence',\n",
              " 'Markov__chain',\n",
              " 'Monte__Carlo',\n",
              " 'Neural__Information',\n",
              " 'Processing__Systems',\n",
              " 'ROC__curve',\n",
              " 'Related__Work',\n",
              " 'abil',\n",
              " 'abl',\n",
              " 'about',\n",
              " 'abov',\n",
              " 'absenc',\n",
              " 'absolut',\n",
              " 'abstract',\n",
              " 'abus',\n",
              " 'acceler',\n",
              " 'accept',\n",
              " 'access',\n",
              " 'accommod',\n",
              " 'accord',\n",
              " 'accordingli',\n",
              " 'account',\n",
              " 'accumul',\n",
              " 'accur',\n",
              " 'accuraci',\n",
              " 'achiev',\n",
              " 'acknowledg',\n",
              " 'acquir',\n",
              " 'across',\n",
              " 'act',\n",
              " 'action',\n",
              " 'activ',\n",
              " 'active__learning',\n",
              " 'actual',\n",
              " 'ad',\n",
              " 'adam',\n",
              " 'adapt',\n",
              " 'add',\n",
              " 'addit',\n",
              " 'address',\n",
              " 'adequ',\n",
              " 'adjac',\n",
              " 'adjoint',\n",
              " 'adjust',\n",
              " 'admit',\n",
              " 'adopt',\n",
              " 'advanc',\n",
              " 'advantag',\n",
              " 'adversari',\n",
              " 'affect',\n",
              " 'affin',\n",
              " 'aforement',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'age',\n",
              " 'agent',\n",
              " 'agent?',\n",
              " 'aggreg',\n",
              " 'agnost',\n",
              " 'agre',\n",
              " 'agreement',\n",
              " 'ahead',\n",
              " 'aid',\n",
              " 'aij',\n",
              " 'aim',\n",
              " 'akin',\n",
              " 'albeit',\n",
              " 'alg',\n",
              " 'algebra',\n",
              " 'algo',\n",
              " 'algorithm',\n",
              " 'algorithm?',\n",
              " 'align',\n",
              " 'all',\n",
              " 'allevi',\n",
              " 'alloc',\n",
              " 'allow',\n",
              " 'almost',\n",
              " 'alon',\n",
              " 'along',\n",
              " 'alreadi',\n",
              " 'also',\n",
              " 'altern',\n",
              " 'although',\n",
              " 'alway',\n",
              " 'amazon',\n",
              " 'ambigu',\n",
              " 'amen',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'amount',\n",
              " 'analog',\n",
              " 'analys',\n",
              " 'analysi',\n",
              " 'analyt',\n",
              " 'analyz',\n",
              " 'and',\n",
              " 'angl',\n",
              " 'angular',\n",
              " 'ani',\n",
              " 'anim',\n",
              " 'annot',\n",
              " 'anonym',\n",
              " 'anoth',\n",
              " 'anr',\n",
              " 'answer',\n",
              " 'anyth',\n",
              " 'apart',\n",
              " 'appar',\n",
              " 'appeal',\n",
              " 'appear',\n",
              " 'appendix',\n",
              " 'appli',\n",
              " 'applic',\n",
              " 'approach',\n",
              " 'appropri',\n",
              " 'approxim',\n",
              " 'arbitrari',\n",
              " 'arbitrarili',\n",
              " 'architectur',\n",
              " 'archiv',\n",
              " 'area',\n",
              " 'arg',\n",
              " 'arg__max',\n",
              " 'arg__min',\n",
              " 'argmax',\n",
              " 'argmin',\n",
              " 'argu',\n",
              " 'arguabl',\n",
              " 'argument',\n",
              " 'aris',\n",
              " 'arm',\n",
              " 'aro',\n",
              " 'around',\n",
              " 'arrang',\n",
              " 'array',\n",
              " 'arriv',\n",
              " 'arrow',\n",
              " 'art',\n",
              " 'articl',\n",
              " 'artifici',\n",
              " 'ascent',\n",
              " 'asid',\n",
              " 'ask',\n",
              " 'aspect',\n",
              " 'assess',\n",
              " 'assign',\n",
              " 'associ',\n",
              " 'assum',\n",
              " 'assumpt',\n",
              " 'asymmetr',\n",
              " 'asymptot',\n",
              " 'asynchron',\n",
              " 'attain',\n",
              " 'attempt',\n",
              " 'attent',\n",
              " 'attract',\n",
              " 'attribut',\n",
              " 'auc',\n",
              " 'audio',\n",
              " 'augment',\n",
              " 'author',\n",
              " 'auto',\n",
              " 'automat',\n",
              " 'auxiliari',\n",
              " 'avail',\n",
              " 'averag',\n",
              " 'avoid',\n",
              " 'awar',\n",
              " 'award',\n",
              " 'away',\n",
              " 'axe',\n",
              " 'axi',\n",
              " 'back',\n",
              " 'background',\n",
              " 'backpropag',\n",
              " 'backward',\n",
              " 'bad',\n",
              " 'badli',\n",
              " 'bag',\n",
              " 'balanc',\n",
              " 'ball',\n",
              " 'band',\n",
              " 'bandit',\n",
              " 'bandwidth',\n",
              " 'bank',\n",
              " 'bar',\n",
              " 'barcelona',\n",
              " 'barrier',\n",
              " 'base',\n",
              " 'based__methods',\n",
              " 'baselin',\n",
              " 'basi',\n",
              " 'basic',\n",
              " 'batch',\n",
              " 'batch__size',\n",
              " 'bay',\n",
              " 'bayesian',\n",
              " 'be',\n",
              " 'beach',\n",
              " 'becaus',\n",
              " 'becom',\n",
              " 'been',\n",
              " 'befor',\n",
              " 'begin',\n",
              " 'behav',\n",
              " 'behavior',\n",
              " 'behaviour',\n",
              " 'behind',\n",
              " 'belief',\n",
              " 'believ',\n",
              " 'belong',\n",
              " 'below',\n",
              " 'ben',\n",
              " 'benchmark',\n",
              " 'benefici',\n",
              " 'benefit',\n",
              " 'berkeley',\n",
              " 'bernoulli',\n",
              " 'besid',\n",
              " 'best',\n",
              " 'beta',\n",
              " 'better',\n",
              " 'between',\n",
              " 'beyond',\n",
              " 'bfg',\n",
              " 'bia',\n",
              " 'bianchi',\n",
              " 'bias',\n",
              " 'bidirect',\n",
              " 'big',\n",
              " 'bigger',\n",
              " 'billion',\n",
              " 'bin',\n",
              " 'binari',\n",
              " 'binomi',\n",
              " 'biolog',\n",
              " 'bipartit',\n",
              " 'bit',\n",
              " 'black',\n",
              " 'blind',\n",
              " 'block',\n",
              " 'blue',\n",
              " 'bodi',\n",
              " 'boil',\n",
              " 'bold',\n",
              " 'boldfac',\n",
              " 'boltzmann',\n",
              " 'boost',\n",
              " 'both',\n",
              " 'bottleneck',\n",
              " 'bottom',\n",
              " 'bound',\n",
              " 'boundari',\n",
              " 'box',\n",
              " 'brain',\n",
              " 'break',\n",
              " 'breakdown',\n",
              " 'breviti',\n",
              " 'brief',\n",
              " 'briefli',\n",
              " 'bring',\n",
              " 'broad',\n",
              " 'broader',\n",
              " 'broadli',\n",
              " 'broken',\n",
              " 'brought',\n",
              " 'budget',\n",
              " 'build',\n",
              " 'built',\n",
              " 'but',\n",
              " 'c1__c2',\n",
              " 'calcul',\n",
              " 'calibr',\n",
              " 'call',\n",
              " 'came',\n",
              " 'camera',\n",
              " 'can',\n",
              " 'cancer',\n",
              " 'candid',\n",
              " 'cannot',\n",
              " 'canon',\n",
              " 'capabl',\n",
              " 'capac',\n",
              " 'capit',\n",
              " 'captur',\n",
              " 'car',\n",
              " 'cardin',\n",
              " 'care',\n",
              " 'career',\n",
              " 'carri',\n",
              " 'cascad',\n",
              " 'case',\n",
              " 'cast',\n",
              " 'categor',\n",
              " 'categori',\n",
              " 'cation',\n",
              " 'cauchi',\n",
              " 'caus',\n",
              " 'causal',\n",
              " 'ccf',\n",
              " 'cell',\n",
              " 'center',\n",
              " 'centr',\n",
              " 'central',\n",
              " 'certain',\n",
              " 'certainli',\n",
              " 'certainti',\n",
              " 'chain',\n",
              " 'challeng',\n",
              " 'chanc',\n",
              " 'chang',\n",
              " 'channel',\n",
              " 'chapter',\n",
              " 'charact',\n",
              " 'character',\n",
              " 'characterist',\n",
              " 'check',\n",
              " 'chen',\n",
              " 'child',\n",
              " 'children',\n",
              " 'choic',\n",
              " 'choleski',\n",
              " 'choos',\n",
              " 'chose',\n",
              " 'chosen',\n",
              " 'cifar',\n",
              " 'circl',\n",
              " 'circuit',\n",
              " 'circular',\n",
              " 'circumv',\n",
              " 'claim',\n",
              " 'clarifi',\n",
              " 'clariti',\n",
              " 'class',\n",
              " 'classic',\n",
              " 'classif',\n",
              " 'classifi',\n",
              " 'clean',\n",
              " 'clear',\n",
              " 'clearli',\n",
              " 'climb',\n",
              " 'cliqu',\n",
              " 'close',\n",
              " 'closed__form',\n",
              " 'closely__related',\n",
              " 'closer',\n",
              " 'closest',\n",
              " 'cluster',\n",
              " 'clutter',\n",
              " 'cnn',\n",
              " 'co',\n",
              " 'coars',\n",
              " 'code',\n",
              " 'coeffici',\n",
              " 'cognit',\n",
              " 'coher',\n",
              " 'coincid',\n",
              " 'collabor',\n",
              " 'collaps',\n",
              " 'collect',\n",
              " 'color',\n",
              " 'column',\n",
              " 'com',\n",
              " 'combin',\n",
              " 'combinatori',\n",
              " 'come',\n",
              " 'comment',\n",
              " 'common',\n",
              " 'commonli',\n",
              " 'commun',\n",
              " 'compact',\n",
              " 'compactli',\n",
              " 'compar',\n",
              " 'comparison',\n",
              " 'compens',\n",
              " 'compet',\n",
              " 'competit',\n",
              " 'complement',\n",
              " 'complementari',\n",
              " 'complet',\n",
              " 'complex',\n",
              " 'complic',\n",
              " 'compon',\n",
              " 'compos',\n",
              " 'composit',\n",
              " 'compound',\n",
              " 'comprehens',\n",
              " 'compress',\n",
              " 'compris',\n",
              " 'comput',\n",
              " 'computational__complexity',\n",
              " 'con',\n",
              " 'concaten',\n",
              " 'concav',\n",
              " 'concentr',\n",
              " 'concept',\n",
              " 'conceptu',\n",
              " 'concern',\n",
              " 'conclud',\n",
              " 'conclus',\n",
              " 'concret',\n",
              " 'condit',\n",
              " 'condition__number',\n",
              " 'conditional__independence',\n",
              " 'conduct',\n",
              " 'cone',\n",
              " 'confer',\n",
              " 'confid',\n",
              " 'configur',\n",
              " 'confirm',\n",
              " 'conjectur',\n",
              " 'conjug',\n",
              " 'conjugaci',\n",
              " 'conjunct',\n",
              " 'connect',\n",
              " 'consecut',\n",
              " 'consensu',\n",
              " 'consequ',\n",
              " 'conserv',\n",
              " 'consid',\n",
              " 'consider',\n",
              " 'consist',\n",
              " 'const',\n",
              " 'constant',\n",
              " 'constitut',\n",
              " 'constrain',\n",
              " 'constraint',\n",
              " 'construct',\n",
              " 'consum',\n",
              " 'contain',\n",
              " 'content',\n",
              " 'context',\n",
              " 'contextu',\n",
              " 'continu',\n",
              " 'contract',\n",
              " 'contradict',\n",
              " 'contrari',\n",
              " 'contrast',\n",
              " 'contribut',\n",
              " 'control',\n",
              " 'conveni',\n",
              " 'convent',\n",
              " 'converg',\n",
              " 'convergence__rate',\n",
              " 'convers',\n",
              " 'convert',\n",
              " 'convex',\n",
              " 'convex__function',\n",
              " 'convex__optimization',\n",
              " 'convolut',\n",
              " 'coordin',\n",
              " 'coordinate__descent',\n",
              " 'copi',\n",
              " 'core',\n",
              " 'corollari',\n",
              " 'corpora',\n",
              " 'corpu',\n",
              " 'correct',\n",
              " 'correctli',\n",
              " 'correl',\n",
              " 'correspond',\n",
              " 'correspondingli',\n",
              " 'corrupt',\n",
              " 'cortex',\n",
              " 'cortic',\n",
              " 'cosin',\n",
              " 'cost',\n",
              " 'cost__function',\n",
              " 'costli',\n",
              " 'could',\n",
              " 'council',\n",
              " 'count',\n",
              " 'counter',\n",
              " 'counterpart',\n",
              " 'coupl',\n",
              " 'cours',\n",
              " 'cov',\n",
              " 'covari',\n",
              " 'covariance__matrix',\n",
              " 'cover',\n",
              " 'coverag',\n",
              " 'cpu',\n",
              " 'creat',\n",
              " 'crf',\n",
              " 'criteria',\n",
              " 'criterion',\n",
              " 'critic',\n",
              " 'cross',\n",
              " 'cross__validation',\n",
              " 'crucial',\n",
              " 'cube',\n",
              " 'cubic',\n",
              " 'cue',\n",
              " 'cumul',\n",
              " 'current',\n",
              " 'curv',\n",
              " 'curvatur',\n",
              " 'cut',\n",
              " 'cycl',\n",
              " 'dark',\n",
              " 'darpa',\n",
              " 'dash',\n",
              " 'data',\n",
              " 'data__matrix',\n",
              " 'data__point',\n",
              " 'data__points',\n",
              " 'data__set',\n",
              " 'data__sets',\n",
              " 'data__structure',\n",
              " 'databas',\n",
              " 'datapoint',\n",
              " 'dataset',\n",
              " 'date',\n",
              " 'david',\n",
              " 'day',\n",
              " 'deal',\n",
              " 'decad',\n",
              " 'decay',\n",
              " 'decid',\n",
              " 'decis',\n",
              " 'decision__boundary',\n",
              " 'decod',\n",
              " 'decompos',\n",
              " 'decomposit',\n",
              " 'decreas',\n",
              " 'dedic',\n",
              " 'deduc',\n",
              " 'deep',\n",
              " 'deep__learning',\n",
              " 'deeper',\n",
              " 'def',\n",
              " 'default',\n",
              " 'defer',\n",
              " 'defin',\n",
              " 'definit',\n",
              " 'degener',\n",
              " 'degrad',\n",
              " 'degre',\n",
              " 'delay',\n",
              " 'delta',\n",
              " 'demonstr',\n",
              " 'denois',\n",
              " 'denomin',\n",
              " 'denot',\n",
              " 'dens',\n",
              " 'densiti',\n",
              " 'depart',\n",
              " 'depend',\n",
              " 'depict',\n",
              " 'deploy',\n",
              " 'depth',\n",
              " 'deriv',\n",
              " 'descent',\n",
              " 'describ',\n",
              " 'descript',\n",
              " 'design',\n",
              " 'desir',\n",
              " 'despit',\n",
              " 'det',\n",
              " 'detail',\n",
              " 'detect',\n",
              " 'detector',\n",
              " 'determin',\n",
              " 'determinist',\n",
              " 'develop',\n",
              " 'deviat',\n",
              " 'devot',\n",
              " 'di',\n",
              " 'diag',\n",
              " 'diagnosi',\n",
              " 'diagon',\n",
              " 'diagonal__matrix',\n",
              " 'diagram',\n",
              " 'diamet',\n",
              " 'dictionari',\n",
              " 'did',\n",
              " 'differ',\n",
              " 'differenti',\n",
              " 'difficult',\n",
              " 'difficulti',\n",
              " 'diffus',\n",
              " 'digit',\n",
              " 'dim',\n",
              " 'dimens',\n",
              " 'dimension',\n",
              " 'diminish',\n",
              " 'direct',\n",
              " 'directli',\n",
              " 'dirichlet',\n",
              " 'disadvantag',\n",
              " 'discard',\n",
              " 'disconnect',\n",
              " 'discount',\n",
              " 'discov',\n",
              " 'discrep',\n",
              " 'discret',\n",
              " 'discrimin',\n",
              " 'discuss',\n",
              " 'diseas',\n",
              " 'disjoint',\n",
              " 'display',\n",
              " 'distanc',\n",
              " 'distant',\n",
              " 'distinct',\n",
              " 'distinguish',\n",
              " 'distribut',\n",
              " 'diverg',\n",
              " 'divers',\n",
              " 'divid',\n",
              " 'divis',\n",
              " 'do',\n",
              " 'document',\n",
              " 'doe',\n",
              " 'doesn?t',\n",
              " 'dom',\n",
              " 'domain',\n",
              " 'domin',\n",
              " 'don?t',\n",
              " 'done',\n",
              " 'dot',\n",
              " 'doubl',\n",
              " 'down',\n",
              " 'download',\n",
              " 'dramat',\n",
              " 'draw',\n",
              " 'drawback',\n",
              " 'drawn',\n",
              " 'drift',\n",
              " 'drive',\n",
              " 'driven',\n",
              " 'drop',\n",
              " 'dropout',\n",
              " 'dual',\n",
              " 'dualiti',\n",
              " 'due',\n",
              " 'durat',\n",
              " 'dure',\n",
              " 'dynam',\n",
              " 'each',\n",
              " 'earli',\n",
              " 'earlier',\n",
              " 'earliest',\n",
              " 'eas',\n",
              " 'easi',\n",
              " 'easier',\n",
              " 'easili',\n",
              " 'econom',\n",
              " 'edg',\n",
              " 'edu',\n",
              " 'educ',\n",
              " 'effect',\n",
              " 'effici',\n",
              " 'effort',\n",
              " 'eigendecomposit',\n",
              " 'eigenvalu',\n",
              " 'eigenvector',\n",
              " 'eight',\n",
              " 'either',\n",
              " 'elabor',\n",
              " 'eleg',\n",
              " 'element',\n",
              " 'elementari',\n",
              " 'elementwis',\n",
              " 'elimin',\n",
              " 'els',\n",
              " 'elsewher',\n",
              " 'emb',\n",
              " 'embed',\n",
              " 'emerg',\n",
              " 'emphas',\n",
              " 'emphasi',\n",
              " 'empir',\n",
              " 'empirical__risk',\n",
              " 'employ',\n",
              " 'empti',\n",
              " 'enabl',\n",
              " 'encod',\n",
              " 'encount',\n",
              " 'encourag',\n",
              " 'end',\n",
              " 'energi',\n",
              " 'enforc',\n",
              " 'engin',\n",
              " 'english',\n",
              " 'enjoy',\n",
              " 'enough',\n",
              " 'ensembl',\n",
              " 'ensur',\n",
              " 'entail',\n",
              " 'entir',\n",
              " 'entiti',\n",
              " 'entri',\n",
              " 'entropi',\n",
              " 'environ',\n",
              " 'epoch',\n",
              " 'eqn',\n",
              " 'equal',\n",
              " 'equat',\n",
              " 'equilibrium',\n",
              " 'equip',\n",
              " 'equival',\n",
              " 'erc',\n",
              " 'erron',\n",
              " 'error',\n",
              " 'especi',\n",
              " 'essenti',\n",
              " 'establish',\n",
              " 'estim',\n",
              " 'estimation__error',\n",
              " 'etc',\n",
              " 'euclidean',\n",
              " 'european',\n",
              " 'evalu',\n",
              " 'even',\n",
              " 'evenli',\n",
              " 'event',\n",
              " 'eventu',\n",
              " 'ever',\n",
              " 'everi',\n",
              " 'everywher',\n",
              " 'evid',\n",
              " 'evolut',\n",
              " 'evolv',\n",
              " 'exact',\n",
              " 'exactli',\n",
              " 'examin',\n",
              " 'exampl',\n",
              " 'exce',\n",
              " 'exceed',\n",
              " 'excel',\n",
              " 'except',\n",
              " 'excess',\n",
              " 'exclud',\n",
              " 'exclus',\n",
              " 'execut',\n",
              " 'exhibit',\n",
              " 'exist',\n",
              " 'exp',\n",
              " 'expand',\n",
              " 'expans',\n",
              " 'expect',\n",
              " 'expens',\n",
              " 'experi',\n",
              " 'experiment',\n",
              " 'experimental__results',\n",
              " 'expert',\n",
              " 'explain',\n",
              " 'explan',\n",
              " 'explicit',\n",
              " 'explicitli',\n",
              " 'exploit',\n",
              " 'explor',\n",
              " 'expon',\n",
              " 'exponenti',\n",
              " 'exposit',\n",
              " 'express',\n",
              " 'extend',\n",
              " 'extens',\n",
              " 'extent',\n",
              " 'extern',\n",
              " 'extra',\n",
              " 'extract',\n",
              " 'extrem',\n",
              " 'face',\n",
              " 'facilit',\n",
              " 'fact',\n",
              " 'factor',\n",
              " 'factori',\n",
              " 'fail',\n",
              " 'failur',\n",
              " 'fair',\n",
              " 'fairli',\n",
              " 'fall',\n",
              " 'fals',\n",
              " 'famili',\n",
              " 'familiar',\n",
              " 'far',\n",
              " 'farther',\n",
              " 'fashion',\n",
              " 'fast',\n",
              " 'faster',\n",
              " 'fastest',\n",
              " 'favor',\n",
              " 'feasibl',\n",
              " 'featur',\n",
              " 'feature__space',\n",
              " 'feature__vector',\n",
              " 'feature__vectors',\n",
              " 'fed',\n",
              " 'feed',\n",
              " 'feedback',\n",
              " 'feedforward',\n",
              " 'fellowship',\n",
              " 'few',\n",
              " 'fewer',\n",
              " 'field',\n",
              " 'fifth',\n",
              " 'fig',\n",
              " 'figur',\n",
              " 'figure__shows',\n",
              " 'fill',\n",
              " 'filter',\n",
              " 'final',\n",
              " 'find',\n",
              " 'fine',\n",
              " 'finer',\n",
              " 'finish',\n",
              " 'finit',\n",
              " 'finite__set',\n",
              " 'fire',\n",
              " 'first',\n",
              " 'firstli',\n",
              " 'fisher',\n",
              " 'fit',\n",
              " 'five',\n",
              " 'fix',\n",
              " 'flat',\n",
              " 'flavor',\n",
              " 'flexibl',\n",
              " 'flow',\n",
              " 'fluctuat',\n",
              " 'focu',\n",
              " 'focus',\n",
              " 'fold',\n",
              " 'follow',\n",
              " 'for',\n",
              " 'forc',\n",
              " 'form',\n",
              " 'formal',\n",
              " 'former',\n",
              " 'formul',\n",
              " 'formula',\n",
              " 'fortun',\n",
              " 'forward',\n",
              " 'found',\n",
              " 'foundat',\n",
              " 'four',\n",
              " 'fourier',\n",
              " 'fourth',\n",
              " 'fp7',\n",
              " 'fraction',\n",
              " 'frame',\n",
              " 'framework',\n",
              " 'free',\n",
              " 'freedom',\n",
              " 'french',\n",
              " 'frequenc',\n",
              " 'frequent',\n",
              " 'fresh',\n",
              " 'frobeniu',\n",
              " 'from',\n",
              " 'front',\n",
              " 'fruit',\n",
              " 'full',\n",
              " 'fulli',\n",
              " 'fully__connected',\n",
              " 'func',\n",
              " 'function',\n",
              " 'fund',\n",
              " 'fundament',\n",
              " 'further',\n",
              " 'furthermor',\n",
              " 'futur',\n",
              " 'future__work',\n",
              " 'gain',\n",
              " 'game',\n",
              " 'gamma',\n",
              " 'gap',\n",
              " 'gate',\n",
              " 'gather',\n",
              " 'gaussian',\n",
              " 'gave',\n",
              " 'gene',\n",
              " 'gener',\n",
              " 'generative__model',\n",
              " 'geometr',\n",
              " 'geometri',\n",
              " 'german',\n",
              " 'get',\n",
              " 'ghz',\n",
              " 'gibb',\n",
              " 'github',\n",
              " 'give',\n",
              " 'given',\n",
              " 'glanc',\n",
              " 'global',\n",
              " 'go',\n",
              " 'goal',\n",
              " 'goe',\n",
              " 'gold',\n",
              " 'good',\n",
              " 'googl',\n",
              " 'govern',\n",
              " 'gp',\n",
              " 'gpu',\n",
              " 'gradient',\n",
              " 'gradient__descent',\n",
              " 'gradual',\n",
              " 'graduat',\n",
              " 'grain',\n",
              " 'grant',\n",
              " 'graph',\n",
              " 'graphic',\n",
              " 'graphical__model',\n",
              " 'graphical__models',\n",
              " 'gray',\n",
              " 'great',\n",
              " 'greater',\n",
              " 'greatli',\n",
              " 'greedi',\n",
              " 'greedili',\n",
              " 'greedy__algorithm',\n",
              " 'green',\n",
              " 'grid',\n",
              " 'ground',\n",
              " 'ground__truth',\n",
              " 'group',\n",
              " 'grow',\n",
              " 'growth',\n",
              " 'guarante',\n",
              " 'guess',\n",
              " 'guid',\n",
              " 'had',\n",
              " 'hadamard',\n",
              " 'half',\n",
              " 'halv',\n",
              " 'hand',\n",
              " 'handl',\n",
              " 'handwritten',\n",
              " 'happen',\n",
              " 'hard',\n",
              " 'harder',\n",
              " 'hardwar',\n",
              " 'have',\n",
              " 'head',\n",
              " 'heavi',\n",
              " 'heavili',\n",
              " 'height',\n",
              " 'held',\n",
              " 'help',\n",
              " 'henc',\n",
              " 'henceforth',\n",
              " 'her',\n",
              " 'here',\n",
              " 'herein',\n",
              " 'hessian',\n",
              " 'heterogen',\n",
              " 'heurist',\n",
              " 'hi',\n",
              " 'hidden',\n",
              " 'hidden__layer',\n",
              " 'hidden__states',\n",
              " 'hidden__units',\n",
              " 'hide',\n",
              " 'hierarch',\n",
              " 'hierarchi',\n",
              " 'high',\n",
              " 'high__dimensional',\n",
              " 'high__probability',\n",
              " 'highdimension',\n",
              " 'higher',\n",
              " 'higher__order',\n",
              " 'highest',\n",
              " 'highli',\n",
              " 'highlight',\n",
              " 'hilbert',\n",
              " 'hindsight',\n",
              " 'hing',\n",
              " 'histogram',\n",
              " 'histori',\n",
              " 'hit',\n",
              " 'hmm',\n",
              " 'hoc',\n",
              " 'hold',\n",
              " 'homogen',\n",
              " 'hope',\n",
              " 'horizon',\n",
              " 'horizont',\n",
              " 'hot',\n",
              " 'hour',\n",
              " 'how',\n",
              " 'howev',\n",
              " 'html',\n",
              " 'http',\n",
              " 'huge',\n",
              " 'human',\n",
              " 'hundr',\n",
              " 'hurt',\n",
              " 'hybrid',\n",
              " 'hyper',\n",
              " 'hypercub',\n",
              " 'hyperparamet',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkGH81YFlDEn"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 5.2. Sparse Matrix <a class=\"anchor\" name=\"write-sparseMat\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtxqUAwmlDEn"
      },
      "source": [
        "For writing sparse matrix for a paper, we firstly calculate the frequency of words for that paper. We achieve this by converting the text to numerical data using CountVectorizer. This is used to convert the text data to a matrix containing count of the tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "__n1fdIqlDEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89732332-1457-4ddb-f08d-0360f27b21ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 ... 0 2 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 8 ... 0 1 0]\n",
            " ...\n",
            " [2 2 2 ... 0 9 0]\n",
            " [1 2 3 ... 0 2 0]\n",
            " [0 1 2 ... 0 1 0]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# creates a CountVectorizer object\n",
        "vectorizer = CountVectorizer(analyzer = \"word\") \n",
        "# creates the sparse matrix        \n",
        "sparse_matrix = vectorizer.fit_transform([' '.join(value) for value in stemmed_dict.values()])\n",
        "print (sparse_matrix.toarray())   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hsDrxijIqxm"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 5.3. Statistics Matrix <a class=\"anchor\" name=\"write-sparseMat\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a file for storing the vector count\n",
        "vector_file = open(\"./task2_31940757_count_vectors.txt\", 'w')\n",
        "# creates a dict for storing numerical tokens\n",
        "sparse_dict = {}\n",
        "# instantiates index \n",
        "idx = 0\n",
        "\n",
        "for token in vocab_list:\n",
        "  # replace the token value with its index value\n",
        "  sparse_dict[token] = idx\n",
        "  idx = idx + 1\n",
        "    \n",
        "# iterates through updated dict    \n",
        "for key, value in stemmed_dict.items():\n",
        "  # token id list\n",
        "  value_id = [sparse_dict[token] for token in value]\n",
        "  vec_str = \"\"\n",
        "  vector_file.write(\"{paper_id},\".format(paper_id = key))\n",
        "  for m, n in FreqDist(value_id).items():\n",
        "    vec_str += \"{token_index}:{word_count},\".format(token_index = m, word_count = n)\n",
        "  vector_file.write(vec_str[:-2])\n",
        "  vector_file.write('\\n')\n",
        "vector_file.close()"
      ],
      "metadata": {
        "id": "-lEPqew0VzTf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmzqbRWdIqxm"
      },
      "source": [
        "For writing overall statistics of papers, we aggregate statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Oi98egm_Iqxm"
      },
      "outputs": [],
      "source": [
        "auth_dict = {}\n",
        "title_dict = {}\n",
        "abstract_dict = {}\n",
        "for keys in file_dict.keys():\n",
        "  # matches for author data\n",
        "  auth_matches = re.search(r'(?:Authored by:)(.*)(?:Abstract\\n)',file_dict[keys], re.DOTALL)\n",
        "  if auth_matches:\n",
        "    # authors names in captured groups\n",
        "    authors = auth_matches.group(1)\n",
        "    authors = re.sub('-\\n','',authors)\n",
        "    authors = re.sub('\\n',',',authors)\n",
        "    auth_dict[keys] = authors\n",
        "  # matches for title data\n",
        "  title_matches = re.search(r'(.*)(?:Authored by:)',file_dict[keys], re.DOTALL)\n",
        "  if title_matches:\n",
        "    title = title_matches.group(1)\n",
        "    title = re.sub('-\\n','',title)\n",
        "    title = re.sub('\\n',' ',title)\n",
        "    title_dict[keys] = title\n",
        "  # matches for abstract data\n",
        "  abstract_matches = re.search(r'(?:Abstract\\n)(.*)(?:\\d*Paper Body)',file_dict[keys], re.DOTALL)\n",
        "  if abstract_matches:\n",
        "    abstract = abstract_matches.group(1)\n",
        "    abstract = re.sub('-\\n','',abstract)\n",
        "    abstract = re.sub('\\n',' ',abstract)\n",
        "    abstract_dict[keys] = abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title_token = {} \n",
        "# create titke tokens\n",
        "tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
        "for keys, values in title_dict.items():\n",
        "    tokens = tokenizer.tokenize(values.lower())\n",
        "    title_token[keys] = tokens"
      ],
      "metadata": {
        "id": "oE3LwgJHOTR-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYU71Ssnh5vk",
        "outputId": "52406c42-e906-48b7-8e63-b1a219944aeb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'PP3164': ['efficient',\n",
              "  'principled',\n",
              "  'learning',\n",
              "  'of',\n",
              "  'thin',\n",
              "  'junction',\n",
              "  'trees'],\n",
              " 'PP3219': ['active',\n",
              "  'preference',\n",
              "  'learning',\n",
              "  'with',\n",
              "  'discrete',\n",
              "  'choice',\n",
              "  'data'],\n",
              " 'PP3220': ['receptive', 'fields', 'without', 'spike-triggering'],\n",
              " 'PP3249': ['computational',\n",
              "  'equivalence',\n",
              "  'of',\n",
              "  'fixed',\n",
              "  'points',\n",
              "  'and',\n",
              "  'no',\n",
              "  'regret',\n",
              "  'algorithms',\n",
              "  'and',\n",
              "  'convergence',\n",
              "  'to',\n",
              "  'equilibria'],\n",
              " 'PP3250': ['theoretical',\n",
              "  'analysis',\n",
              "  'of',\n",
              "  'heuristic',\n",
              "  'search',\n",
              "  'methods',\n",
              "  'for',\n",
              "  'online',\n",
              "  'pomdps'],\n",
              " 'PP3279': ['modeling',\n",
              "  'image',\n",
              "  'patches',\n",
              "  'with',\n",
              "  'directed',\n",
              "  'hierarchy',\n",
              "  'of',\n",
              "  'markov',\n",
              "  'random',\n",
              "  'fields'],\n",
              " 'PP3296': ['variational', 'inference', 'for', 'markov', 'jump', 'processes'],\n",
              " 'PP3305': ['general',\n",
              "  'boosting',\n",
              "  'method',\n",
              "  'and',\n",
              "  'its',\n",
              "  'application',\n",
              "  'to',\n",
              "  'learning',\n",
              "  'ranking',\n",
              "  'functions',\n",
              "  'for',\n",
              "  'web',\n",
              "  'search'],\n",
              " 'PP3309': ['the', 'infinite', 'gamma-poisson', 'feature', 'model'],\n",
              " 'PP3357': ['learning', 'framework', 'for', 'nearest', 'neighbor', 'search'],\n",
              " 'PP3402': ['global',\n",
              "  'ranking',\n",
              "  'using',\n",
              "  'continuous',\n",
              "  'conditional',\n",
              "  'random',\n",
              "  'fields'],\n",
              " 'PP3420': ['automatic',\n",
              "  'online',\n",
              "  'tuning',\n",
              "  'for',\n",
              "  'fast',\n",
              "  'gaussian',\n",
              "  'summation'],\n",
              " 'PP3464': ['robust',\n",
              "  'near-isometric',\n",
              "  'matching',\n",
              "  'via',\n",
              "  'structured',\n",
              "  'learning',\n",
              "  'of',\n",
              "  'graphical',\n",
              "  'models'],\n",
              " 'PP3466': ['sparsity',\n",
              "  'of',\n",
              "  'svms',\n",
              "  'that',\n",
              "  'use',\n",
              "  'the',\n",
              "  'epsilon-insensitive',\n",
              "  'loss'],\n",
              " 'PP3469': ['overlaying',\n",
              "  'classifiers',\n",
              "  'practical',\n",
              "  'approach',\n",
              "  'for',\n",
              "  'optimal',\n",
              "  'ranking'],\n",
              " 'PP3485': ['on',\n",
              "  'the',\n",
              "  'efficient',\n",
              "  'minimization',\n",
              "  'of',\n",
              "  'classification',\n",
              "  'calibrated',\n",
              "  'surrogates'],\n",
              " 'PP3501': ['fitted',\n",
              "  'iteration',\n",
              "  'by',\n",
              "  'advantage',\n",
              "  'weighted',\n",
              "  'regression'],\n",
              " 'PP3533': ['shape',\n",
              "  'aware',\n",
              "  'model',\n",
              "  'for',\n",
              "  'semi-supervised',\n",
              "  'learning',\n",
              "  'of',\n",
              "  'objects',\n",
              "  'and',\n",
              "  'its',\n",
              "  'context'],\n",
              " 'PP3575': ['deflation', 'methods', 'for', 'sparse', 'pca'],\n",
              " 'PP3583': ['scalable', 'hierarchical', 'distributed', 'language', 'model'],\n",
              " 'PP3587': ['unifying',\n",
              "  'the',\n",
              "  'sensory',\n",
              "  'and',\n",
              "  'motor',\n",
              "  'components',\n",
              "  'of',\n",
              "  'sensorimotor',\n",
              "  'adaptation'],\n",
              " 'PP3618': ['improved', 'moves', 'for', 'truncated', 'convex', 'models'],\n",
              " 'PP3631': ['neurometric',\n",
              "  'function',\n",
              "  'analysis',\n",
              "  'of',\n",
              "  'population',\n",
              "  'codes'],\n",
              " 'PP3632': ['white',\n",
              "  'functionals',\n",
              "  'for',\n",
              "  'anomaly',\n",
              "  'detection',\n",
              "  'in',\n",
              "  'dynamical',\n",
              "  'systems'],\n",
              " 'PP3637': ['nonparametric',\n",
              "  'bayesian',\n",
              "  'models',\n",
              "  'for',\n",
              "  'unsupervised',\n",
              "  'event',\n",
              "  'coreference',\n",
              "  'resolution'],\n",
              " 'PP3675': ['efficient',\n",
              "  'and',\n",
              "  'accurate',\n",
              "  'lp-norm',\n",
              "  'multiple',\n",
              "  'kernel',\n",
              "  'learning'],\n",
              " 'PP3707': ['fast',\n",
              "  'image',\n",
              "  'deconvolution',\n",
              "  'using',\n",
              "  'hyper-laplacian',\n",
              "  'priors'],\n",
              " 'PP3766': ['region-based', 'segmentation', 'and', 'object', 'detection'],\n",
              " 'PP3785': ['biologically',\n",
              "  'plausible',\n",
              "  'model',\n",
              "  'for',\n",
              "  'rapid',\n",
              "  'natural',\n",
              "  'scene',\n",
              "  'identification'],\n",
              " 'PP3809': ['convergent',\n",
              "  'temporal-difference',\n",
              "  'learning',\n",
              "  'with',\n",
              "  'arbitrary',\n",
              "  'smooth',\n",
              "  'function',\n",
              "  'approximation'],\n",
              " 'PP3847': ['sparse', 'metric', 'learning', 'via', 'smooth', 'optimization'],\n",
              " 'PP3855': ['graph-based',\n",
              "  'consensus',\n",
              "  'maximization',\n",
              "  'among',\n",
              "  'multiple',\n",
              "  'supervised',\n",
              "  'and',\n",
              "  'unsupervised',\n",
              "  'models'],\n",
              " 'PP3869': ['conditional', 'neural', 'fields'],\n",
              " 'PP3870': ['sequential',\n",
              "  'effects',\n",
              "  'reflect',\n",
              "  'parallel',\n",
              "  'learning',\n",
              "  'of',\n",
              "  'multiple',\n",
              "  'environmental',\n",
              "  'regularities'],\n",
              " 'PP3872': ['object', 'recognition', 'with', 'deep', 'belief', 'nets'],\n",
              " 'PP3906': ['new', 'probabilistic', 'model', 'for', 'rank', 'aggregation'],\n",
              " 'PP3910': ['unified',\n",
              "  'model',\n",
              "  'of',\n",
              "  'short-range',\n",
              "  'and',\n",
              "  'long-range',\n",
              "  'motion',\n",
              "  'perception'],\n",
              " 'PP3950': ['log-domain',\n",
              "  'implementation',\n",
              "  'of',\n",
              "  'the',\n",
              "  'diffusion',\n",
              "  'network',\n",
              "  'in',\n",
              "  'very',\n",
              "  'large',\n",
              "  'scale',\n",
              "  'integration'],\n",
              " 'PP4004': ['on', 'herding', 'and', 'the', 'perceptron', 'cycling', 'theorem'],\n",
              " 'PP4050': ['accounting',\n",
              "  'for',\n",
              "  'network',\n",
              "  'effects',\n",
              "  'in',\n",
              "  'neuronal',\n",
              "  'responses',\n",
              "  'using',\n",
              "  'l1',\n",
              "  'regularized',\n",
              "  'point',\n",
              "  'process',\n",
              "  'models'],\n",
              " 'PP4070': ['identifying', 'dendritic', 'processing'],\n",
              " 'PP4094': ['word', 'features', 'for', 'latent', 'dirichlet', 'allocation'],\n",
              " 'PP4111': ['large-scale',\n",
              "  'matrix',\n",
              "  'factorization',\n",
              "  'with',\n",
              "  'missing',\n",
              "  'data',\n",
              "  'under',\n",
              "  'additional',\n",
              "  'constraints'],\n",
              " 'PP4127': ['learning',\n",
              "  'to',\n",
              "  'localise',\n",
              "  'sounds',\n",
              "  'with',\n",
              "  'spiking',\n",
              "  'neural',\n",
              "  'networks'],\n",
              " 'PP4134': ['sidestepping',\n",
              "  'intractable',\n",
              "  'inference',\n",
              "  'with',\n",
              "  'structured',\n",
              "  'ensemble',\n",
              "  'cascades'],\n",
              " 'PP4157': ['fast',\n",
              "  'detection',\n",
              "  'of',\n",
              "  'multiple',\n",
              "  'change-points',\n",
              "  'shared',\n",
              "  'by',\n",
              "  'many',\n",
              "  'signals',\n",
              "  'using',\n",
              "  'group',\n",
              "  'lars'],\n",
              " 'PP4210': ['learning',\n",
              "  'patient-specific',\n",
              "  'cancer',\n",
              "  'survival',\n",
              "  'distributions',\n",
              "  'as',\n",
              "  'sequence',\n",
              "  'of',\n",
              "  'dependent',\n",
              "  'regressors'],\n",
              " 'PP4219': ['automated',\n",
              "  'refinement',\n",
              "  'of',\n",
              "  'bayes',\n",
              "  'networks',\n",
              "  'parameters',\n",
              "  'based',\n",
              "  'on',\n",
              "  'test',\n",
              "  'ordering',\n",
              "  'constraints'],\n",
              " 'PP4222': ['universal',\n",
              "  'low-rank',\n",
              "  'matrix',\n",
              "  'recovery',\n",
              "  'from',\n",
              "  'pauli',\n",
              "  'measurements'],\n",
              " 'PP4225': ['finite',\n",
              "  'time',\n",
              "  'analysis',\n",
              "  'of',\n",
              "  'stratified',\n",
              "  'sampling',\n",
              "  'for',\n",
              "  'monte',\n",
              "  'carlo'],\n",
              " 'PP4246': ['sparse', 'manifold', 'clustering', 'and', 'embedding'],\n",
              " 'PP4259': ['the',\n",
              "  'local',\n",
              "  'rademacher',\n",
              "  'complexity',\n",
              "  'of',\n",
              "  'lp-norm',\n",
              "  'multiple',\n",
              "  'kernel',\n",
              "  'learning'],\n",
              " 'PP4364': ['two',\n",
              "  'is',\n",
              "  'better',\n",
              "  'than',\n",
              "  'one',\n",
              "  'distinct',\n",
              "  'roles',\n",
              "  'for',\n",
              "  'familiarity',\n",
              "  'and',\n",
              "  'recollection',\n",
              "  'in',\n",
              "  'retrieving',\n",
              "  'palimpsest',\n",
              "  'memories'],\n",
              " 'PP4421': ['convergence', 'analysis', 'of', 'log-linear', 'training'],\n",
              " 'PP4429': ['efficient',\n",
              "  'learning',\n",
              "  'of',\n",
              "  'generalized',\n",
              "  'linear',\n",
              "  'and',\n",
              "  'single',\n",
              "  'index',\n",
              "  'models',\n",
              "  'with',\n",
              "  'isotonic',\n",
              "  'regression'],\n",
              " 'PP4457': ['shaping', 'level', 'sets', 'with', 'submodular', 'functions'],\n",
              " 'PP4477': ['monte', 'carlo', 'value', 'iteration', 'with', 'macro-actions'],\n",
              " 'PP4502': ['high-order',\n",
              "  'multi-task',\n",
              "  'feature',\n",
              "  'learning',\n",
              "  'to',\n",
              "  'identify',\n",
              "  'longitudinal',\n",
              "  'phenotypic',\n",
              "  'markers',\n",
              "  'for',\n",
              "  'alzheimer',\n",
              "  'disease',\n",
              "  'progression',\n",
              "  'prediction'],\n",
              " 'PP4509': ['query', 'complexity', 'of', 'derivative-free', 'optimization'],\n",
              " 'PP4512': ['affine', 'independent', 'variational', 'inference'],\n",
              " 'PP4558': ['polynomial-time', 'form', 'of', 'robust', 'regression'],\n",
              " 'PP4569': ['efficient',\n",
              "  'monte',\n",
              "  'carlo',\n",
              "  'counterfactual',\n",
              "  'regret',\n",
              "  'minimization',\n",
              "  'in',\n",
              "  'games',\n",
              "  'with',\n",
              "  'many',\n",
              "  'player',\n",
              "  'actions'],\n",
              " 'PP4608': ['systematic',\n",
              "  'approach',\n",
              "  'to',\n",
              "  'extracting',\n",
              "  'semantic',\n",
              "  'information',\n",
              "  'from',\n",
              "  'functional',\n",
              "  'mri',\n",
              "  'data'],\n",
              " 'PP4622': ['tensor',\n",
              "  'decomposition',\n",
              "  'for',\n",
              "  'fast',\n",
              "  'parsing',\n",
              "  'with',\n",
              "  'latent-variable',\n",
              "  'pcfgs'],\n",
              " 'PP4640': ['best',\n",
              "  'arm',\n",
              "  'identification',\n",
              "  'unified',\n",
              "  'approach',\n",
              "  'to',\n",
              "  'fixed',\n",
              "  'budget',\n",
              "  'and',\n",
              "  'fixed',\n",
              "  'confidence'],\n",
              " 'PP4654': ['augmented-svm',\n",
              "  'automatic',\n",
              "  'space',\n",
              "  'partitioning',\n",
              "  'for',\n",
              "  'combining',\n",
              "  'multiple',\n",
              "  'non-linear',\n",
              "  'dynamics'],\n",
              " 'PP4670': ['rational', 'inference', 'of', 'relative', 'preferences'],\n",
              " 'PP4675': ['perfect',\n",
              "  'dimensionality',\n",
              "  'recovery',\n",
              "  'by',\n",
              "  'variational',\n",
              "  'bayesian',\n",
              "  'pca'],\n",
              " 'PP4689': ['selecting',\n",
              "  'diverse',\n",
              "  'features',\n",
              "  'via',\n",
              "  'spectral',\n",
              "  'regularization'],\n",
              " 'PP4701': ['iterative', 'ranking', 'from', 'pair-wise', 'comparisons'],\n",
              " 'PP4732': ['pointwise',\n",
              "  'tracking',\n",
              "  'the',\n",
              "  'optimal',\n",
              "  'regression',\n",
              "  'function'],\n",
              " 'PP4768': ['exponential',\n",
              "  'concentration',\n",
              "  'for',\n",
              "  'mutual',\n",
              "  'information',\n",
              "  'estimation',\n",
              "  'with',\n",
              "  'application',\n",
              "  'to',\n",
              "  'forests'],\n",
              " 'PP4784': ['factorial',\n",
              "  'lda',\n",
              "  'sparse',\n",
              "  'multi-dimensional',\n",
              "  'text',\n",
              "  'models'],\n",
              " 'PP4792': ['fusion',\n",
              "  'with',\n",
              "  'diffusion',\n",
              "  'for',\n",
              "  'robust',\n",
              "  'visual',\n",
              "  'tracking'],\n",
              " 'PP4800': ['on',\n",
              "  'the',\n",
              "  'use',\n",
              "  'of',\n",
              "  'non-stationary',\n",
              "  'policies',\n",
              "  'for',\n",
              "  'stationary',\n",
              "  'infinite-horizon',\n",
              "  'markov',\n",
              "  'decision',\n",
              "  'processes'],\n",
              " 'PP4814': ['learning',\n",
              "  'visual',\n",
              "  'motion',\n",
              "  'in',\n",
              "  'recurrent',\n",
              "  'neural',\n",
              "  'networks'],\n",
              " 'PP4836': ['spectral',\n",
              "  'learning',\n",
              "  'of',\n",
              "  'linear',\n",
              "  'dynamics',\n",
              "  'from',\n",
              "  'generalised-linear',\n",
              "  'observations',\n",
              "  'with',\n",
              "  'application',\n",
              "  'to',\n",
              "  'neural',\n",
              "  'population',\n",
              "  'data'],\n",
              " 'PP4863': ['on', 'decomposing', 'the', 'proximal', 'map'],\n",
              " 'PP4873': ['bayesian',\n",
              "  'entropy',\n",
              "  'estimation',\n",
              "  'for',\n",
              "  'binary',\n",
              "  'spike',\n",
              "  'train',\n",
              "  'data',\n",
              "  'using',\n",
              "  'parametric',\n",
              "  'prior',\n",
              "  'knowledge'],\n",
              " 'PP4949': ['graphical',\n",
              "  'transformation',\n",
              "  'for',\n",
              "  'belief',\n",
              "  'propagation',\n",
              "  'maximum',\n",
              "  'weight',\n",
              "  'matchings',\n",
              "  'and',\n",
              "  'odd-sized',\n",
              "  'cycles'],\n",
              " 'PP4950': ['sensor',\n",
              "  'selection',\n",
              "  'in',\n",
              "  'high-dimensional',\n",
              "  'gaussian',\n",
              "  'trees',\n",
              "  'with',\n",
              "  'nuisances'],\n",
              " 'PP4958': ['active',\n",
              "  'learning',\n",
              "  'for',\n",
              "  'probabilistic',\n",
              "  'hypotheses',\n",
              "  'using',\n",
              "  'the',\n",
              "  'maximum',\n",
              "  'gibbs',\n",
              "  'error',\n",
              "  'criterion'],\n",
              " 'PP4979': ['relevance',\n",
              "  'topic',\n",
              "  'model',\n",
              "  'for',\n",
              "  'unstructured',\n",
              "  'social',\n",
              "  'group',\n",
              "  'activity',\n",
              "  'recognition'],\n",
              " 'PP4980': ['streaming', 'variational', 'bayes'],\n",
              " 'PP4988': ['reflection',\n",
              "  'methods',\n",
              "  'for',\n",
              "  'user-friendly',\n",
              "  'submodular',\n",
              "  'optimization'],\n",
              " 'PP5042': ['machine',\n",
              "  'teaching',\n",
              "  'for',\n",
              "  'bayesian',\n",
              "  'learners',\n",
              "  'in',\n",
              "  'the',\n",
              "  'exponential',\n",
              "  'family'],\n",
              " 'PP5072': ['efficient',\n",
              "  'online',\n",
              "  'inference',\n",
              "  'for',\n",
              "  'bayesian',\n",
              "  'nonparametric',\n",
              "  'relational',\n",
              "  'models'],\n",
              " 'PP5090': ['spike',\n",
              "  'train',\n",
              "  'entropy-rate',\n",
              "  'estimation',\n",
              "  'using',\n",
              "  'hierarchical',\n",
              "  'dirichlet',\n",
              "  'process',\n",
              "  'priors'],\n",
              " 'PP5108': ['prior-free',\n",
              "  'and',\n",
              "  'prior-dependent',\n",
              "  'regret',\n",
              "  'bounds',\n",
              "  'for',\n",
              "  'thompson',\n",
              "  'sampling'],\n",
              " 'PP5131': ['online', 'robust', 'pca', 'via', 'stochastic', 'optimization'],\n",
              " 'PP5148': ['minimax',\n",
              "  'optimal',\n",
              "  'algorithms',\n",
              "  'for',\n",
              "  'unconstrained',\n",
              "  'linear',\n",
              "  'optimization'],\n",
              " 'PP5179': ['learning',\n",
              "  'trajectory',\n",
              "  'preferences',\n",
              "  'for',\n",
              "  'manipulators',\n",
              "  'via',\n",
              "  'iterative',\n",
              "  'improvement'],\n",
              " 'PP5184': ['projected', 'natural', 'actor-critic'],\n",
              " 'PP5193': ['learning', 'the', 'local', 'statistics', 'of', 'optical', 'flow'],\n",
              " 'PP5198': ['higher',\n",
              "  'order',\n",
              "  'priors',\n",
              "  'for',\n",
              "  'joint',\n",
              "  'intrinsic',\n",
              "  'image',\n",
              "  'objects',\n",
              "  'and',\n",
              "  'attributes',\n",
              "  'estimation'],\n",
              " 'PP5258': ['saga',\n",
              "  'fast',\n",
              "  'incremental',\n",
              "  'gradient',\n",
              "  'method',\n",
              "  'with',\n",
              "  'support',\n",
              "  'for',\n",
              "  'non-strongly',\n",
              "  'convex',\n",
              "  'composite',\n",
              "  'objectives'],\n",
              " 'PP5261': ['simple', 'map', 'inference', 'via', 'low-rank', 'relaxations'],\n",
              " 'PP5307': ['convex',\n",
              "  'optimization',\n",
              "  'procedure',\n",
              "  'for',\n",
              "  'clustering',\n",
              "  'theoretical',\n",
              "  'revisit'],\n",
              " 'PP5315': ['projecting',\n",
              "  'markov',\n",
              "  'random',\n",
              "  'field',\n",
              "  'parameters',\n",
              "  'for',\n",
              "  'fast',\n",
              "  'mixing'],\n",
              " 'PP5325': ['submodular',\n",
              "  'meets',\n",
              "  'structured',\n",
              "  'finding',\n",
              "  'diverse',\n",
              "  'subsets',\n",
              "  'in',\n",
              "  'exponentially-large',\n",
              "  'structured',\n",
              "  'item',\n",
              "  'sets'],\n",
              " 'PP5341': ['scalable',\n",
              "  'inference',\n",
              "  'for',\n",
              "  'neuronal',\n",
              "  'connectivity',\n",
              "  'from',\n",
              "  'calcium',\n",
              "  'imaging'],\n",
              " 'PP5380': ['discovering', 'learning', 'and', 'exploiting', 'relevance'],\n",
              " 'PP5385': ['discriminative',\n",
              "  'metric',\n",
              "  'learning',\n",
              "  'by',\n",
              "  'neighborhood',\n",
              "  'gerrymandering'],\n",
              " 'PP5403': ['compressive',\n",
              "  'sensing',\n",
              "  'of',\n",
              "  'signals',\n",
              "  'from',\n",
              "  'gmm',\n",
              "  'with',\n",
              "  'sparse',\n",
              "  'precision',\n",
              "  'matrices'],\n",
              " 'PP5425': ['multiplicative',\n",
              "  'model',\n",
              "  'for',\n",
              "  'learning',\n",
              "  'distributed',\n",
              "  'text-based',\n",
              "  'attribute',\n",
              "  'representations'],\n",
              " 'PP5437': ['learning', 'mixtures', 'of', 'ranking', 'models'],\n",
              " 'PP5443': ['difference',\n",
              "  'of',\n",
              "  'convex',\n",
              "  'functions',\n",
              "  'programming',\n",
              "  'for',\n",
              "  'reinforcement',\n",
              "  'learning'],\n",
              " 'PP5477': ['neural',\n",
              "  'word',\n",
              "  'embedding',\n",
              "  'as',\n",
              "  'implicit',\n",
              "  'matrix',\n",
              "  'factorization'],\n",
              " 'PP5482': ['testing', 'unfaithful', 'gaussian', 'graphical', 'models'],\n",
              " 'PP5503': ['simultaneous',\n",
              "  'model',\n",
              "  'selection',\n",
              "  'and',\n",
              "  'optimization',\n",
              "  'through',\n",
              "  'parameter-free',\n",
              "  'stochastic',\n",
              "  'learning'],\n",
              " 'PP5507': ['bayesian',\n",
              "  'nonlinear',\n",
              "  'support',\n",
              "  'vector',\n",
              "  'machines',\n",
              "  'and',\n",
              "  'discriminative',\n",
              "  'factor',\n",
              "  'modeling'],\n",
              " 'PP5512': ['boosting',\n",
              "  'framework',\n",
              "  'on',\n",
              "  'grounds',\n",
              "  'of',\n",
              "  'online',\n",
              "  'learning'],\n",
              " 'PP5549': ['modeling',\n",
              "  'deep',\n",
              "  'temporal',\n",
              "  'dependencies',\n",
              "  'with',\n",
              "  'recurrent',\n",
              "  'grammar',\n",
              "  'cells'],\n",
              " 'PP5569': ['hardness',\n",
              "  'of',\n",
              "  'parameter',\n",
              "  'estimation',\n",
              "  'in',\n",
              "  'graphical',\n",
              "  'models'],\n",
              " 'PP5575': ['graphical',\n",
              "  'models',\n",
              "  'for',\n",
              "  'recovering',\n",
              "  'probabilistic',\n",
              "  'and',\n",
              "  'causal',\n",
              "  'queries',\n",
              "  'from',\n",
              "  'missing',\n",
              "  'data'],\n",
              " 'PP5594': ['incremental', 'local', 'gaussian', 'regression'],\n",
              " 'PP5618': ['cone-constrained', 'principal', 'component', 'analysis'],\n",
              " 'PP5640': ['exploring',\n",
              "  'models',\n",
              "  'and',\n",
              "  'data',\n",
              "  'for',\n",
              "  'image',\n",
              "  'question',\n",
              "  'answering'],\n",
              " 'PP5650': ['theory',\n",
              "  'of',\n",
              "  'decision',\n",
              "  'making',\n",
              "  'under',\n",
              "  'dynamic',\n",
              "  'context'],\n",
              " 'PP5662': ['robust',\n",
              "  'feature-sample',\n",
              "  'linear',\n",
              "  'discriminant',\n",
              "  'analysis',\n",
              "  'for',\n",
              "  'brain',\n",
              "  'disorders',\n",
              "  'diagnosis'],\n",
              " 'PP5671': ['rethinking',\n",
              "  'lda',\n",
              "  'moment',\n",
              "  'matching',\n",
              "  'for',\n",
              "  'discrete',\n",
              "  'ica'],\n",
              " 'PP5696': ['on',\n",
              "  'the',\n",
              "  'convergence',\n",
              "  'of',\n",
              "  'stochastic',\n",
              "  'gradient',\n",
              "  'mcmc',\n",
              "  'algorithms',\n",
              "  'with',\n",
              "  'high-order',\n",
              "  'integrators'],\n",
              " 'PP5751': ['asynchronous',\n",
              "  'parallel',\n",
              "  'stochastic',\n",
              "  'gradient',\n",
              "  'for',\n",
              "  'nonconvex',\n",
              "  'optimization'],\n",
              " 'PP5752': ['distributed',\n",
              "  'submodular',\n",
              "  'cover',\n",
              "  'succinctly',\n",
              "  'summarizing',\n",
              "  'massive',\n",
              "  'data'],\n",
              " 'PP5808': ['community', 'detection', 'via', 'measure', 'space', 'embedding'],\n",
              " 'PP5813': ['scale',\n",
              "  'up',\n",
              "  'nonlinear',\n",
              "  'component',\n",
              "  'analysis',\n",
              "  'with',\n",
              "  'doubly',\n",
              "  'stochastic',\n",
              "  'gradients'],\n",
              " 'PP5860': ['on-the',\n",
              "  'job',\n",
              "  'learning',\n",
              "  'with',\n",
              "  'bayesian',\n",
              "  'decision',\n",
              "  'theory'],\n",
              " 'PP5876': ['streaming',\n",
              "  'distributed',\n",
              "  'variational',\n",
              "  'inference',\n",
              "  'for',\n",
              "  'bayesian',\n",
              "  'nonparametrics'],\n",
              " 'PP5905': ['approximating', 'sparse', 'pca', 'from', 'incomplete', 'data'],\n",
              " 'PP5916': ['matrix',\n",
              "  'completion',\n",
              "  'under',\n",
              "  'monotonic',\n",
              "  'single',\n",
              "  'index',\n",
              "  'models'],\n",
              " 'PP5975': ['discriminative', 'robust', 'transformation', 'learning'],\n",
              " 'PP5993': ['generalization',\n",
              "  'in',\n",
              "  'adaptive',\n",
              "  'data',\n",
              "  'analysis',\n",
              "  'and',\n",
              "  'holdout',\n",
              "  'reuse'],\n",
              " 'PP6017': ['sparse', 'and', 'low-rank', 'tensor', 'decomposition'],\n",
              " 'PP6018': ['analysis', 'of', 'robust', 'pca', 'via', 'local', 'incoherence'],\n",
              " 'PP6019': ['algorithmic', 'stability', 'and', 'uniform', 'generalization'],\n",
              " 'PP6020': ['mixing',\n",
              "  'time',\n",
              "  'estimation',\n",
              "  'in',\n",
              "  'reversible',\n",
              "  'markov',\n",
              "  'chains',\n",
              "  'from',\n",
              "  'single',\n",
              "  'sample',\n",
              "  'path'],\n",
              " 'PP6022': ['unified',\n",
              "  'view',\n",
              "  'of',\n",
              "  'matrix',\n",
              "  'completion',\n",
              "  'under',\n",
              "  'general',\n",
              "  'structural',\n",
              "  'constraints'],\n",
              " 'PP6030': ['fighting', 'bandits', 'with', 'new', 'kind', 'of', 'smoothness'],\n",
              " 'PP6035': ['adaptive',\n",
              "  'low-complexity',\n",
              "  'sequential',\n",
              "  'inference',\n",
              "  'for',\n",
              "  'dirichlet',\n",
              "  'process',\n",
              "  'mixture',\n",
              "  'models'],\n",
              " 'PP6037': ['sub-sampled',\n",
              "  'newton',\n",
              "  'methods',\n",
              "  'with',\n",
              "  'non-uniform',\n",
              "  'sampling'],\n",
              " 'PP6043': ['unified',\n",
              "  'methods',\n",
              "  'for',\n",
              "  'exploiting',\n",
              "  'piecewise',\n",
              "  'linear',\n",
              "  'structure',\n",
              "  'in',\n",
              "  'convex',\n",
              "  'optimization'],\n",
              " 'PP6045': ['learning',\n",
              "  'shape',\n",
              "  'correspondence',\n",
              "  'with',\n",
              "  'anisotropic',\n",
              "  'convolutional',\n",
              "  'neural',\n",
              "  'networks'],\n",
              " 'PP6050': ['differential', 'privacy', 'without', 'sensitivity'],\n",
              " 'PP6067': ['tagger', 'deep', 'unsupervised', 'perceptual', 'grouping'],\n",
              " 'PP6104': ['fast', 'learning', 'rates', 'with', 'heavy-tailed', 'losses'],\n",
              " 'PP6126': ['robust', 'means', 'theoretical', 'revisit'],\n",
              " 'PP6130': ['probing',\n",
              "  'the',\n",
              "  'compositionality',\n",
              "  'of',\n",
              "  'intuitive',\n",
              "  'functions'],\n",
              " 'PP6153': ['estimating',\n",
              "  'nonlinear',\n",
              "  'neural',\n",
              "  'response',\n",
              "  'functions',\n",
              "  'using',\n",
              "  'gp',\n",
              "  'priors',\n",
              "  'and',\n",
              "  'kronecker',\n",
              "  'methods'],\n",
              " 'PP6203': ['understanding',\n",
              "  'the',\n",
              "  'effective',\n",
              "  'receptive',\n",
              "  'field',\n",
              "  'in',\n",
              "  'deep',\n",
              "  'convolutional',\n",
              "  'neural',\n",
              "  'networks'],\n",
              " 'PP6207': ['efficient',\n",
              "  'second',\n",
              "  'order',\n",
              "  'online',\n",
              "  'learning',\n",
              "  'by',\n",
              "  'sketching'],\n",
              " 'PP6208': ['nyi', 'divergence', 'variational', 'inference'],\n",
              " 'PP6231': ['probabilistic', 'framework', 'for', 'deep', 'learning'],\n",
              " 'PP6237': ['eliciting',\n",
              "  'categorical',\n",
              "  'data',\n",
              "  'for',\n",
              "  'optimal',\n",
              "  'aggregation'],\n",
              " 'PP6284': ['latent', 'attention', 'for', 'if-then', 'program', 'synthesis'],\n",
              " 'PP6299': ['breaking',\n",
              "  'the',\n",
              "  'bandwidth',\n",
              "  'barrier',\n",
              "  'geometrical',\n",
              "  'adaptive',\n",
              "  'entropy',\n",
              "  'estimation'],\n",
              " 'PP6331': ['robustness',\n",
              "  'of',\n",
              "  'classifiers',\n",
              "  'from',\n",
              "  'adversarial',\n",
              "  'to',\n",
              "  'random',\n",
              "  'noise'],\n",
              " 'PP6333': ['regularization',\n",
              "  'with',\n",
              "  'stochastic',\n",
              "  'transformations',\n",
              "  'and',\n",
              "  'perturbations',\n",
              "  'for',\n",
              "  'deep',\n",
              "  'semi-supervised',\n",
              "  'learning'],\n",
              " 'PP6354': ['theoretical',\n",
              "  'comparisons',\n",
              "  'of',\n",
              "  'positive-unlabeled',\n",
              "  'learning',\n",
              "  'against',\n",
              "  'positive-negative',\n",
              "  'learning'],\n",
              " 'PP6400': ['improved',\n",
              "  'regret',\n",
              "  'bounds',\n",
              "  'for',\n",
              "  'oracle-based',\n",
              "  'adversarial',\n",
              "  'contextual',\n",
              "  'bandits'],\n",
              " 'PP6408': ['learning',\n",
              "  'from',\n",
              "  'small',\n",
              "  'sample',\n",
              "  'sets',\n",
              "  'by',\n",
              "  'combining',\n",
              "  'unsupervised',\n",
              "  'meta-training',\n",
              "  'with',\n",
              "  'cnns'],\n",
              " 'PP6428': ['the',\n",
              "  'sound',\n",
              "  'of',\n",
              "  'apalm',\n",
              "  'clapping',\n",
              "  'faster',\n",
              "  'nonsmooth',\n",
              "  'nonconvex',\n",
              "  'optimization',\n",
              "  'with',\n",
              "  'stochastic',\n",
              "  'asynchronous',\n",
              "  'palm'],\n",
              " 'PP6430': ['linear',\n",
              "  'dynamical',\n",
              "  'neural',\n",
              "  'population',\n",
              "  'models',\n",
              "  'through',\n",
              "  'nonlinear',\n",
              "  'embeddings'],\n",
              " 'PP6509': ['deep',\n",
              "  'learning',\n",
              "  'for',\n",
              "  'predicting',\n",
              "  'human',\n",
              "  'strategic',\n",
              "  'behavior'],\n",
              " 'PP6516': ['tensor', 'switching', 'networks'],\n",
              " 'PP6546': ['multiple-play',\n",
              "  'bandits',\n",
              "  'in',\n",
              "  'the',\n",
              "  'position-based',\n",
              "  'model'],\n",
              " 'PP6560': ['dual',\n",
              "  'space',\n",
              "  'gradient',\n",
              "  'descent',\n",
              "  'for',\n",
              "  'online',\n",
              "  'learning'],\n",
              " 'PP6561': ['improved',\n",
              "  'dropout',\n",
              "  'for',\n",
              "  'shallow',\n",
              "  'and',\n",
              "  'deep',\n",
              "  'learning'],\n",
              " 'PP6575': ['pac',\n",
              "  'reinforcement',\n",
              "  'learning',\n",
              "  'with',\n",
              "  'rich',\n",
              "  'observations'],\n",
              " 'PP6617': ['machine',\n",
              "  'learning',\n",
              "  'with',\n",
              "  'adversaries',\n",
              "  'byzantine',\n",
              "  'tolerant',\n",
              "  'gradient',\n",
              "  'descent'],\n",
              " 'PP6652': ['non-monotone',\n",
              "  'continuous',\n",
              "  'dr-submodular',\n",
              "  'maximization',\n",
              "  'structure',\n",
              "  'and',\n",
              "  'algorithms'],\n",
              " 'PP6672': ['unsupervised', 'image-to', 'image', 'translation', 'networks'],\n",
              " 'PP6694': ['matching',\n",
              "  'on',\n",
              "  'balanced',\n",
              "  'nonlinear',\n",
              "  'representations',\n",
              "  'for',\n",
              "  'treatment',\n",
              "  'effects',\n",
              "  'estimation'],\n",
              " 'PP6695': ['learning', 'overcomplete', 'hmms'],\n",
              " 'PP6703': ['inductive',\n",
              "  'representation',\n",
              "  'learning',\n",
              "  'on',\n",
              "  'large',\n",
              "  'graphs'],\n",
              " 'PP6716': ['improving',\n",
              "  'regret',\n",
              "  'bounds',\n",
              "  'for',\n",
              "  'combinatorial',\n",
              "  'semi-bandits',\n",
              "  'with',\n",
              "  'probabilistically',\n",
              "  'triggered',\n",
              "  'arms',\n",
              "  'and',\n",
              "  'its',\n",
              "  'applications'],\n",
              " 'PP6729': ['online', 'prediction', 'with', 'selfish', 'experts'],\n",
              " 'PP6767': ['counterfactual',\n",
              "  'gaussian',\n",
              "  'processes',\n",
              "  'for',\n",
              "  'reliable',\n",
              "  'decision-making',\n",
              "  'and',\n",
              "  'what-if',\n",
              "  'reasoning'],\n",
              " 'PP6770': ['train',\n",
              "  'longer',\n",
              "  'generalize',\n",
              "  'better',\n",
              "  'closing',\n",
              "  'the',\n",
              "  'generalization',\n",
              "  'gap',\n",
              "  'in',\n",
              "  'large',\n",
              "  'batch',\n",
              "  'training',\n",
              "  'of',\n",
              "  'neural',\n",
              "  'networks'],\n",
              " 'PP6773': ['minimal',\n",
              "  'exploration',\n",
              "  'in',\n",
              "  'structured',\n",
              "  'stochastic',\n",
              "  'bandits'],\n",
              " 'PP6789': ['speeding',\n",
              "  'up',\n",
              "  'latent',\n",
              "  'variable',\n",
              "  'gaussian',\n",
              "  'graphical',\n",
              "  'model',\n",
              "  'estimation',\n",
              "  'via',\n",
              "  'nonconvex',\n",
              "  'optimization'],\n",
              " 'PP6806': ['consistent', 'robust', 'regression'],\n",
              " 'PP6862': ['decomposition-invariant',\n",
              "  'conditional',\n",
              "  'gradient',\n",
              "  'for',\n",
              "  'general',\n",
              "  'polytopes',\n",
              "  'with',\n",
              "  'line',\n",
              "  'search'],\n",
              " 'PP6885': ['beyond',\n",
              "  'parity',\n",
              "  'fairness',\n",
              "  'objectives',\n",
              "  'for',\n",
              "  'collaborative',\n",
              "  'filtering'],\n",
              " 'PP6888': ['model-powered', 'conditional', 'independence', 'test'],\n",
              " 'PP6945': ['limitations',\n",
              "  'on',\n",
              "  'variance-reduction',\n",
              "  'and',\n",
              "  'acceleration',\n",
              "  'schemes',\n",
              "  'for',\n",
              "  'finite',\n",
              "  'sums',\n",
              "  'optimization'],\n",
              " 'PP6953': ['bayesian', 'gan'],\n",
              " 'PP6959': ['quantifying',\n",
              "  'how',\n",
              "  'much',\n",
              "  'sensory',\n",
              "  'information',\n",
              "  'in',\n",
              "  'neural',\n",
              "  'code',\n",
              "  'is',\n",
              "  'relevant',\n",
              "  'for',\n",
              "  'behavior'],\n",
              " 'PP6998': ['efficient',\n",
              "  'sublinear-regret',\n",
              "  'algorithms',\n",
              "  'for',\n",
              "  'online',\n",
              "  'sparse',\n",
              "  'linear',\n",
              "  'regression',\n",
              "  'with',\n",
              "  'limited',\n",
              "  'observation'],\n",
              " 'PP7027': ['polynomial',\n",
              "  'codes',\n",
              "  'an',\n",
              "  'optimal',\n",
              "  'design',\n",
              "  'for',\n",
              "  'high-dimensional',\n",
              "  'coded',\n",
              "  'matrix',\n",
              "  'multiplication'],\n",
              " 'PP7038': ['trimmed', 'density', 'ratio', 'estimation'],\n",
              " 'PP7039': ['training',\n",
              "  'recurrent',\n",
              "  'networks',\n",
              "  'to',\n",
              "  'generate',\n",
              "  'hypotheses',\n",
              "  'about',\n",
              "  'how',\n",
              "  'the',\n",
              "  'brain',\n",
              "  'solves',\n",
              "  'hard',\n",
              "  'navigation',\n",
              "  'problems'],\n",
              " 'PP7093': ['perturbative', 'black', 'box', 'variational', 'inference'],\n",
              " 'PP7103': ['variational',\n",
              "  'inference',\n",
              "  'for',\n",
              "  'gaussian',\n",
              "  'process',\n",
              "  'models',\n",
              "  'with',\n",
              "  'linear',\n",
              "  'complexity'],\n",
              " 'PP7123': ['hybrid',\n",
              "  'reward',\n",
              "  'architecture',\n",
              "  'for',\n",
              "  'reinforcement',\n",
              "  'learning'],\n",
              " 'PP7129': ['greedy',\n",
              "  'approach',\n",
              "  'for',\n",
              "  'budgeted',\n",
              "  'maximum',\n",
              "  'inner',\n",
              "  'product',\n",
              "  'search'],\n",
              " 'PP7131': ['plan',\n",
              "  'attend',\n",
              "  'generate',\n",
              "  'planning',\n",
              "  'for',\n",
              "  'sequence-to',\n",
              "  'sequence',\n",
              "  'models'],\n",
              " 'PP7146': ['do', 'deep', 'neural', 'networks', 'suffer', 'from', 'crowding'],\n",
              " 'PP7245': ['spectral',\n",
              "  'mixture',\n",
              "  'kernels',\n",
              "  'for',\n",
              "  'multi-output',\n",
              "  'gaussian',\n",
              "  'processes'],\n",
              " 'PP7249': ['learning',\n",
              "  'hierarchical',\n",
              "  'information',\n",
              "  'flow',\n",
              "  'with',\n",
              "  'recurrent',\n",
              "  'neural',\n",
              "  'modules'],\n",
              " 'PP7262': ['affinity',\n",
              "  'clustering',\n",
              "  'hierarchical',\n",
              "  'clustering',\n",
              "  'at',\n",
              "  'scale']}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# removing stop words\n",
        "title_tok_list = list(chain.from_iterable(title_token.values()))\n",
        "stop_title = {}\n",
        "for keys, values in title_token.items():\n",
        "  stopped_tokens = []\n",
        "  for tokens in values:\n",
        "    if tokens not in stopwords_list:\n",
        "      stopped_tokens.append(tokens)\n",
        "  stop_title[keys] = stopped_tokens"
      ],
      "metadata": {
        "id": "klOwpD2HhbWL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# updated title token list\n",
        "title_tok_list = list(chain.from_iterable(stop_title.values()))\n",
        "# creates iter object with frequency distribution of tokens\n",
        "title_freq = FreqDist(title_tok_list).most_common(10)[:10]\n",
        "# sorts data in descending order of frequency and alphabetically\n",
        "sorted_title = sorted(sorted(title_freq, key = lambda x: x[0]), key = lambda x: x[1], reverse = True)\n",
        "# creates list of the top 10 tokens\n",
        "top_title = list([value[0] for value in sorted_title])"
      ],
      "metadata": {
        "id": "zf1GU4ox2kyV"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_freq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qtBixhmIeDa",
        "outputId": "d501160f-8b89-4b6a-95f5-f57f75124b99"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('learning', 46),\n",
              " ('models', 18),\n",
              " ('neural', 13),\n",
              " ('inference', 12),\n",
              " ('optimization', 12),\n",
              " ('data', 11),\n",
              " ('model', 11),\n",
              " ('networks', 10),\n",
              " ('estimation', 10),\n",
              " ('analysis', 9)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abstract_seg = {}\n",
        "# loops through the content dictionary\n",
        "for key, value in abstract_dict.items():\n",
        "  abstract_sentence = []\n",
        "  # segements the entire paper body into different sentences \n",
        "  sentence = nltk.sent_tokenize(value)\n",
        "  # sentence normaliation\n",
        "  for i in range(0, len(sentence)):\n",
        "    segment = sentence[i].strip()\n",
        "    norm_list = segment.split(' ')\n",
        "    norm_list[0] = norm_list[0].lower()\n",
        "    norm_sentence = ' '.join(norm_list)\n",
        "    # first word of each sentence is changed to lower case\n",
        "    #sentence[i] = sentence[i][0].lower() + sentence[i][1:]\n",
        "    # stores the normalized sentence\n",
        "    abstract_sentence.append(norm_sentence)\n",
        "  abstract_seg[key] = ''.join(abstract_sentence)"
      ],
      "metadata": {
        "id": "AJTNeH7_31Lh"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstract_token = {} \n",
        "tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
        "for keys, values in abstract_dict.items():\n",
        "    tokens = tokenizer.tokenize(values.lower())\n",
        "    abstract_token[keys] = tokens"
      ],
      "metadata": {
        "id": "Z599DFfb_Jyu"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstract_tok_list = list(chain.from_iterable(abstract_token.values()))\n",
        "stop_abstract = {}\n",
        "for keys, values in abstract_token.items():\n",
        "  stopped_tokens = []\n",
        "  for tokens in values:\n",
        "    if tokens not in stopwords_list:\n",
        "      stopped_tokens.append(tokens)\n",
        "  stop_abstract[keys] = stopped_tokens"
      ],
      "metadata": {
        "id": "5zBCcCFs6SsX"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstract_tok_list = list(chain.from_iterable(stop_abstract.values()))\n",
        "abstract_freq = FreqDist(abstract_tok_list).most_common(10)[:10]\n",
        "sorted_abstract = sorted(sorted(abstract_freq, key = lambda x: x[0]), key = lambda x: x[1], reverse = True)\n",
        "top_abstract = list([value[0] for value in sorted_abstract])"
      ],
      "metadata": {
        "id": "kL5jIThM_pcc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_abstract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0Y2xhDZpZBr",
        "outputId": "58032cc2-b2cd-4039-c8b7-7464e952a59f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model',\n",
              " 'learning',\n",
              " 'algorithm',\n",
              " 'data',\n",
              " 'problem',\n",
              " 'show',\n",
              " 'algorithms',\n",
              " 'models',\n",
              " 'paper',\n",
              " 'methods']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "authors = []\n",
        "# separates the full name of all the authors in the data\n",
        "for key in auth_dict.keys():\n",
        "  auth_list = auth_dict[key].split(\",\")\n",
        "  for i in auth_list:\n",
        "    if i != '':\n",
        "       authors.append(i)\n",
        "authors[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Y_cTxtjGAWyM",
        "outputId": "f666cd78-b026-4468-c485-226bbce5099d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Matthias Bethge'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auth_freq = FreqDist(authors).most_common(10)[:10]\n",
        "sorted_authors = sorted(sorted(auth_freq, key = lambda x: x[0]), key = lambda x: x[1], reverse = True)\n",
        "top_authors = list([value[0] for value in sorted_authors])"
      ],
      "metadata": {
        "id": "FyotPG_LI2ZK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create df for the stats\n",
        "stats_df = pd.DataFrame({\"top10_terms_in_abstracts\" : top_abstract,\n",
        "                        \"top10_terms_in_titles\" : top_title,\n",
        "                        \"top10_authors\" : top_authors})\n",
        "stats_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "hs_4Wa52uUTu",
        "outputId": "cd8033bd-56aa-4e88-89a9-c65c68df1dbb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  top10_terms_in_abstracts top10_terms_in_titles       top10_authors\n",
              "0                    model              learning        Francis Bach\n",
              "1                 learning                models  Geoffrey E. Hinton\n",
              "2                algorithm                neural      Lawrence Carin\n",
              "3                     data             inference      Percy S. Liang\n",
              "4                  problem          optimization       Richard Zemel\n",
              "5                     show                  data     Carlos Guestrin\n",
              "6               algorithms                 model      Jakob H. Macke\n",
              "7                   models            estimation       Manfred Opper\n",
              "8                    paper              networks     Matthias Bethge\n",
              "9                  methods              analysis             Tao Qin"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-29f5ebb3-ecee-403c-9560-9cdcf62abf64\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>top10_terms_in_abstracts</th>\n",
              "      <th>top10_terms_in_titles</th>\n",
              "      <th>top10_authors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>model</td>\n",
              "      <td>learning</td>\n",
              "      <td>Francis Bach</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>learning</td>\n",
              "      <td>models</td>\n",
              "      <td>Geoffrey E. Hinton</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>algorithm</td>\n",
              "      <td>neural</td>\n",
              "      <td>Lawrence Carin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data</td>\n",
              "      <td>inference</td>\n",
              "      <td>Percy S. Liang</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>problem</td>\n",
              "      <td>optimization</td>\n",
              "      <td>Richard Zemel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>show</td>\n",
              "      <td>data</td>\n",
              "      <td>Carlos Guestrin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>algorithms</td>\n",
              "      <td>model</td>\n",
              "      <td>Jakob H. Macke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>models</td>\n",
              "      <td>estimation</td>\n",
              "      <td>Manfred Opper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>paper</td>\n",
              "      <td>networks</td>\n",
              "      <td>Matthias Bethge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>methods</td>\n",
              "      <td>analysis</td>\n",
              "      <td>Tao Qin</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29f5ebb3-ecee-403c-9560-9cdcf62abf64')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-29f5ebb3-ecee-403c-9560-9cdcf62abf64 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-29f5ebb3-ecee-403c-9560-9cdcf62abf64');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stats_df.to_csv('/content/drive/MyDrive/Asessment1/OutputFiles/Task2/task2_31940757_stats.csv')"
      ],
      "metadata": {
        "id": "5FOlJubAqv6_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUFQU-QXlDEn"
      },
      "source": [
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HppxDtWNlDEn"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 6. References <a class=\"anchor\" name=\"Ref\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCkWr-M1lDEo"
      },
      "source": [
        "[1] https://cheatography.com/davechild/cheat-sheets/regular-expressions/\n",
        "\n",
        "[2] https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "\n",
        "[3] https://towardsdatascience.com/basics-of-countvectorizer-e26677900f9c#:~:text=Countvectorizer%20is%20a%20method%20to,sparse%20matrix%20as%20shown%20below.\n",
        "\n",
        "[4] https://stackoverflow.com/questions/4233476/sort-a-list-by-multiple-attributes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp9O-a1UlDEo"
      },
      "source": [
        "## --------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}